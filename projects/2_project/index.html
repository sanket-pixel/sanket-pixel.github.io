<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Webcam-GPT with visual language models | Sanket Shah</title>
    <meta name="author" content="Sanket R. Shah">
    <meta name="description" content="Visual Language Model based Webcam Interactive QnA">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://sanket-pixel.github.io//projects/2_project/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-J2Z5HX2M1E"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-J2Z5HX2M1E');
  </script>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">Sanket Shah</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Webcam-GPT with visual language models</h1>
            <p class="post-description">Visual Language Model based Webcam Interactive QnA</p>
          </header>

          <article>
            <p>Github Link : <a href="https://github.com/sanket-pixel/webcam-gpt" rel="external nofollow noopener" target="_blank">https://github.com/sanket-pixel/webcam-gpt</a></p>

<p>This project is a high-performance proof-of-concept demonstrating a real-time, interactive visual question &amp; answering system. It leverages a local Flask web server to stream a user’s webcam, accept natural language questions, and provide context-aware answers generated by the <code class="language-plaintext highlighter-rouge">moondream2</code> Vision Language Model (VLM).</p>

<p>The architecture is designed for low-latency interaction, making it a powerful baseline for more advanced multimodal AI applications.
Live Demo</p>

<p><br></p>
<div style="width: 90%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/project/project_2/demo.gif-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/project/project_2/demo.gif-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/project/project_2/demo.gif-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/project/project_2/demo.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
   Part 3 : WebcamGPT
</div>
</div>

<p>A live demonstration showing the webcam feed and the model answering three distinct questions about the scene in real-time.</p>

<p><strong>Core Technologies</strong></p>

<ul>
  <li>
<em>Backend</em>: Python 3.9+, Flask, Flask-SocketIO</li>
  <li>
<em>AI/ML</em>: PyTorch, Hugging Face Transformers (vikhyatk/moondream2)</li>
  <li>
<em>Frontend</em>: HTML5, JavaScript (WebRTC for webcam, WebSockets for communication)</li>
  <li>
<em>Image Processing</em>: Pillow</li>
</ul>

<h4 id="1-system-architecture-deep-dive">1. System Architecture Deep Dive</h4>

<p>The application is built on a robust client-server architecture designed for real-time, bidirectional communication. The system avoids simple request-response cycles in favor of a persistent WebSocket connection, which is critical for minimizing latency in an interactive AI application.</p>

<h5 id="11-frontend-the-eye">1.1 Frontend (The Eye):</h5>

<p><strong>Webcam Streaming</strong>: The browser’s navigator.mediaDevices.getUserMedia API is used to access the webcam feed, providing a live video stream directly in the UI without server-side processing.</p>

<p><strong>Frame Capture</strong>: A hidden HTML element acts as an intermediate buffer. When a query is submitted, the current frame from the <code class="language-plaintext highlighter-rouge">&lt;video&gt;</code>element is drawn onto the canvas.</p>

<p><strong>Data Serialization</strong>: The captured frame is converted into a Base64-encoded JPEG string. This efficient serialization allows the image data to be transmitted as text within a JSON payload.</p>

<p><strong>WebSocket Communication</strong>: The client communicates with the backend via a Socket.IO connection. The JSON payload containing the user’s question and the Base64 image string is emitted to the server on a query event.</p>

<h5 id="12-backend-the-brain">1.2 Backend (The Brain):</h5>

<p><strong>Flask &amp; SocketIO Server</strong>: A Python server manages the application logic. Flask handles the initial serving of the index.html page, while Flask-SocketIO manages the persistent WebSocket connection.</p>

<p><strong>Singleton Model Initialization</strong>: A critical design choice for performance is loading the moondream2 model into GPU VRAM only once when the server starts. This singleton pattern ensures that the heavyweight model is always resident in memory, avoiding costly loading delays on each query and enabling near-instantaneous inference.</p>

<p><strong>Asynchronous Event Handling</strong>: The server listens for the query event. Upon receiving the data, it decodes the Base64 string back into a PIL Image object and passes both the image and the question to the VLM for inference. The generated answer is then emitted back to the client on a response event.</p>

<h4 id="2-vlm-inference-pipeline-a-network-level-explanation">2. VLM Inference Pipeline: A Network-Level Explanation</h4>

<p>The magic of this application lies in how the <code class="language-plaintext highlighter-rouge">moondream2</code> model processes and reasons about two distinct data modalities: <code class="language-plaintext highlighter-rouge">image</code> and <code class="language-plaintext highlighter-rouge">text</code>. The high-level model.query() function abstracts a sophisticated series of network-level operations.</p>

<h5 id="21-multimodal-input-processing">2.1. Multimodal Input Processing</h5>

<p>Before inference can begin, the raw inputs must be converted into a format the Transformer architecture can understand: numerical embeddings.</p>

<p><strong>Vision Encoding</strong>: The input image is not processed as a whole. Instead, a Vision Transformer (ViT) backbone performs the following:</p>

<p><strong>Patching</strong>: The image is divided into a grid of smaller, fixed-size patches (e.g., 14x14 pixels).</p>

<p><strong>Embedding</strong>: Each patch is flattened and linearly projected into a high-dimensional vector space, creating “image patch embeddings.”</p>

<p><strong>Positional Encoding</strong>: Information about the original position of each patch is added to its embedding.</p>

<p><strong>Transformer Blocks</strong>: This sequence of patch embeddings is processed through several layers of the ViT. The self-attention mechanism within these layers allows the model to understand the relationships between different parts of the image. The final output is a sequence of contextualized image_embeds.</p>

<p><strong>Text Tokenization</strong>: Simultaneously, the user’s text query is processed by a specialized tokenizer. It converts the string into a sequence of numerical token IDs (input_ids), where each ID corresponds to a word or sub-word in the model’s vocabulary.</p>

<h5 id="22-autoregressive-generation-with-cross-modal-attention">2.2 Autoregressive Generation with Cross-Modal Attention</h5>

<p>This is the core of the VLM’s reasoning capability. The <code class="language-plaintext highlighter-rouge">image_embeds</code> and the <code class="language-plaintext highlighter-rouge">input_ids</code> are fed into the main language model decoder.</p>

<p><strong>Combining Modalities</strong>: The image embeddings and text embeddings are concatenated to form a single input sequence.</p>

<p><strong>Cross-Modal Attention</strong>: As the language model processes this combined sequence, its cross-attention layers allow the text tokens to “attend to” or “look at” the image patch embeddings. This is the mechanism that grounds the language in the visual data, enabling the model to answer questions about the image content.</p>

<p><strong>The Generation Loop</strong>: The model does not generate the entire answer at once. Instead, it performs an autoregressive loop:</p>

<p><strong>Prediction</strong>: Based on the combined image and text input, the model’s final layers predict a probability distribution over its entire vocabulary for the single most likely next token.</p>

<p><strong>Sampling</strong>: The token with the highest probability is chosen.</p>

<p><strong>Appending</strong>: This new token is appended to the input sequence.</p>

<p><strong>Iteration</strong>: The entire process repeats, with the now-extended sequence fed back into the model to predict the next token.</p>

<p>This loop continues, generating one token at a time, until the model predicts a special End-of-Sequence (EOS) token, signaling that the answer is complete.</p>

<h4 id="3-setup-and-installation">3. Setup and Installation</h4>

<ol>
  <li>Clone the repository:
    <div class="language-plaintext highlighter-rouge">
<div class="highlight"><pre class="highlight"><code> git clone &lt;your-repo-url&gt;
 cd &lt;your-repo-name&gt;
</code></pre></div>    </div>
  </li>
  <li>
    <p>Create and activate a virtual environment:</p>

    <div class="language-plaintext highlighter-rouge">
<div class="highlight"><pre class="highlight"><code> # Using uv (recommended)
 uv venv
 source .venv/bin/activate
</code></pre></div>    </div>
  </li>
  <li>Install dependencies:
    <div class="language-plaintext highlighter-rouge">
<div class="highlight"><pre class="highlight"><code> uv pip install -r requirements.txt
</code></pre></div>    </div>
  </li>
  <li>Run the application:
    <div class="language-plaintext highlighter-rouge">
<div class="highlight"><pre class="highlight"><code> python app.py
</code></pre></div>    </div>
  </li>
  <li>Open your web browser and navigate to <code class="language-plaintext highlighter-rouge">http://localhost:5000</code>.</li>
</ol>

          </article>

        </div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Sanket R. Shah. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
