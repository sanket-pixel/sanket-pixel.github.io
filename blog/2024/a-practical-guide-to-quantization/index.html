<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>A practical guide to Quantization | Sanket Shah</title>
    <meta name="author" content="Sanket R. Shah">
    <meta name="description" content="How to exactly quantize models and still not lose accuracy.">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://sanket-pixel.github.io//blog/2024/a-practical-guide-to-quantization/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-J2Z5HX2M1E"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-J2Z5HX2M1E');
  </script>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">Sanket Shah</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">A practical guide to Quantization</h1>
    <p class="post-meta">July 21, 2024</p>
    <p class="post-tags">
      <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>
        ·  
        <a href="/blog/tag/nvidia">
          <i class="fas fa-hashtag fa-sm"></i> nvidia</a>  
          <a href="/blog/tag/tensorrt">
          <i class="fas fa-hashtag fa-sm"></i> tensorrt</a>  
          <a href="/blog/tag/deep-learning">
          <i class="fas fa-hashtag fa-sm"></i> deep-learning</a>  
          
        ·  
        <a href="/blog/category/cuda">
          <i class="fas fa-tag fa-sm"></i> cuda</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <h4 id="in-this-blog-we-delve-into-the-practical-side-of-model-optimization-focusing-on-how-to-leverage-tensorrt-for-int8-quantization-to-drastically-improve-inference-speed-by-walking-through-the-process-step-by-step-we-compare-pure-pytorch-inference-tensorrt-optimization-and-finally-int8-quantization-with-calibration-the-results-highlight-the-incredible-potential-of-these-techniques-with-an-over-10x-speedup-in-performance-whether-youre-aiming-to-deploy-deep-learning-models-in-production-or-simply-seeking-to-enhance-your-understanding-of-model-optimization-this-blog-provides-valuable-insights-into-achieving-faster-and-more-efficient-inference">In this blog, we delve into the practical side of model optimization, focusing on how to leverage TensorRT for INT8 quantization to drastically improve inference speed. By walking through the process step-by-step, we compare pure PyTorch inference, TensorRT optimization, and finally, INT8 quantization with calibration. The results highlight the incredible potential of these techniques, with an over <code class="language-plaintext highlighter-rouge">10x speedup</code> in performance. Whether you’re aiming to deploy deep learning models in production or simply seeking to enhance your understanding of model optimization, this blog provides valuable insights into achieving faster and more efficient inference.</h4>

<p><br></p>
<div style="width: 100%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_6/compare-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_6/compare-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_6/compare-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_6/compare.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
 A practical guide to Quantization
</div>
</div>

<p>In our previous blog post,  <a href="/blog/2024/quantization-explained-like-youre-five/">Quantization explained, like you are five</a>, we covered the theory and intuition behind quantization in detail. If you haven’t read it yet, we highly recommend doing so to gain a solid understanding of the fundamental concepts. This current post serves as a sequel, diving into the practical aspects of quantization. We will guide you through the process of applying quantization to a neural network, demonstrating how it can significantly speed up inference.</p>

<p>In this blog, we would using the resnet based image classfication model as an example. We would be using TensorRT as well for quantization. This constrains this code only for NVIDIA GPU devices. But these same concepts can be extended to other devices and tools without loss of generality. 
If you need a primer on how resnet based image classifciation can be deployed using TensorRT on nvidia devices, refer to previous blog <a href="/blog/2023/introduction-to-tensorrt/">Have you met TensorRT?</a> where we saw how to use FP16 based TensorRT engine for inference. In this blog we will take it a step further by quantizing the same model to INT8.</p>

<p>We will go about this using 3 major steps :</p>

<p><strong>1. Inference with Pure PyTorch</strong></p>
<ul>
  <li>Establish a baseline for latency and accuracy using the original PyTorch model.</li>
  <li>Measure inference time and accuracy on a sample dataset.</li>
</ul>

<p><strong>2. Inference with TensorRT Engine</strong></p>
<ul>
  <li>Convert the PyTorch model to a TensorRT engine using FP16 precision.</li>
  <li>Measure and compare the latency and accuracy to the PyTorch baseline.</li>
</ul>

<p><strong>3. Inference with TensorRT Engine (INT8, With Calibration)</strong></p>
<ul>
  <li>Perform INT8 quantization with calibration using a calibration dataset.</li>
  <li>Measure latency and accuracy, highlighting the trade-offs and benefits of quantization.</li>
</ul>

<h4 id="1-inference-with-pure-pytorch">1. Inference with Pure Pytorch</h4>

<p>To establish a baseline for both accuracy and latency, we will first run inference using the ResNet-50 model in PyTorch. This section will cover the preprocessing, inference, and postprocessing steps. The results here will serve as a comparison for the performance improvements we achieve using TensorRT and INT8 quantization in later steps.</p>

<h5 id="a-preprocessing">a. Preprocessing</h5>

<p>The preprocessing step involves preparing the input image so that it is suitable for the ResNet-50 model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">cv2</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torchvision.transforms</span> <span class="kn">import</span> <span class="n">Resize</span><span class="p">,</span> <span class="n">Compose</span><span class="p">,</span> <span class="n">ToTensor</span><span class="p">,</span> <span class="n">Normalize</span>

<span class="k">def</span> <span class="nf">preprocess_image</span><span class="p">(</span><span class="n">img_path</span><span class="p">):</span>
    <span class="c1"># Transformations for the input data
</span>    <span class="n">transforms</span> <span class="o">=</span> <span class="nc">Compose</span><span class="p">([</span>
        <span class="nc">ToTensor</span><span class="p">(),</span>
        <span class="nc">Resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)),</span>
        <span class="nc">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]),</span>
    <span class="p">])</span>
    <span class="c1"># Read input image
</span>    <span class="n">input_img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">imread</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
    <span class="c1"># Apply transformations
</span>    <span class="n">input_data</span> <span class="o">=</span> <span class="nf">transforms</span><span class="p">(</span><span class="n">input_img</span><span class="p">)</span>
    <span class="n">batch_data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch_data</span>
</code></pre></div></div>

<h5 id="b-postprocessing">b. Postprocessing</h5>
<p>After inference, the output from the model needs to be processed to extract meaningful predictions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">postprocess</span><span class="p">(</span><span class="n">output_data</span><span class="p">):</span>
    <span class="c1"># Get class names
</span>    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">data/imagenet-classes.txt</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">.</span><span class="nf">readlines</span><span class="p">()]</span>
    <span class="c1"># Calculate human-readable values by softmax
</span>    <span class="n">confidences</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">output_data</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="c1"># Find top predicted classes
</span>    <span class="n">_</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">output_data</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Print the top classes predicted by the model
</span>    <span class="k">while</span> <span class="n">confidences</span><span class="p">[</span><span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]]</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>
        <span class="n">class_idx</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
        <span class="nf">print</span><span class="p">(</span>
            <span class="sh">"</span><span class="s">class:</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">classes</span><span class="p">[</span><span class="n">class_idx</span><span class="p">],</span>
            <span class="sh">"</span><span class="s">, confidence:</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">confidences</span><span class="p">[</span><span class="n">class_idx</span><span class="p">].</span><span class="nf">item</span><span class="p">(),</span>
            <span class="sh">"</span><span class="s">%, index:</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">class_idx</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="c1"># Postprocess the output
</span><span class="nf">postprocess</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></div>
<h5 id="c-inference">c. Inference</h5>

<p>This step involves running the preprocessed image through the ResNet-50 model to obtain predictions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">import</span> <span class="n">torch.backends.cudnn</span> <span class="k">as</span> <span class="n">cudnn</span>
<span class="kn">import</span> <span class="n">time</span>
<span class="c1"># Load and prepare the model
</span><span class="n">cudnn</span><span class="p">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="c1"># Preprocess the input image
</span><span class="nb">input</span> <span class="o">=</span> <span class="nf">preprocess_image</span><span class="p">(</span><span class="sh">"</span><span class="s">data/hotdog.jpg</span><span class="sh">"</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
<span class="c1"># Perform inference
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="c1"># Warm-up the GPU
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="c1"># Measure latency
</span><span class="n">latencies</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>  <span class="c1"># Ensure that all CUDA operations are finished
</span>        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
        
        <span class="n">latency</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
        <span class="n">latencies</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">latency</span><span class="p">)</span>
<span class="nf">postprocess</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="n">average_latency</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Average Latency: </span><span class="si">{</span><span class="n">average_latency</span> <span class="o">*</span> <span class="mi">1000</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> ms</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h5 id="d-performance">d. Performance</h5>
<p>The output from pure Pytorch inference would look something like this.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class: hotdog, hot dog, red hot , confidence: 85.24353790283203 %, index: 934
Average Latency: 3.94 ms
</code></pre></div></div>

<h4 id="2-tensorrt-inference">2. TensorRT inference</h4>

<p>In this section, we demonstrate how to convert a trained PyTorch model to an ONNX format, then build a TensorRT engine, and finally perform inference using the TensorRT engine. This process significantly optimizes the inference performance on NVIDIA GPUs.</p>

<p>For more detailed explanations on converting PyTorch models to TensorRT engines, refer to our previous blog post, <a href="/blog/2023/introduction-to-tensorrt/">Have you met TensorRT?</a>.</p>

<h5 id="a-convert-pytorch-model-to-onnx">a. Convert PyTorch Model to ONNX</h5>

<p>First, we export the PyTorch model to the ONNX format. This intermediate representation serves as a bridge between different deep learning frameworks and tools like TensorRT.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ONNX_FILE_PATH</span> <span class="o">=</span> <span class="sh">'</span><span class="s">deploy_tools/resnet50.onnx</span><span class="sh">'</span>
<span class="n">torch</span><span class="p">.</span><span class="n">onnx</span><span class="p">.</span><span class="nf">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">ONNX_FILE_PATH</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">input</span><span class="sh">'</span><span class="p">],</span>
                  <span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">output</span><span class="sh">'</span><span class="p">],</span> <span class="n">export_params</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>
<h5 id="b-build-tensorrt-engine">b. Build TensorRT Engine</h5>

<p>Next, we parse the ONNX model and build the TensorRT engine, which is optimized for the target NVIDIA GPU.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pycuda.driver</span> <span class="k">as</span> <span class="n">cuda</span>
<span class="kn">import</span> <span class="n">pycuda.autoinit</span>
<span class="kn">import</span> <span class="n">tensorrt</span> <span class="k">as</span> <span class="n">trt</span>

<span class="n">TRT_LOGGER</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Logger</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">Logger</span><span class="p">.</span><span class="n">ERROR</span><span class="p">)</span>
<span class="n">builder</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Builder</span><span class="p">(</span><span class="n">TRT_LOGGER</span><span class="p">)</span>

<span class="n">EXPLICIT_BATCH</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="nf">int</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">NetworkDefinitionCreationFlag</span><span class="p">.</span><span class="n">EXPLICIT_BATCH</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">create_network</span><span class="p">(</span><span class="n">EXPLICIT_BATCH</span><span class="p">)</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">OnnxParser</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">TRT_LOGGER</span><span class="p">)</span>
<span class="n">success</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="nf">parse_from_file</span><span class="p">(</span><span class="n">ONNX_FILE_PATH</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">parser</span><span class="p">.</span><span class="n">num_errors</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">parser</span><span class="p">.</span><span class="nf">get_error</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">create_builder_config</span><span class="p">()</span>
<span class="n">config</span><span class="p">.</span><span class="nf">set_memory_pool_limit</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">MemoryPoolType</span><span class="p">.</span><span class="n">WORKSPACE</span><span class="p">,</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">30</span><span class="p">)</span>  <span class="c1"># 1 MiB
</span><span class="n">serialized_engine</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">build_serialized_network</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">deploy_tools/resnet50.engine</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">wb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">serialized_engine</span><span class="p">)</span>
</code></pre></div></div>
<p>This code snippet handles the following:</p>

<ul>
  <li>
<strong>Logger</strong>: Initializes the TensorRT logger to monitor errors during the engine creation.</li>
  <li>
<strong>Builder and Network</strong>: Creates the TensorRT builder and network definition.</li>
  <li>
<strong>ONNX Parser</strong>: Parses the ONNX model into the TensorRT network.</li>
  <li>
<strong>Engine Serialization</strong>: Serializes the TensorRT engine and saves it to a file for later use.</li>
</ul>

<h5 id="c-inference-with-tensorrt-engine">c. Inference with TensorRT Engine</h5>

<p>After building the TensorRT engine, we can load it and run inference on the input data. The process involves transferring data to the GPU, executing the engine, and retrieving the results.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">runtime</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Runtime</span><span class="p">(</span><span class="n">TRT_LOGGER</span><span class="p">)</span>

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">deploy_tools/resnet50.engine</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">serialized_engine</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="n">engine</span> <span class="o">=</span> <span class="n">runtime</span><span class="p">.</span><span class="nf">deserialize_cuda_engine</span><span class="p">(</span><span class="n">serialized_engine</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">engine</span><span class="p">.</span><span class="nf">create_execution_context</span><span class="p">()</span>

<span class="c1"># Determine dimensions and create memory buffers for inputs/outputs
</span><span class="n">h_input</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">pagelocked_empty</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="nf">volume</span><span class="p">(</span><span class="n">engine</span><span class="p">.</span><span class="nf">get_binding_shape</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">h_output</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">pagelocked_empty</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="nf">volume</span><span class="p">(</span><span class="n">engine</span><span class="p">.</span><span class="nf">get_binding_shape</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">d_input</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">mem_alloc</span><span class="p">(</span><span class="n">h_input</span><span class="p">.</span><span class="n">nbytes</span><span class="p">)</span>
<span class="n">d_output</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">mem_alloc</span><span class="p">(</span><span class="n">h_output</span><span class="p">.</span><span class="n">nbytes</span><span class="p">)</span>
<span class="n">stream</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nc">Stream</span><span class="p">()</span>

<span class="c1"># Preprocess input image
</span><span class="n">host_input</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">preprocess_image</span><span class="p">(</span><span class="sh">"</span><span class="s">data/hotdog.jpg</span><span class="sh">"</span><span class="p">).</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Warm-up the GPU
</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_htod_async</span><span class="p">(</span><span class="n">d_input</span><span class="p">,</span> <span class="n">host_input</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">context</span><span class="p">.</span><span class="nf">execute_async_v2</span><span class="p">(</span><span class="n">bindings</span><span class="o">=</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">d_input</span><span class="p">),</span> <span class="nf">int</span><span class="p">(</span><span class="n">d_output</span><span class="p">)],</span> <span class="n">stream_handle</span><span class="o">=</span><span class="n">stream</span><span class="p">.</span><span class="n">handle</span><span class="p">)</span>
    <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_dtoh_async</span><span class="p">(</span><span class="n">h_output</span><span class="p">,</span> <span class="n">d_output</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">stream</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>

<span class="c1"># Measure latency
</span><span class="n">latencies</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_htod_async</span><span class="p">(</span><span class="n">d_input</span><span class="p">,</span> <span class="n">host_input</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">context</span><span class="p">.</span><span class="nf">execute_async_v2</span><span class="p">(</span><span class="n">bindings</span><span class="o">=</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">d_input</span><span class="p">),</span> <span class="nf">int</span><span class="p">(</span><span class="n">d_output</span><span class="p">)],</span> <span class="n">stream_handle</span><span class="o">=</span><span class="n">stream</span><span class="p">.</span><span class="n">handle</span><span class="p">)</span>
    <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_dtoh_async</span><span class="p">(</span><span class="n">h_output</span><span class="p">,</span> <span class="n">d_output</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">stream</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">latency</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="n">latencies</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">latency</span><span class="p">)</span>

<span class="n">average_latency</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Average Latency: </span><span class="si">{</span><span class="n">average_latency</span> <span class="o">*</span> <span class="mi">1000</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> ms</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Postprocess and display results
</span><span class="n">tensorrt_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">h_output</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">postprocess</span><span class="p">(</span><span class="n">tensorrt_output</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">empty_cache</span><span class="p">()</span>
</code></pre></div></div>

<p>In this step:</p>

<ul>
  <li>
<strong>Runtime and Engine</strong>: We deserialize the saved TensorRT engine and create an execution context.</li>
  <li>
<strong>Memory Allocation</strong>: Allocate memory for inputs and outputs on both host (CPU) and device (GPU).</li>
  <li>
<strong>Inference</strong>: Perform inference using the TensorRT engine, measuring the latency over multiple iterations.</li>
  <li>
<strong>Postprocessing</strong>: Convert the output back to a PyTorch tensor for easier postprocessing and interpretation of the results.</li>
</ul>

<h5 id="d-performance-1">d. Performance</h5>
<p>The output from TensorRT inference is as follows ( may vary on different GPUs)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Average Latency: 2.73 ms
class: hotdog, hot dog, red hot , confidence: 85.29612731933594 %, index: 934
</code></pre></div></div>
<p>With just basic TensorRT engine conversion, we achieved a <code class="language-plaintext highlighter-rouge">1.44x</code> speedup. Now lets convert it to a quantized INT8 TensorRT engine and see how much speedup we can achieve.</p>

<h4 id="3-inference-with-int8-quantization">3. Inference with INT8 Quantization</h4>

<p><br></p>
<div style="width: 100%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_6/quantization-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_6/quantization-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_6/quantization-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_6/quantization.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Steps for Quantization
</div>
</div>

<p>In this section, we’ll dive into the heart of the optimization process: calibration and quantization. This process is essential for converting a model to INT8 precision, which can significantly boost inference speed while maintaining a good level of accuracy.</p>

<h5 id="a-implementing-a-custom-calibration-class">a. Implementing a Custom Calibration Class</h5>

<p>Before we can convert our model to INT8, we need to calibrate it. Calibration is the process where we run a representative dataset through the model to collect statistical information about its activations. This data is crucial for accurately mapping the floating-point values to INT8 values. To do this in TensorRT, we implement a custom calibration class that extends from the base trt.IInt8EntropyCalibrator2 class.</p>

<p>Here’s how we set up the calibration process:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ImageCalibrator</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">IInt8EntropyCalibrator2</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">image_dir_path</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ImageCalibrator</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">current_index</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">image_paths</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">load_images_paths</span><span class="p">(</span><span class="n">image_dir_path</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_samples</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">image_paths</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">device_image</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">mem_alloc</span><span class="p">(</span><span class="mi">1</span><span class="o">*</span><span class="mi">3</span><span class="o">*</span><span class="mi">224</span><span class="o">*</span><span class="mi">224</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">load_images_paths</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">image_dir_path</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="n">image_dir_path</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_batch_size</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">batch_size</span>

    <span class="k">def</span> <span class="nf">get_batch</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">names</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">current_index</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">num_samples</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">None</span>
        <span class="n">image_path</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">image_paths</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">current_index</span><span class="p">]</span>
        <span class="n">image_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">data/calibration_dataset/</span><span class="sh">"</span> <span class="o">+</span> <span class="n">image_path</span>
        <span class="n">image</span> <span class="o">=</span> <span class="nf">preprocess_image</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>
        <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_htod</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device_image</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">current_index</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="n">batch_size</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">device_image</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">read_calibration_cache</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">calibration.cache</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">rb</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>
        <span class="k">except</span> <span class="nb">FileNotFoundError</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">write_calibration_cache</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
        <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">calibration.cache</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">wb</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">cache</span><span class="p">)</span>
</code></pre></div></div>

<p>What’s Happening Here?</p>

<p><strong>Initialization</strong>: The class is initialized with the directory path containing calibration images. These images are loaded into memory as the calibration process iterates through them.</p>

<p><strong>Image Preprocessing</strong>: Each image from the calibration dataset is preprocessed and then copied to the GPU’s memory for use in calibration. The batch size is set to 1 for simplicity, but this can be adjusted based on your needs.</p>

<p><strong>Handling the Calibration Cache</strong>: TensorRT can cache the calibration data, so you don’t have to recalibrate the model every time you run this process. The cache is read from and written to the disk, allowing for faster reuse of calibration data in future runs.</p>

<p>When calibrating other models, you’ll need to implement a similar class tailored to the specific preprocessing and data handling requirements of your model.</p>

<h5 id="b-building-the-tensorrt-engine-with-int8-quantization">b. Building the TensorRT Engine with INT8 Quantization</h5>

<p>Once we’ve set up our calibrator, we’re ready to build the TensorRT engine with INT8 precision. The steps involve parsing the model from an ONNX file and configuring the builder to use the INT8 calibration data.</p>

<p>Here’s how it’s done:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ONNX_FILE_PATH</span> <span class="o">=</span> <span class="sh">"</span><span class="s">deploy_tools/resnet50.onnx</span><span class="sh">"</span>
<span class="n">image_dir</span> <span class="o">=</span> <span class="sh">"</span><span class="s">data/calibration_dataset</span><span class="sh">"</span>
<span class="n">TRT_LOGGER</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Logger</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">Logger</span><span class="p">.</span><span class="n">ERROR</span><span class="p">)</span>
<span class="n">builder</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Builder</span><span class="p">(</span><span class="n">TRT_LOGGER</span><span class="p">)</span>
<span class="n">EXPLICIT_BATCH</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="nf">int</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">NetworkDefinitionCreationFlag</span><span class="p">.</span><span class="n">EXPLICIT_BATCH</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">create_network</span><span class="p">(</span><span class="n">EXPLICIT_BATCH</span><span class="p">)</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">OnnxParser</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">TRT_LOGGER</span><span class="p">)</span>
<span class="n">success</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="nf">parse_from_file</span><span class="p">(</span><span class="n">ONNX_FILE_PATH</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">parser</span><span class="p">.</span><span class="n">num_errors</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">parser</span><span class="p">.</span><span class="nf">get_error</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">create_builder_config</span><span class="p">()</span>
<span class="n">config</span><span class="p">.</span><span class="nf">set_memory_pool_limit</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">MemoryPoolType</span><span class="p">.</span><span class="n">WORKSPACE</span><span class="p">,</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">30</span><span class="p">)</span>  <span class="c1"># 1 GiB
</span>
<span class="c1"># Assign the custom calibrator
</span><span class="n">config</span><span class="p">.</span><span class="n">int8_calibrator</span> <span class="o">=</span> <span class="nc">ImageCalibrator</span><span class="p">(</span><span class="n">image_dir</span><span class="p">)</span>
<span class="n">config</span><span class="p">.</span><span class="nf">set_flag</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">BuilderFlag</span><span class="p">.</span><span class="n">INT8</span><span class="p">)</span>

<span class="c1"># Build and serialize the INT8 TensorRT engine
</span><span class="n">serialized_engine</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">build_serialized_network</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">deploy_tools/resnet_int8.engine</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">wb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">serialized_engine</span><span class="p">)</span>
</code></pre></div></div>

<p>Key Points:</p>

<ul>
  <li>
    <p><strong>ONNX Parser</strong>: The model is parsed from the ONNX format, and the network is created using TensorRT.</p>
  </li>
  <li>
    <p><strong>INT8 Calibration</strong>: The custom calibrator class we defined earlier is assigned to the builder configuration. The INT8 flag is set, instructing TensorRT to use the calibration data for quantizing the model.</p>
  </li>
  <li>
    <p><strong>Engine Serialization</strong>: Once the engine is built with INT8 precision, it is serialized and saved to disk. This engine is now optimized for fast inference using 8-bit integer operations.</p>
  </li>
</ul>

<p>For more details on the general process of building TensorRT engines, refer to our previous post, “Have you met TensorRT?”.</p>

<h5 id="c-running-inference-with-the-int8-optimized-engine">c. Running Inference with the INT8-Optimized Engine</h5>

<p>Finally, we can run inference using the INT8-optimized TensorRT engine. Here’s how it’s done:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">runtime</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Runtime</span><span class="p">(</span><span class="n">TRT_LOGGER</span><span class="p">)</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">deploy_tools/resnet_int8.engine</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">serialized_engine</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">runtime</span><span class="p">.</span><span class="nf">deserialize_cuda_engine</span><span class="p">(</span><span class="n">serialized_engine</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">engine</span><span class="p">.</span><span class="nf">create_execution_context</span><span class="p">()</span>

<span class="c1"># Allocate memory for inputs and outputs
</span><span class="n">h_input</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">pagelocked_empty</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="nf">volume</span><span class="p">(</span><span class="n">engine</span><span class="p">.</span><span class="nf">get_binding_shape</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">h_output</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">pagelocked_empty</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="nf">volume</span><span class="p">(</span><span class="n">engine</span><span class="p">.</span><span class="nf">get_binding_shape</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">d_input</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">mem_alloc</span><span class="p">(</span><span class="n">h_input</span><span class="p">.</span><span class="n">nbytes</span><span class="p">)</span>
<span class="n">d_output</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">mem_alloc</span><span class="p">(</span><span class="n">h_output</span><span class="p">.</span><span class="n">nbytes</span><span class="p">)</span>
<span class="n">stream</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nc">Stream</span><span class="p">()</span>

<span class="c1"># Preprocess and load the input image
</span><span class="n">host_input</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">preprocess_image</span><span class="p">(</span><span class="sh">"</span><span class="s">data/hotdog.jpg</span><span class="sh">"</span><span class="p">).</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Warm-up the GPU
</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_htod_async</span><span class="p">(</span><span class="n">d_input</span><span class="p">,</span> <span class="n">host_input</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">context</span><span class="p">.</span><span class="nf">execute_async_v2</span><span class="p">(</span><span class="n">bindings</span><span class="o">=</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">d_input</span><span class="p">),</span> <span class="nf">int</span><span class="p">(</span><span class="n">d_output</span><span class="p">)],</span> <span class="n">stream_handle</span><span class="o">=</span><span class="n">stream</span><span class="p">.</span><span class="n">handle</span><span class="p">)</span>
    <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_dtoh_async</span><span class="p">(</span><span class="n">h_output</span><span class="p">,</span> <span class="n">d_output</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">stream</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>

<span class="c1"># Measure latency for INT8 inference
</span><span class="n">iterations</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">latencies</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_htod_async</span><span class="p">(</span><span class="n">d_input</span><span class="p">,</span> <span class="n">host_input</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">context</span><span class="p">.</span><span class="nf">execute_async_v2</span><span class="p">(</span><span class="n">bindings</span><span class="o">=</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">d_input</span><span class="p">),</span> <span class="nf">int</span><span class="p">(</span><span class="n">d_output</span><span class="p">)],</span> <span class="n">stream_handle</span><span class="o">=</span><span class="n">stream</span><span class="p">.</span><span class="n">handle</span><span class="p">)</span>
    <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_dtoh_async</span><span class="p">(</span><span class="n">h_output</span><span class="p">,</span> <span class="n">d_output</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">stream</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">latencies</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>

<span class="c1"># Calculate average latency
</span><span class="n">average_latency</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Average Latency for INT8 Inference: </span><span class="si">{</span><span class="n">average_latency</span> <span class="o">*</span> <span class="mi">1000</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> ms</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Post-process the output
</span><span class="n">tensorrt_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">h_output</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">postprocess</span><span class="p">(</span><span class="n">tensorrt_output</span><span class="p">)</span>
</code></pre></div></div>

<p>What’s Happening Here?</p>

<ul>
  <li>
    <p><strong>Engine Deserialization</strong>: The INT8 engine is loaded from disk and deserialized into memory.</p>
  </li>
  <li>
    <p><strong>Memory Management</strong>: We allocate memory for inputs and outputs on both the host and device, setting up everything needed to run inference.</p>
  </li>
  <li>
    <p><strong>Inference Execution</strong>: After warming up the GPU, inference is run multiple times to measure latency. Using INT8 should provide a significant reduction in latency compared to FP32 inference.</p>
  </li>
  <li>
    <p><strong>Output Post-processing</strong>: Finally, the output is converted back to a PyTorch tensor and processed just like we did with the FP32 model.</p>
  </li>
</ul>

<p>This step demonstrates the power of quantization. With the INT8 engine, we can achieve faster inference times, which is crucial for deploying models in real-time applications.</p>

<h5 id="d-performance-2">d. Performance</h5>
<p>The output from INT8 quantized TensorRT engine is as follows :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Average Latency for INT8 Inference: 0.39 ms
class: hotdog, hot dog, red hot , confidence: 93.55050659179688 %, index: 934
</code></pre></div></div>

<p>As we can see, INT8 quantization achieved a whooping <code class="language-plaintext highlighter-rouge">10.25x speedup</code> as compared to pure Pytorch inference. All the struggle we went through for calibration and quantization was indeed worth it. ;)</p>

<h4 id="summary">Summary</h4>

<p>In this blog, we explore the practical application of quantization using TensorRT to significantly speed up inference on a ResNet-based image classification model. We begin with a baseline comparison, demonstrating inference using pure PyTorch, which provides a foundational understanding of the model’s performance. Next, we transition to using a TensorRT engine, showcasing the initial speed improvements by optimizing the model for NVIDIA GPUs. Finally, we delve into INT8 quantization, applying calibration techniques to maximize efficiency. The results are striking, with a speedup of over 10x compared to the original PyTorch inference, illustrating the power of quantization in real-world scenarios.</p>

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/that-first-cuda-blog-3/">That First CUDA Blog I Needed :Part 3</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/that-first-cuda-blog-2/">That First CUDA Blog I Needed :Part 2</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/that-first-cuda-blog-1/">That First CUDA Blog I Needed</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/that-first-cuda-blog/">That First CUDA Blog I Needed</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/hidden-speed-in-shared-memory/">Hidden Speed in CUDA's Shared Memory</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Sanket R. Shah. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
