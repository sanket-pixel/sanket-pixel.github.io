<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>That First CUDA Blog I Needed | Sanket Shah</title>
    <meta name="author" content="Sanket R. Shah">
    <meta name="description" content="The ideal first blog to start learning CUDA.">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://sanket-pixel.github.io//blog/2024/that-first-cuda-blog/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-J2Z5HX2M1E"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-J2Z5HX2M1E');
  </script>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">Sanket Shah</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">That First CUDA Blog I Needed</h1>
    <p class="post-meta">October 20, 2024</p>
    <p class="post-tags">
      <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>
        ·  
        <a href="/blog/tag/nvidia">
          <i class="fas fa-hashtag fa-sm"></i> nvidia</a>  
          <a href="/blog/tag/cuda">
          <i class="fas fa-hashtag fa-sm"></i> cuda</a>  
          
        ·  
        <a href="/blog/category/cuda">
          <i class="fas fa-tag fa-sm"></i> cuda</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <h4 id="in-this-blog-were-going-to-dive-into-one-of-the-most-critical-concepts-in-cuda-programming-shared-memory-shared-memory-is-like-the-secret-ingredient-that-can-supercharge-your-gpu-code-while-cudas-global-memory-serves-as-the-main-storage-its-often-slow-to-access-repeatedly-thats-where-shared-memory-comes-in-it-acts-as-a-customizable-fast-access-scratchpad-where-you-can-store-data-that-is-frequently-reused-by-threads-within-the-same-block-helping-you-avoid-costly-memory-transfers-well-explore-how-this-works-why-it-matters-and-how-you-can-use-it-to-make-your-cuda-programs-much-faster">In this blog, we’re going to dive into one of the most critical concepts in CUDA programming: shared memory. Shared memory is like the secret ingredient that can supercharge your GPU code. While CUDA’s global memory serves as the main storage, it’s often slow to access repeatedly. That’s where shared memory comes in. It acts as a customizable, fast-access scratchpad where you can store data that is frequently reused by threads within the same block, helping you avoid costly memory transfers. We’ll explore how this works, why it matters, and how you can use it to make your CUDA programs much faster.</h4>

<p><br></p>
<div style="width: 80%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_8/kindergarten-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_8/kindergarten-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_8/kindergarten-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_8/kindergarten.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
   Let us begin the CUDA journey, the right way.
</div>
</div>

<h4 id="1-the-paradigm-shift-from-cpu-to-gpu-world"><strong>1. The paradigm shift from CPU to GPU World</strong></h4>

<p>A GPU is not just a faster CPU. It is rather, a hardware designed specifically for doing <strong><em>similar things, a lot of times, all at once</em></strong>. 
When one goes from programming on the CPU to the GPU, the mindset shifts from <code class="language-plaintext highlighter-rouge">"How do I do this?"</code> to <code class="language-plaintext highlighter-rouge">"How do I do this with a 1000 workers?"</code>.
The most challenging part in the beginnning is not the CUDA syntax or the intricacies of the GPU memory. But it is the shift from thinking serially or sequentially 
to thinking in parallel.</p>

<p>This shift requires you to stop thinking like a single worker doing one task after another, and start thinking like a supervisor assigning the same task to a massive team—each working on a different part of the problem at the same time. You no longer write instructions for the whole job; instead, you write instructions for one worker and trust the system to repeat it across thousands. The challenge is learning to break down problems into small, identical tasks that can run side by side—without depending on each other or getting in each other’s way.</p>

<p><br></p>
<div style="width: 80%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_8/cpu_vs_gpu-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_8/cpu_vs_gpu-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_8/cpu_vs_gpu-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_8/cpu_vs_gpu.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="cpugpu" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
   CPU: The lone chef. GPU: The parallel kitchen team.
</div>
</div>

<p>Let’s say we want to multiply each number in an array by 2.</p>

<p>On the CPU, you would think sequentially as shown below :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="k">from</span> <span class="mi">0</span> <span class="n">to</span> <span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
    <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span>
</code></pre></div></div>

<p>Here, you think like a <code class="language-plaintext highlighter-rouge">single worker</code> walking through the entire list, <code class="language-plaintext highlighter-rouge">one element at a time</code>.<br>
While on the GPU you must think of processing in parallel as shown below :</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">function</span> <span class="nf">worker</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span>

<span class="n">launch</span> <span class="n">N</span> <span class="n">workers</span><span class="p">:</span>
    <span class="n">each</span> <span class="n">runs</span> <span class="nf">worker</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">with</span> <span class="n">its</span> <span class="n">own</span> <span class="n">i</span>
</code></pre></div></div>
<p>Instead of one worker doing the whole loop, you write code for just <code class="language-plaintext highlighter-rouge">one worker</code>, and let <code class="language-plaintext highlighter-rouge">thousands of them</code> each handle their own <code class="language-plaintext highlighter-rouge">i</code> independently.
You’re no longer in control of the whole process—you’re only describing what one tiny part of the system should do. This mental shift—from controlling a loop to writing instructions for an army of workers—is what makes parallel thinking hard at first.</p>

<h4 id="2-groundwork-what-cuda-assumes-you-know"><strong>2. Groundwork: What CUDA Assumes You Know</strong></h4>
<p>If you’ve spent most of your time in languages like Python, JavaScript, or even high-level C++ without touching low-level memory concepts, CUDA will feel different. That’s because CUDA code is almost always written in C or C++, and runs in an environment where you’re much closer to the hardware. Let us understand some core low level programming concepts you should know, before finally writing our first CUDA kernel in the next section.</p>

<h5 id="21-pointers--variables-that-point-to-other-variable"><strong>2.1 Pointers : Variables that point to other Variable</strong></h5>
<p>A pointer is a variable that stores the memory address of another variable. In Python, you deal with lists and objects without thinking about where they live in memory. But in CUDA (and C/C++), you often work with memory addresses directly. In this fighure below, <code class="language-plaintext highlighter-rouge">x</code> is an integer variable that holds the value <code class="language-plaintext highlighter-rouge">10</code>, and it lives at memory address <code class="language-plaintext highlighter-rouge">0x1234</code>.
<br></p>
<div style="width: 50%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_8/pointer-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_8/pointer-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_8/pointer-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_8/pointer.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="pointer" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
   Illustration of a pointer in C++ — ptr stores the memory address of variable x, allowing indirect access to its value.
</div>
</div>

<p>The pointer <code class="language-plaintext highlighter-rouge">ptr</code> is declared to hold the address of an integer. When we assign it <code class="language-plaintext highlighter-rouge">&amp;x</code>, we’re storing the address of <code class="language-plaintext highlighter-rouge">x</code> in <code class="language-plaintext highlighter-rouge">ptr</code>. So now <code class="language-plaintext highlighter-rouge">ptr</code> contains <code class="language-plaintext highlighter-rouge">0x1234</code> — it points to <code class="language-plaintext highlighter-rouge">x</code>.</p>
<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">10</span><span class="p">;</span>
<span class="kt">int</span><span class="o">*</span> <span class="n">ptr</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">x</span><span class="p">;</span>  <span class="c1">// ptr now holds the memory address of x (e.g., 0x1234)</span>
</code></pre></div></div>
<p>The pointer itself lives at a different memory location, say <code class="language-plaintext highlighter-rouge">0x1550</code>. If we access <code class="language-plaintext highlighter-rouge">*ptr</code>, we get the value stored at the address it points to — in this case, <code class="language-plaintext highlighter-rouge">10</code>. And if we use <code class="language-plaintext highlighter-rouge">&amp;ptr</code>, we get the address where the pointer itself is stored — <code class="language-plaintext highlighter-rouge">0x1550</code>.</p>

<h5 id="22-functions--parameter-passing-by-value-vs-reference"><strong>2.2 Functions : Parameter Passing by Value vs. Reference</strong></h5>
<p>A function is a reusable block of code that performs a specific task and can take inputs (parameters) and return outputs — just like how functions work in Python.
In C, when you <code class="language-plaintext highlighter-rouge">pass by value</code>, the function gets a <code class="language-plaintext highlighter-rouge">copy of the data</code>. When you <code class="language-plaintext highlighter-rouge">pass by reference</code> (using a pointer), the function gets access to the <code class="language-plaintext highlighter-rouge">original</code>, so it can modify it.</p>
<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">modify</span><span class="p">(</span><span class="kt">int</span> <span class="n">x</span><span class="p">);</span>     <span class="c1">// gets a copy of x</span>
<span class="kt">void</span> <span class="nf">modify</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">x</span><span class="p">);</span>    <span class="c1">// gets the original x via address</span>
</code></pre></div></div>

<p>Passing by value is like giving someone a photocopy of a document, while passing by reference is like giving them the original paper to make changes on.</p>

<h5 id="23-arrays-and-memory-layout"><strong>2.3 Arrays and Memory Layout</strong></h5>
<p>In high-level languages, arrays feel like magical lists, but under the hood, an array is just a block of memory where all elements sit <code class="language-plaintext highlighter-rouge">side by side</code>.
This figure below illustrates how array elements, like integers, are stored contiguously in memory. Each element occupies a specific block of memory, and for <code class="language-plaintext highlighter-rouge">int</code> types, these blocks are typically separated by <code class="language-plaintext highlighter-rouge">4 bytes</code>, allowing precise calculation of each element’s address from the array’s start.</p>
<div style="width: 70%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_8/array.svg-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_8/array.svg-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_8/array.svg-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_8/array.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="array" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Memory layout of a 4-element integer array with 4-byte spacing.
</div>
</div>

<p>Because array elements are stored contiguously in memory, a pointer to the first element can be used to access any other element using pointer arithmetic. If <code class="language-plaintext highlighter-rouge">ptr</code> points to the start of the array, <code class="language-plaintext highlighter-rouge">ptr + i</code> moves the pointer <code class="language-plaintext highlighter-rouge">i</code> elements forward (not bytes — it accounts for the size of each element).
For example, to increment the third element (index 2):</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">arr</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">};</span>
<span class="kt">int</span> <span class="o">*</span><span class="n">ptr</span> <span class="o">=</span> <span class="n">arr</span><span class="p">;</span>

<span class="n">ptr</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">ptr</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>  <span class="c1">// arr[2] becomes 4</span>
</code></pre></div></div>
<p>Here, <code class="language-plaintext highlighter-rouge">ptr[2]</code> accesses the third element of the array, just like <code class="language-plaintext highlighter-rouge">arr[2]</code> would. This highlights the deep connection between arrays and pointers in C.</p>

<h4 id="24-2d-arrays-and-memory-layout"><strong>2.4 2D Arrays and Memory Layout</strong></h4>
<p>In C/C++, 2D arrays are stored in row-major order — meaning all elements of the first row come first in memory, followed by the second row, and so on. So even though we access elements using two indices (row and column), in memory it’s just a flat, contiguous block. This is illustrated in the figure below.</p>
<div style="width: 90%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_8/matrix.svg-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_8/matrix.svg-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_8/matrix.svg-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_8/matrix.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="matrix" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
2D matrices, like this (3,4) example, are arranged row-major and stored contiguously in memory.
</div>
</div>

<p>You can calculate the memory index of any element using:</p>
<blockquote>
  <p><code class="language-plaintext highlighter-rouge">index = row * num_columns + col</code></p>
</blockquote>

<p>In the code below, <code class="language-plaintext highlighter-rouge">ptr[9]</code> accesses the same memory location as<code class="language-plaintext highlighter-rouge">matrix[2][1]</code>, because it is the 10th element in the row-major flattened memory layout (starting from index 0).</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">matrix</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
    <span class="p">{</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">3</span><span class="p">},</span>
    <span class="p">{</span><span class="mi">11</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">},</span>
    <span class="p">{</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">}</span>
<span class="p">};</span>
<span class="kt">int</span> <span class="o">*</span><span class="n">ptr</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">];</span>
<span class="kt">int</span> <span class="n">r</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">num_columns</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">r</span> <span class="o">*</span> <span class="n">num_columns</span> <span class="o">+</span> <span class="n">c</span><span class="p">;</span>
<span class="n">printf</span><span class="p">(</span><span class="s">"%d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">ptr</span><span class="p">[</span><span class="n">index</span><span class="p">]);</span>  <span class="c1">// Output: 3</span>
</code></pre></div></div>

<h5 id="25-stack-vs-heap-memory"><strong>2.5 Stack vs Heap Memory</strong></h5>
<p>This is often overlooked but important. In C/C++, small, fixed-size variables (like integers or small structs) are stored on the <code class="language-plaintext highlighter-rouge">stack</code> — a <code class="language-plaintext highlighter-rouge">fast</code>, temporary memory area that <code class="language-plaintext highlighter-rouge">automatically manages variable lifetime</code>.</p>

<p>In contrast, <code class="language-plaintext highlighter-rouge">dynamically allocated</code> memory (such as arrays created with <code class="language-plaintext highlighter-rouge">malloc</code> or <code class="language-plaintext highlighter-rouge">new</code>) lives on the <code class="language-plaintext highlighter-rouge">heap</code>, which is larger but slower and must be <code class="language-plaintext highlighter-rouge">manually managed</code> (allocated and freed).</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">10</span><span class="p">;</span>                 <span class="c1">// Stored on the stack</span>
<span class="kt">int</span><span class="o">*</span> <span class="n">arr</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="mi">10</span> <span class="o">*</span> <span class="nf">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>  <span class="c1">// Allocated on the heap</span>
</code></pre></div></div>

<p>We have now laid the essential groundwork of concepts—pointers, functions, memory layout, stack, and heap—on which CUDA programming is built.
Understanding these basics will make your journey into parallel programming much smoother.</p>

<h4 id="3-your-first-cuda-kernel--hello-world"><strong>3. Your First CUDA Kernel : Hello World!</strong></h4>

<p>Now that we have a solid understanding of the foundational concepts, let’s dive into writing our very first CUDA kernel. The goal here isn’t complex computation, but to bridge the gap between CPU-style sequential thinking and GPU-style parallel execution, and to see your GPU actually <em>do something</em>.</p>

<p>A good way to learn something new, is to begin from something you already know and then connect the dots. Let us first look at a simple <code class="language-plaintext highlighter-rouge">Hello World</code> program in C++.</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span><span class="kt">int</span> <span class="nf">main</span><span class="p">(){</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Hello World!"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>
<p>To execute this program on your machine, follow the following steps :</p>

<ol>
  <li>Clone the <a href="https://github.com/sanket-pixel/blog_code" rel="external nofollow noopener" target="_blank">repository</a> that stores the code associated with my blogs :
    <div class="language-bash highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>git clone https://github.com/sanket-pixel/blog_code
<span class="nb">cd </span>blog_code
</code></pre></div>    </div>
  </li>
  <li>Navigate to the directory <code class="language-plaintext highlighter-rouge">8_that_first_cuda_blog/1_hello_world</code>
    <div class="language-bash highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>8_that_first_cuda_blog/1_hello_world
</code></pre></div>    </div>
  </li>
  <li>Compile the program using the following command
    <div class="language-bash highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>g++ hello_world_cpu.cpp <span class="nt">-o</span> hello_world_cpu
</code></pre></div>    </div>
  </li>
  <li>This will create an executable hello_world in this directory. Execute it using
    <div class="language-bash highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>./hello_world_cpu
</code></pre></div>    </div>
  </li>
</ol>

<p>This should print <code class="language-plaintext highlighter-rouge">Hello World from CPU!</code> in the terminal, as expected. Here, the g++ compiler translates your C++ code into instructions that the CPU can execute directly.</p>

<p>Finally, let’s write our first CUDA kernel that performs the same task — printing “Hello World” — but this time the message will come from the GPU.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="c1">  // This include statement allows us to use cuda library in our code</span><span class="cp">
</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">gpu_hello_world</span><span class="p">(){</span>
  <span class="n">printf</span><span class="p">(</span><span class="s">"Hello World from GPU! </span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(){</span>
  <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Hello World from CPU!"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
  <span class="n">gpu_hello_world</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span>
  <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div>

<p>To execute this program on your machine, follow the following steps :</p>

<ol>
  <li>Navigate to the directory <code class="language-plaintext highlighter-rouge">8_that_first_cuda_blog/1_hello_world</code>
    <div class="language-bash highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>8_that_first_cuda_blog/1_hello_world
</code></pre></div>    </div>
  </li>
  <li>Compile the program using the following command
    <div class="language-bash highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>nvcc hello_world_gpu.cu <span class="nt">-o</span> hello_world_gpu
</code></pre></div>    </div>
  </li>
  <li>This will create an executable hello_world in this directory. Execute it using
    <div class="language-bash highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>./hello_world_gpu
</code></pre></div>    </div>
  </li>
  <li>The output should be the following
    <div class="language-plaintext highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>Hello World from CPU!
Hello World from GPU! 
</code></pre></div>    </div>
  </li>
</ol>

<p>Now that we have our first CUDA program running, let us dissect this CUDA program, and understand how it works from first principles. <br></p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">gpu_hello_world</span><span class="p">(){</span>
  <span class="n">printf</span><span class="p">(</span><span class="s">"Hello World from GPU! </span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The code snippet above is a function that is intended to run on the GPU. <br> 
In the GPU jargon, such a function is called <strong>kernel</strong>. <br></p>

<p>Kernel, specifically, is a special function, that can be invoked from the CPU, but runs only on the GPU.
CPU, is generally referred to as <code class="language-plaintext highlighter-rouge">host</code> and GPU is referred to as <code class="language-plaintext highlighter-rouge">device</code>, since the CPU hosts the GPU in some sense.
The <code class="language-plaintext highlighter-rouge">__global__</code> keyword is used to specify that this function is a <strong>kernel</strong>, in that, it can be called from the host but executed on the device.</p>

<p><code class="language-plaintext highlighter-rouge">gpu_hello_world&lt;&lt;&lt;1,1&gt;&gt;&gt;();</code> is a CUDA-specific syntax. We will discuss what <code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;1,1&gt;&gt;&gt;</code> means later in this blog. 
For now it is sufficient to understand that <code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;1,1&gt;&gt;&gt;</code>, allocates 1 thread for executing this kernel.</p>

<p>Let us now extend our single thread CUDA Hello World, to run it with 8 threads. We would like the GPU to repeat this same <code class="language-plaintext highlighter-rouge">Hello World from GPU</code> operation 8 times. Just one small change in our original code will make this happen.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">gpu_hello_world</span><span class="p">(){</span>
  <span class="n">printf</span><span class="p">(</span><span class="s">"Hello World from GPU! </span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(){</span>
  <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Hello World from CPU!"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
  <span class="c1">// HERE , we replace &lt;&lt;&lt;1,1&gt;&gt;&gt; with &lt;&lt;&lt;1,8&gt;&gt;&gt;.</span>
  <span class="n">gpu_hello_world</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span>
  <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The output of this code, will be one <code class="language-plaintext highlighter-rouge">Hello World from CPU!</code> and 8 <code class="language-plaintext highlighter-rouge">Hello World from GPU!</code>s. 
The main change as explained in the comment above the kernel code is replace <code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;1,1&gt;&gt;&gt;</code> with <code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;1,8&gt;&gt;&gt;</code>, which essentially
means launching the same kernel with 8 threads. The GPU runs 8 “print Hello World” operations in parallel.</p>

<p>We will understand what <code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;1,8&gt;&gt;&gt;</code> exactly means in absolute detail, but at this point, it is sufficient to understand that <code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;1,1&gt;&gt;&gt;</code>
launches one thread and <code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;1,8&gt;&gt;&gt;</code> launches 8 threads in parallel.</p>

<p>In summary, in this <strong>Hello World</strong> section, we first looked at how to print Hello World using the CPU, followed by the same using the GPU.
The major takeaway from this section is to understand what are kernels in general, and how <em>exactly</em> is a kernel launched from the <code class="language-plaintext highlighter-rouge">host</code>, to run the same operations in parallel on the <code class="language-plaintext highlighter-rouge">device</code>.</p>

<h4 id="4-thread-organization-in-cuda"><strong>4. Thread Organization in CUDA</strong></h4>

<p>In the previous section, we briefly saw this line , <code class="language-plaintext highlighter-rouge">gpu_hello_world&lt;&lt;&lt;1,8&gt;&gt;&gt;();</code>. We used it without explaining what <code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;1,8&gt;&gt;&gt;</code> means. To truly understand CUDA programming, it’s important to unpack this syntax. This leads us to understanding how threads are organized in CUDA.</p>

<h5 id="41-heirarchy-in-cuda"><strong>4.1 Heirarchy in CUDA</strong></h5>
<p>When you launch a CUDA kernel, you typically launch many threads, not just one. These threads are organized in a hierarchical structure as shown in the figure below.</p>

<div style="width: 70%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_8/heir.svg-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_8/heir.svg-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_8/heir.svg-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_8/heir.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="heirarchy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
CUDA hierarchy: A grid contains blocks, and each block contains threads.
</div>
</div>

<p>Each kernel launch, creates one <em>Grid</em>, which contains many <em>Blocks</em>, which in turn contains many <em>Threads</em>. <br>
To understand how the threads, blocks and grids are organized, let us use the analogy of a building with <em>3 floors</em>, and with <em>4 apartments</em> on each floor. Thus we have 12 apartments in total.</p>
<blockquote>
  <p>Each <em>thread</em> is like an <em>apartment</em> (flat) on the floor.<br>
Each <em>block</em> is like a <em>floor</em> in the building.<br>
The <em>grid</em> is the entire <em>building</em>.</p>
</blockquote>

<p>This maps to <code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;3, 4&gt;&gt;&gt;</code> in CUDA, which means launching 3 blocks, each containing 4 threads — totaling 12 threads. The two parameters inside the kernel launch syntax <code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt;</code> represent the number of blocks in the grid (<code class="language-plaintext highlighter-rouge">gridDim</code>) and the number of threads in each block(<code class="language-plaintext highlighter-rouge">blockDim</code>).</p>

<h5 id="42-finding-global-index-of-a-thread"><strong>4.2 Finding global index of a thread</strong></h5>
<p>In CUDA, thousands of threads may run in parallel, and each thread typically processes a different portion of data—like one element in an array. To do this correctly, each thread must know <em>exactly which piece of data it’s responsible for</em>. That’s where the <em>global thread index</em> comes in: it gives every thread a unique ID across the entire grid so it can access the correct memory location.</p>

<div style="width: 90%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_8/building.svg-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_8/building.svg-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_8/building.svg-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_8/building.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="matrix" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
2D matrices, like this (3,4) example, are arranged row-major and stored contiguously in memory.
</div>
</div>

<p>Extending our analogy as shown above, if every flat needs a unique global ID across the entire building, we do this:<br>
I’m on floor <code class="language-plaintext highlighter-rouge">floorIdx</code>, in flat <code class="language-plaintext highlighter-rouge">flatIdx</code>. Multiply the floor number by the number of flats per floor (<code class="language-plaintext highlighter-rouge">floorDim</code>) and add the flat index <code class="language-plaintext highlighter-rouge">flatIdx</code>.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">global_flat_id</span> <span class="o">=</span> <span class="n">floorIdx</span> <span class="o">*</span> <span class="n">floorDim</span> <span class="o">+</span> <span class="n">flatIdx</span><span class="p">;</span>
</code></pre></div></div>

<p>For example :</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Flat 2 on Floor 1: 1 * 4 + 2 = 6
Flat 3 on Floor 2: 2 * 4 + 3 = 11
</code></pre></div></div>
<p>In the figure above, <code class="language-plaintext highlighter-rouge">GID</code> refers to the <code class="language-plaintext highlighter-rouge">global_flat_id</code> . As shown on the left, every flat has a unique ID ranging from 0 to 11.</p>

<p>Along similar lines, to get the <em>global thread index</em>, we can use the <code class="language-plaintext highlighter-rouge">blockIdx</code>, <code class="language-plaintext highlighter-rouge">blockDim</code> and <code class="language-plaintext highlighter-rouge">threadIdx</code>. CUDA gives us built-in variables 
to retrieve this information:</p>
<blockquote>
  <p><code class="language-plaintext highlighter-rouge">blockIdx.x</code> → which block (floor) you’re in<br>
<code class="language-plaintext highlighter-rouge">threadIdx.x</code> → which thread (room) inside the block<br>
<code class="language-plaintext highlighter-rouge">blockDim.x</code> → how many threads per block (rooms per floor)</p>
</blockquote>

<p>Using these, every thread can compute its global ID using:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">global_thread_id</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</code></pre></div></div>
<p>This index lets the thread determine exactly which data element to work on.<br>
Let’s look at a simple example where each thread doubles one element in an array:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">double_elements</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">arr</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="n">arr</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>
<p>In this kernel, every thread calculates its global index and uses it to access the correct element in <code class="language-plaintext highlighter-rouge">arr</code>. Thanks to this indexing, all threads operate independently, and no two threads overwrite each other’s work.</p>

<h5 id="43-a-hands-on-example-squaring-numbers-in-parallel"><strong>4.3 A Hands-on Example: Squaring Numbers in Parallel</strong></h5>
<p>Let us now walk through a concrete example to reinforce what we’ve learned about thread organization in CUDA. This example not only demonstrates how threads are structured using blocks and grids but also highlights some important subtleties that apply to most CUDA kernels you will write.</p>

<p>We’ll square numbers from 0 to 9 in parallel. Each thread computes its global ID and prints the square of its assigned number.</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span>
<span class="cp">#define THREADS_PER_BLOCK 4
#define DATA_SIZE 10
</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">print_square</span><span class="p">()</span> <span class="p">{</span>
  <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">id</span> <span class="o">&lt;</span> <span class="n">DATA_SIZE</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"Thread %d: %d squared = %d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">id</span> <span class="o">*</span> <span class="n">id</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="c1">// compute how many blocks we need</span>
  <span class="kt">int</span> <span class="n">blocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">DATA_SIZE</span> <span class="o">+</span> <span class="n">THREADS_PER_BLOCK</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">THREADS_PER_BLOCK</span><span class="p">;</span>
  <span class="n">print_square</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span> <span class="n">THREADS_PER_BLOCK</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span>
  <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div>
<p>How to run it:</p>
<ol>
  <li>Navigate to the directory:
    <div class="language-bash highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>8_that_first_cuda_blog/2_squared_numbers
</code></pre></div>    </div>
  </li>
  <li>Compile the program:
    <div class="language-bash highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>nvcc squared_numbers.cu <span class="nt">-o</span> squared_numbers
</code></pre></div>    </div>
  </li>
  <li>Run the executable:
    <div class="language-bash highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>./squared_numbers
</code></pre></div>    </div>
  </li>
  <li>You’ll see output like:
    <div class="language-plaintext highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>Thread 0: 0 squared = 0
Thread 1: 1 squared = 1
Thread 2: 2 squared = 4
// and so on...
</code></pre></div>    </div>
  </li>
</ol>

<p>This diagram below visually explains how CUDA computes each thread’s global ID using <code class="language-plaintext highlighter-rouge">blockIdx.x</code>, <code class="language-plaintext highlighter-rouge">threadIdx.x</code>, and <code class="language-plaintext highlighter-rouge">blockDim.x</code>.  It also shows which threads execute the computation based on the condition <code class="language-plaintext highlighter-rouge">if (id &lt; DATA_SIZE)</code>.</p>

<div style="width: 80%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_8/square.svg-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_8/square.svg-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_8/square.svg-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_8/square.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="square" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
2D matrices, like this (3,4) example, are arranged row-major and stored contiguously in memory.
</div>
</div>

<p>Let’s break down the key ideas:</p>
<ul>
  <li>We define two constants at the top:
    <ul>
      <li>
<code class="language-plaintext highlighter-rouge">THREADS_PER_BLOCK = 4</code> → Each block contains 4 threads.</li>
      <li>
<code class="language-plaintext highlighter-rouge">DATA_SIZE = 10</code> → We want to square numbers from 0 to 9.<br>
<br>
</li>
    </ul>
  </li>
  <li>In <code class="language-plaintext highlighter-rouge">main()</code>, we compute how many blocks we need. As shown in the figure above, <code class="language-plaintext highlighter-rouge">int blocks =3</code>:
    <div class="language-cpp highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">blocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">DATA_SIZE</span> <span class="o">+</span> <span class="n">THREADS_PER_BLOCK</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">THREADS_PER_BLOCK</span><span class="p">;</span>
</code></pre></div>    </div>
  </li>
  <li>We launch the kernel:
    <div class="language-cpp highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="n">print_square</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span> <span class="n">THREADS_PER_BLOCK</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span>
</code></pre></div>    </div>
  </li>
  <li>Inside the kernel, each thread calculates its global ID:
    <div class="language-cpp highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</code></pre></div>    </div>
  </li>
  <li>The check <code class="language-plaintext highlighter-rouge">if (id &lt; DATA_SIZE)</code> ensures that no thread attempts to process an index beyond the data size — an essential safeguard when the total number of threads exceeds the amount of data.</li>
</ul>

<h5 id="44-thread-and-block-ids-in-2d-and-3d"><strong>4.4 Thread and Block IDs in 2D and 3D</strong></h5>
<p>So far, we’ve been using variables like <code class="language-plaintext highlighter-rouge">threadIdx.x</code>, <code class="language-plaintext highlighter-rouge">blockIdx.x</code>, and <code class="language-plaintext highlighter-rouge">blockDim.x</code> in our CUDA kernels — but we haven’t really explored what the <code class="language-plaintext highlighter-rouge">.x</code> means, or what happens when we go beyond 1D.</p>

<p>Understanding how threads and blocks are structured in <em>multiple dimensions (2D and 3D)</em> is essential because many real-world problems are naturally multi-dimensional — like image processing (2D pixels), volumetric data (3D grids), or matrix operations. CUDA’s thread hierarchy lets us mirror this structure directly, so we can write cleaner, more intuitive code.</p>

<p>In CUDA, both threads and blocks can be organized in 1D, 2D, or 3D layouts. That means:</p>
<ul>
  <li>Each <em>block</em> can have threads arranged like a line (1D), a grid (2D), or a cube (3D).</li>
  <li>Similarly, the <em>grid of blocks</em> itself can follow any of these layouts.</li>
</ul>

<p>Each thread and block has 3 coordinate components:</p>
<ul>
  <li>
<code class="language-plaintext highlighter-rouge">threadIdx.x</code>, <code class="language-plaintext highlighter-rouge">threadIdx.y</code>, <code class="language-plaintext highlighter-rouge">threadIdx.z</code> tell you the thread’s position <strong>within its block</strong>.</li>
  <li>
<code class="language-plaintext highlighter-rouge">blockIdx.x</code>, <code class="language-plaintext highlighter-rouge">blockIdx.y</code>, <code class="language-plaintext highlighter-rouge">blockIdx.z</code> tell you the block’s position <strong>within the grid</strong>
</li>
  <li>
<code class="language-plaintext highlighter-rouge">blockDim.x</code>, <code class="language-plaintext highlighter-rouge">blockDim.y</code>, <code class="language-plaintext highlighter-rouge">blockDim.z</code> tell you how many threads are there <strong>per block</strong> in each direction</li>
</ul>

<p>This gives every thread a unique multi-dimensional ID — but to index into flat memory like an array, we typically compute a flattened 1D global index from these coordinates.</p>

<p><strong>2D grid and blocks</strong><br>
Let’s say each block is a 2D grid of threads and the entire grid has a 2D arrangement of blocks. Then, we compute a <em>flattened global thread ID</em> like this:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">y</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</code></pre></div></div>
<p>You now have coordinates <code class="language-plaintext highlighter-rouge">(x, y)</code> representing the thread’s unique position in the global 2D grid. If you want to flatten this into a 1D index (e.g. for array access), you can do:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">total_width</span> <span class="o">+</span> <span class="n">x</span><span class="p">;</span>  <span class="c1">// where total_width is the width of the full grid</span>
</code></pre></div></div>

<p><strong>3D grid and blocks</strong><br>
In case the grid has blocks in 3D, and the blocks have threads in 3D,</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">y</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">z</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">z</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">z</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">z</span><span class="p">;</span>
</code></pre></div></div>
<p>And to get a linear index:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">z</span> <span class="o">*</span> <span class="p">(</span><span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">x</span><span class="p">;</span>
</code></pre></div></div>

<p>These computations let each thread know <em>exactly what data to work on</em>, even in multi-dimensional problems. CUDA doesn’t care if you use 1D, 2D, or 3D — it just provides the structure so you can map the problem domain naturally and write code that’s easier to reason about.</p>
<blockquote>
  <p>TL;DR: Think of .x, .y, and .z as the coordinates in a virtual 3D thread universe. You use them to uniquely identify and assign work to each thread, especially in problems where your data naturally lives in 2D or 3D.</p>
</blockquote>

<p>Let’s Understand This with a Concrete 2D Example</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span>
<span class="cp">#define WIDTH 10      // width of simulated 2D data (columns)
#define HEIGHT 5      // height of simulated 2D data (rows)
</span>
<span class="cp">#define THREADS_X 4   // threads per block in X
#define THREADS_Y 2   // threads per block in Y
</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">print_2d_coordinates</span><span class="p">()</span> <span class="p">{</span>
  <span class="c1">// Get global 2D index</span>
  <span class="kt">int</span> <span class="n">global_x</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span>  <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">global_y</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>

  <span class="c1">// Compute linear ID (row-major order)</span>
  <span class="kt">int</span> <span class="n">global_id</span> <span class="o">=</span> <span class="n">global_y</span> <span class="o">*</span> <span class="n">WIDTH</span> <span class="o">+</span> <span class="n">global_x</span><span class="p">;</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">global_x</span> <span class="o">&lt;</span> <span class="n">WIDTH</span> <span class="o">&amp;&amp;</span> <span class="n">global_y</span> <span class="o">&lt;</span> <span class="n">HEIGHT</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"Block (%d,%d) Thread (%d,%d) → Global ID: %2d → Pixel (%d,%d)</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span>
           <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">global_id</span><span class="p">,</span> <span class="n">global_y</span><span class="p">,</span> <span class="n">global_x</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="n">dim3</span> <span class="n">threads_per_block</span><span class="p">(</span><span class="n">THREADS_X</span><span class="p">,</span> <span class="n">THREADS_Y</span><span class="p">);</span>

  <span class="kt">int</span> <span class="n">blocks_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">WIDTH</span> <span class="o">+</span> <span class="n">THREADS_X</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">THREADS_X</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">blocks_y</span> <span class="o">=</span> <span class="p">(</span><span class="n">HEIGHT</span> <span class="o">+</span> <span class="n">THREADS_Y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">THREADS_Y</span><span class="p">;</span>
  <span class="n">dim3</span> <span class="n">num_blocks</span><span class="p">(</span><span class="n">blocks_x</span><span class="p">,</span> <span class="n">blocks_y</span><span class="p">);</span>

  <span class="n">print_2d_coordinates</span><span class="o">&lt;&lt;&lt;</span><span class="n">num_blocks</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span>
  <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div>

<p>To execute this program on your machine, follow the following steps :</p>

<ol>
  <li>Navigate to the directory <em>8_that_first_cuda_blog/3_print_2d_coordinates</em>
    <div class="language-bash highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>8_that_first_cuda_blog/3_print_2d_coordinates
</code></pre></div>    </div>
  </li>
  <li>Compile the program using the following command
    <div class="language-bash highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>nvcc print_2d_coordinates.cu <span class="nt">-o</span> print_2d_coordinates
</code></pre></div>    </div>
  </li>
  <li>This will create an executable <code class="language-plaintext highlighter-rouge">print_2d_coordinates</code> in this directory. Execute it using
    <div class="language-bash highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>./print_2d_coordinates
</code></pre></div>    </div>
  </li>
  <li>The output will start with something like :
    <div class="language-plaintext highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>Block (2,2) Thread (0,0) → Global ID: 48 → Pixel (4,8)
Block (2,2) Thread (0,1) → Global ID: 49 → Pixel (4,9)
Block (2,1) Thread (0,0) → Global ID: 44 → Pixel (4,4)
Block (2,1) Thread (0,1) → Global ID: 45 → Pixel (4,5)
Block (2,1) Thread (0,2) → Global ID: 46 → Pixel (4,6)
Block (2,1) Thread (0,3) → Global ID: 47 → Pixel (4,7)
// and so on..
</code></pre></div>    </div>
  </li>
</ol>

<p>In this code :</p>
<ul>
  <li>We’re simulating a <strong>2D image</strong> of size <code class="language-plaintext highlighter-rouge">10 x 5</code> (10 columns × 5 rows).</li>
  <li>Each block is configured to launch 4 x 2 threads. ( blockDim.x=4, blockDim.y=2)\</li>
  <li>Based on the data dimensions and thread layout, we calculate how many blocks we need in X and Y directions:
    <div class="language-cpp highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">blocks_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">WIDTH</span> <span class="o">+</span> <span class="n">THREADS_X</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">THREADS_X</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">blocks_y</span> <span class="o">=</span> <span class="p">(</span><span class="n">HEIGHT</span> <span class="o">+</span> <span class="n">THREADS_Y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">THREADS_Y</span><span class="p">;</span>
</code></pre></div>    </div>
  </li>
  <li>Inside the kernel, each thread computes:
    <ul>
      <li>Its global X and Y position using its thread and block indices</li>
      <li>A linear global ID, assuming row-major layout</li>
    </ul>

    <div class="language-cpp highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>  <span class="c1">// Get global 2D index</span>
  <span class="kt">int</span> <span class="n">global_x</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span>  <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">global_y</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>

  <span class="c1">// Compute linear ID (row-major order)</span>
  <span class="kt">int</span> <span class="n">global_id</span> <span class="o">=</span> <span class="n">global_y</span> <span class="o">*</span> <span class="n">WIDTH</span> <span class="o">+</span> <span class="n">global_x</span><span class="p">;</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>The <code class="language-plaintext highlighter-rouge">if</code> condition:</p>

    <div class="language-cpp highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>  <span class="k">if</span> <span class="p">(</span><span class="n">global_x</span> <span class="o">&lt;</span> <span class="n">WIDTH</span> <span class="o">&amp;&amp;</span> <span class="n">global_y</span> <span class="o">&lt;</span> <span class="n">HEIGHT</span><span class="p">)</span>
</code></pre></div>    </div>
    <p>ensures that only threads within bounds print output. Without this, some threads (especially in the last block row/column) might run out-of-bounds and access invalid pixels.</p>
  </li>
  <li>In the earlier 1D example, we used plain integers like <code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;</code>. In this 2D case, we use <code class="language-plaintext highlighter-rouge">dim3</code> to pass 2D configurations — a CUDA struct that lets us naturally map threads to 1D, 2D, or 3D data layouts.</li>
</ul>

<p>That wraps up our section on thread organization — we now understand how threads and blocks are structured and how they map to data. Whether 1D or 2D, it’s all about aligning the thread layout to your problem.</p>

<p>Next, we’ll explore how memory works in CUDA — including how to allocate memory on the GPU and transfer data between the CPU and GPU.</p>

<h4 id="5-managing-data-from-cpu-to-gpu-and-back">5. Managing Data: From CPU to GPU and Back</h4>
<p>In this section, we’ll understand how data moves between the CPU and GPU, and why explicit memory management is crucial in CUDA programming.</p>

<p>Here, we only see the essentials, but you can check out my previous blog post <a href="/blog/2024/down-the-cudamemory-lane">Down the CUDA Memory Lane</a> for a deep dive into CUDA Memory.</p>

<p>Before diving deeper, it’s important to understand a key idea —<strong>the CPU (host) and GPU (device) have separate memory spaces</strong>. They do not share memory by default and cannot directly access each other’s data. If you want the GPU to work on data from the CPU (or send results back), <strong>you must explicitly transfer that data between the two.</strong></p>

<p>There are a few subtle but essential things to keep in mind:</p>
<ul>
  <li>Simply defining an array on the CPU doesn’t make it visible to the GPU.</li>
  <li>The GPU cannot allocate or manage host memory directly.</li>
  <li>Data must be copied from host to device before the kernel runs, and back afterward if needed.</li>
  <li>Memory transfers are relatively expensive compared to computation, so minimizing them is often important for performance.</li>
</ul>

<p>In this section, we’ll <strong>briefly summarize how to allocate memory using <code class="language-plaintext highlighter-rouge">cudaMalloc</code> and copy data with <code class="language-plaintext highlighter-rouge">cudaMemcpy</code></strong>, just enough to get us started with real examples.</p>

<p>As always, lets first directly look at a working example and then understand these memory management thought that.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
    <span class="c1">// Step 1: Allocate Memory on the CPU for an Integer Array of Size 10</span>
    <span class="kt">size_t</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span>

    <span class="c1">// Allocate memory on the CPU (host)</span>
    <span class="kt">int</span> <span class="o">*</span><span class="n">h_array</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span> <span class="n">malloc</span><span class="p">(</span><span class="n">size</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">h_array</span> <span class="o">==</span> <span class="nb">nullptr</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">"Failed to allocate CPU memory"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
        <span class="k">return</span> <span class="n">EXIT_FAILURE</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// Step 2: Allocate Memory on the GPU for an Integer Array of Size 10</span>
    <span class="kt">int</span> <span class="o">*</span><span class="n">d_array</span><span class="p">;</span>
    <span class="n">cudaError_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_array</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">err</span> <span class="o">!=</span> <span class="n">cudaSuccess</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">"Failed to allocate GPU memory: "</span> <span class="o">&lt;&lt;</span> <span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
        <span class="n">free</span><span class="p">(</span><span class="n">h_array</span><span class="p">);</span> <span class="c1">// Free CPU memory if GPU allocation fails</span>
        <span class="k">return</span> <span class="n">EXIT_FAILURE</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// Step 3: Initialize the CPU Array with Squares of Integers from 1 to 10</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">h_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// Squares of integers from 1 to 10</span>
    <span class="p">}</span>

    <span class="c1">// Step 4: Copy Data from the CPU to the GPU</span>
    <span class="n">err</span> <span class="o">=</span> <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_array</span><span class="p">,</span> <span class="n">h_array</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">err</span> <span class="o">!=</span> <span class="n">cudaSuccess</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">"Failed to copy data from CPU to GPU: "</span> <span class="o">&lt;&lt;</span> <span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
        <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_array</span><span class="p">);</span> <span class="c1">// Free GPU memory if copy fails</span>
        <span class="n">free</span><span class="p">(</span><span class="n">h_array</span><span class="p">);</span>     <span class="c1">// Free CPU memory</span>
        <span class="k">return</span> <span class="n">EXIT_FAILURE</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// Step 5: Copy the Data Back from the GPU to the CPU</span>
    <span class="n">err</span> <span class="o">=</span> <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">h_array</span><span class="p">,</span> <span class="n">d_array</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">err</span> <span class="o">!=</span> <span class="n">cudaSuccess</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">"Failed to copy data from GPU to CPU: "</span> <span class="o">&lt;&lt;</span> <span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
        <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_array</span><span class="p">);</span> <span class="c1">// Free GPU memory if copy fails</span>
        <span class="n">free</span><span class="p">(</span><span class="n">h_array</span><span class="p">);</span>     <span class="c1">// Free CPU memory</span>
        <span class="k">return</span> <span class="n">EXIT_FAILURE</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// Step 6: Print Values to Verify It All Works</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">h_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

    <span class="c1">// Free the allocated memory</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_array</span><span class="p">);</span>
    <span class="n">free</span><span class="p">(</span><span class="n">h_array</span><span class="p">);</span>

    <span class="k">return</span> <span class="n">EXIT_SUCCESS</span><span class="p">;</span>
<span class="p">}</span>


</code></pre></div></div>

<p>In this example, we prepare a list of 10 numbers on the CPU, send it to the GPU, bring it back, and check that everything transferred correctly.
To achieve this, we perform the following operations :
In this example, we will perform the following operations:</p>

<ol>
  <li>Allocate memory on the CPU for an integer array of size 10
    <div class="language-cpp highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="o">*</span><span class="n">h_array</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span> <span class="n">malloc</span><span class="p">(</span><span class="n">size</span><span class="p">);</span>
</code></pre></div>    </div>
    <p><code class="language-plaintext highlighter-rouge">malloc</code>, short for “memory allocation,” is a standard C/C++ function that reserves a block of memory on the CPU.</p>
  </li>
  <li>Allocate memory on the GPU for an integer array of size 10
    <div class="language-cpp highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="n">cudaError_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_array</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
</code></pre></div>    </div>
    <p><code class="language-plaintext highlighter-rouge">cudaMalloc</code> is the CUDA equivalent of <code class="language-plaintext highlighter-rouge">malloc</code>, used to allocate memory on the GPU.Even though d_array is created in CPU code, it doesn’t store a regular number — it stores the address of memory that lives on the GPU. It’s like a note on the CPU that says, <em>“Hey, the actual data is over there on the GPU.”</em> The function takes a pointer to the pointer (&amp;d_array) and the size in bytes.<br> <br>
Since <code class="language-plaintext highlighter-rouge">cudaMalloc</code> expects a <code class="language-plaintext highlighter-rouge">void**</code>, we pass the address of the pointer (<code class="language-plaintext highlighter-rouge">&amp;d_array</code>) so it can fill it with the location of the allocated GPU memory. Think of it like this: we’re giving CUDA a place to <em>write down the GPU address</em>, and after the call, <code class="language-plaintext highlighter-rouge">d_array</code> will hold the actual memory location on the GPU. It’s a bit of a tongue twister — passing the address of an address — but that’s how the pointer gets set correctly.</p>
  </li>
  <li>Initialize the CPU array with squares of integers from 1 to 10
    <div class="language-cpp highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
     <span class="n">h_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// Squares of integers from 1 to 10</span>
 <span class="p">}</span>
</code></pre></div>    </div>
  </li>
  <li>Copy the data from the CPU to the GPU
    <div class="language-cpp highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="n">err</span> <span class="o">=</span> <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_array</span><span class="p">,</span> <span class="n">h_array</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</code></pre></div>    </div>
    <p><code class="language-plaintext highlighter-rouge">cudaMemcpy</code> is used to transfer data between the CPU and GPU. Here, we’re copying data from the CPU array <code class="language-plaintext highlighter-rouge">h_array</code> to the GPU array <code class="language-plaintext highlighter-rouge">d_array</code>. The last argument, <code class="language-plaintext highlighter-rouge">cudaMemcpyHostToDevice</code>, tells CUDA the direction of the copy. This step is crucial — the GPU can’t access CPU memory directly, so we have to send it over manually.</p>
  </li>
  <li>Copy the data back from the GPU to the CPU
    <div class="language-cpp highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="n">err</span> <span class="o">=</span> <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">h_array</span><span class="p">,</span> <span class="n">d_array</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
</code></pre></div>    </div>
    <p>This line copies the results from GPU memory (<code class="language-plaintext highlighter-rouge">d_array</code>) back to CPU memory (<code class="language-plaintext highlighter-rouge">h_array</code>). The direction flag <code class="language-plaintext highlighter-rouge">cudaMemcpyDeviceToHost</code> tells CUDA we’re transferring data from the GPU to the CPU.</p>
  </li>
</ol>

<p>With this, we’ve covered the basics of how to manage memory between the CPU and GPU — a crucial foundation for any CUDA program.</p>

<h4 id="6-your-first-real-cuda-example-grayscale-conversion">6. Your First Real CUDA Example: Grayscale Conversion</h4>
<p>We’ve now covered key CUDA concepts like thread organization, memory management, and kernel launches, and written several simple toy kernels to make them stick. It’s time to take off the training wheels and write a full CUDA kernel to solve a real-world problem.</p>

<p>In this next section, we’ll convert a color image to grayscale — not one pixel at a time like we would on the CPU, but all at once by leveraging CUDA’s parallel threads. It’s a practical use case that brings everything we’ve learned together. Let us first look at the code and run it locally to convert a sample color image to grayscale.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;cuda.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;opencv2/opencv.hpp&gt;</span><span class="cp">
#define BLOCKSIZE_X 32
#define BLOCKSIZE_Y 32
</span><span class="k">using</span> <span class="k">namespace</span> <span class="n">std</span><span class="p">;</span>

<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">convert_rgb_to_grayscale</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">dsample_image</span><span class="p">,</span>
                                         <span class="kt">float</span> <span class="o">*</span><span class="n">dgrayscale_sample_image</span><span class="p">,</span>
                                         <span class="kt">int</span> <span class="n">rows</span><span class="p">,</span> <span class="kt">int</span> <span class="n">cols</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1">// compute the (x, y) coordinates of the thread in the image</span>
  <span class="kt">int</span> <span class="n">global_x</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">global_y</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>

  <span class="c1">// check that we are within image bounds</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">global_x</span> <span class="o">&lt;</span> <span class="n">cols</span> <span class="o">&amp;&amp;</span> <span class="n">global_y</span> <span class="o">&lt;</span> <span class="n">rows</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// flatten 2D coordinates into a 1D index</span>
    <span class="kt">int</span> <span class="n">global_id</span> <span class="o">=</span> <span class="n">global_y</span> <span class="o">*</span> <span class="n">cols</span> <span class="o">+</span> <span class="n">global_x</span><span class="p">;</span>

    <span class="c1">// fetch the RGB values for the current pixel</span>
    <span class="kt">float</span> <span class="n">r</span> <span class="o">=</span> <span class="n">dsample_image</span><span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="n">global_id</span><span class="p">];</span>
    <span class="kt">float</span> <span class="n">g</span> <span class="o">=</span> <span class="n">dsample_image</span><span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="n">global_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">];</span>
    <span class="kt">float</span> <span class="n">b</span> <span class="o">=</span> <span class="n">dsample_image</span><span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="n">global_id</span> <span class="o">+</span> <span class="mi">2</span><span class="p">];</span>

    <span class="c1">// compute grayscale using weighted sum (perceptual luminance)</span>
    <span class="n">dgrayscale_sample_image</span><span class="p">[</span><span class="n">global_id</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.144</span> <span class="o">*</span> <span class="n">r</span> <span class="o">+</span> <span class="mf">0.587</span> <span class="o">*</span> <span class="n">g</span> <span class="o">+</span> <span class="mf">0.299</span> <span class="o">*</span> <span class="n">b</span><span class="p">;</span>
<span class="p">}</span>


<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="c1">// read image from filepath</span>
  <span class="n">string</span> <span class="n">sample_image_path</span> <span class="o">=</span> <span class="s">"../sample.png"</span><span class="p">;</span>
  <span class="n">cv</span><span class="o">::</span><span class="n">Mat</span> <span class="n">sample_image</span> <span class="o">=</span> <span class="n">cv</span><span class="o">::</span><span class="n">imread</span><span class="p">(</span><span class="n">sample_image_path</span><span class="p">);</span>
  <span class="kt">int</span> <span class="n">width</span> <span class="o">=</span> <span class="n">sample_image</span><span class="p">.</span><span class="n">cols</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">height</span> <span class="o">=</span> <span class="n">sample_image</span><span class="p">.</span><span class="n">rows</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">channels</span> <span class="o">=</span> <span class="n">sample_image</span><span class="p">.</span><span class="n">channels</span><span class="p">();</span>
  <span class="n">sample_image</span><span class="p">.</span><span class="n">convertTo</span><span class="p">(</span><span class="n">sample_image</span><span class="p">,</span> <span class="n">CV_32F</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">);</span>

  <span class="c1">// allocate memory on GPU</span>
  <span class="kt">int</span> <span class="n">sample_image_size_in_bytes</span> <span class="o">=</span> <span class="n">width</span> <span class="o">*</span> <span class="n">height</span> <span class="o">*</span> <span class="n">channels</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
  <span class="kt">float</span> <span class="o">*</span><span class="n">dsample_image</span><span class="p">;</span>
  <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">dsample_image</span><span class="p">,</span> <span class="n">sample_image_size_in_bytes</span><span class="p">);</span>
  <span class="c1">// allocate memory on GPU to store the grayscale image</span>
  <span class="kt">float</span> <span class="o">*</span><span class="n">dgrayscale_sample_image</span><span class="p">;</span>
  <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">dgrayscale_sample_image</span><span class="p">,</span> <span class="n">width</span> <span class="o">*</span> <span class="n">height</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>

  <span class="c1">// copy image from CPU to GPU</span>
  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dsample_image</span><span class="p">,</span> <span class="n">sample_image</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">sample_image_size_in_bytes</span><span class="p">,</span>
             <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

  <span class="c1">// compute number of blocks in x and y dimensions</span>
  <span class="kt">int</span> <span class="n">number_of_blocks_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">width</span> <span class="o">+</span> <span class="n">BLOCKSIZE_X</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">BLOCKSIZE_X</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">number_of_blocks_y</span> <span class="o">=</span> <span class="p">(</span><span class="n">height</span> <span class="o">+</span> <span class="n">BLOCKSIZE_Y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">BLOCKSIZE_Y</span><span class="p">;</span>
  <span class="c1">// define grid dimension and block dimension for kernel launch</span>
  <span class="n">dim3</span> <span class="n">grid_dim</span><span class="p">(</span><span class="n">number_of_blocks_x</span><span class="p">,</span> <span class="n">number_of_blocks_y</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
  <span class="n">dim3</span> <span class="n">block_dim</span><span class="p">(</span><span class="n">BLOCKSIZE_X</span><span class="p">,</span> <span class="n">BLOCKSIZE_Y</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
  <span class="c1">// launch the kernel</span>
  <span class="n">convert_rgb_to_grayscale</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid_dim</span><span class="p">,</span> <span class="n">block_dim</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
      <span class="n">dsample_image</span><span class="p">,</span> <span class="n">dgrayscale_sample_image</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">);</span>


  <span class="c1">// copy the grayscale image back from GPU to CPU</span>
  <span class="n">cv</span><span class="o">::</span><span class="n">Mat</span> <span class="n">himage_grayscale</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">CV_32FC1</span><span class="p">);</span>
  <span class="kt">float</span> <span class="o">*</span><span class="n">himage_grayscale_data</span> <span class="o">=</span>
      <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span> <span class="o">*&gt;</span><span class="p">(</span><span class="n">himage_grayscale</span><span class="p">.</span><span class="n">data</span><span class="p">);</span>
  <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">himage_grayscale_data</span><span class="p">,</span> <span class="n">dgrayscale_sample_image</span><span class="p">,</span>
             <span class="n">width</span> <span class="o">*</span> <span class="n">height</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
  <span class="n">himage_grayscale</span><span class="p">.</span><span class="n">convertTo</span><span class="p">(</span><span class="n">himage_grayscale</span><span class="p">,</span> <span class="n">CV_8U</span><span class="p">,</span> <span class="mf">255.0</span><span class="p">);</span>
  <span class="n">cv</span><span class="o">::</span><span class="n">imwrite</span><span class="p">(</span><span class="s">"../grayscale_sample.png"</span><span class="p">,</span> <span class="n">himage_grayscale</span><span class="p">);</span>
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>To execute this program on your machine, follow the following steps :</p>

<ol>
  <li>Navigate to the directory <em>8_that_first_cuda_blog/3_print_2d_coordinates</em>
    <div class="language-bash highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>8_that_first_cuda_blog/4_grayscale_2d
</code></pre></div>    </div>
  </li>
  <li>Make a <code class="language-plaintext highlighter-rouge">build</code> directory and navigate into it
    <div class="language-bash highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="nb">mkdir </span>build <span class="o">&amp;&amp;</span> <span class="nb">cd </span>build
</code></pre></div>    </div>
  </li>
  <li>Generate the Makefile. Specify the appropriate CUDA path
    <div class="language-bash highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>cmake .. <span class="nt">-DCMAKE_CUDA_COMPILER</span><span class="o">=</span>/usr/local/cuda-12.1/bin/nvcc
</code></pre></div>    </div>
  </li>
  <li>Compile the program using the following command
    <div class="language-bash highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>make
</code></pre></div>    </div>
  </li>
  <li>This will create an executable <code class="language-plaintext highlighter-rouge">grayscale_2d</code> in the <code class="language-plaintext highlighter-rouge">build</code> directory. Execute it using
    <div class="language-bash highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>./grayscale_2d
</code></pre></div>    </div>
  </li>
  <li>The grayscale image will be stored in the <em>8_that_first_cuda_blog/4_grayscale_2d</em> folder as <code class="language-plaintext highlighter-rouge">grayscale_sample.png</code>
</li>
</ol>

<p>If all went well, you just wrote your first real and useful CUDA kernel. In essence, the code above does the following:</p>
<ol>
  <li>We loaded a color image using OpenCV and converted its pixel values to floats.</li>
  <li>We allocated memory on the GPU for both the input image and the grayscale output.</li>
  <li>We copied the image data from the CPU to the GPU.</li>
  <li>We calculated how many blocks and threads we need to cover all pixels—each block handles a small tile of the image, and each thread processes one pixel.</li>
  <li>The CUDA kernel ran in parallel, where each thread took one pixel and computed the grayscale value as a weighted sum of its R, G, B components.</li>
  <li>Finally, we copied the grayscale result back to the CPU and saved it as an image file.</li>
</ol>

<p>Let us look at the above code, one crucial part at a time.</p>

<ul>
  <li>Firstly, we read the image using <code class="language-plaintext highlighter-rouge">OpenCV</code>, extract the <code class="language-plaintext highlighter-rouge">width</code>,<code class="language-plaintext highlighter-rouge">height</code> and <code class="language-plaintext highlighter-rouge">channels</code>.</li>
  <li>By default, OpenCV loads the image in <code class="language-plaintext highlighter-rouge">int8</code> format. We convert it to <code class="language-plaintext highlighter-rouge">float32</code> for ease of processing. <br>
    <div class="language-cpp highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="c1">// read image from filepath</span>
<span class="n">string</span> <span class="n">sample_image_path</span> <span class="o">=</span> <span class="s">"../sample.png"</span><span class="p">;</span>
<span class="n">cv</span><span class="o">::</span><span class="n">Mat</span> <span class="n">sample_image</span> <span class="o">=</span> <span class="n">cv</span><span class="o">::</span><span class="n">imread</span><span class="p">(</span><span class="n">sample_image_path</span><span class="p">);</span> <span class="c1">// read the image</span>
<span class="kt">int</span> <span class="n">width</span> <span class="o">=</span> <span class="n">sample_image</span><span class="p">.</span><span class="n">cols</span><span class="p">;</span> <span class="c1">// extract height, width, channels</span>
<span class="kt">int</span> <span class="n">height</span> <span class="o">=</span> <span class="n">sample_image</span><span class="p">.</span><span class="n">rows</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">channels</span> <span class="o">=</span> <span class="n">sample_image</span><span class="p">.</span><span class="n">channels</span><span class="p">();</span>
<span class="n">sample_image</span><span class="p">.</span><span class="n">convertTo</span><span class="p">(</span><span class="n">sample_image</span><span class="p">,</span> <span class="n">CV_32F</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">);</span> <span class="c1">// convert the image from int8 to float32</span>
</code></pre></div>    </div>
    <p>The details of OpenCV and its interfaces are beyond the scope of this blog post. 
For our purposes, it is enough to understand that the code above simply loads the image into CPU memory in a row-major, contiguous format.</p>
  </li>
  <li>We then calculate how much memory the image will occupy in bytes <code class="language-plaintext highlighter-rouge">(width × height × channels × sizeof(float))</code>.</li>
  <li>Then we allocate that much space on the GPU using <code class="language-plaintext highlighter-rouge">cudaMalloc</code>.</li>
  <li>We copy the image data from the CPU (<code class="language-plaintext highlighter-rouge">sample_image.data</code>) to the allocated GPU memory (<code class="language-plaintext highlighter-rouge">dsample_image</code>) using cudaMemcpy.
This ensures that the image is now available on the GPU for parallel processing.
    <div class="language-cpp highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">sample_image_size_in_bytes</span> <span class="o">=</span> <span class="n">width</span> <span class="o">*</span> <span class="n">height</span> <span class="o">*</span> <span class="n">channels</span> <span class="o">*</span> <span class="nf">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
<span class="kt">float</span> <span class="o">*</span><span class="n">dsample_image</span><span class="p">;</span>
<span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">dsample_image</span><span class="p">,</span> <span class="n">sample_image_size_in_bytes</span><span class="p">);</span>
<span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dsample_image</span><span class="p">,</span> <span class="n">sample_image</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">sample_image_size_in_bytes</span><span class="p">,</span>
        <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</code></pre></div>    </div>
  </li>
  <li>
<code class="language-plaintext highlighter-rouge">dsample_image</code> is the pointer to the input RGB image, and <code class="language-plaintext highlighter-rouge">dgrayscale_sample_image</code> is the pointer to the output grayscale image.
We pass pointers because CUDA kernels operate on data already present on the GPU — they don’t copy data themselves.</li>
  <li>We also pass the image dimensions (<code class="language-plaintext highlighter-rouge">rows</code> and <code class="language-plaintext highlighter-rouge">cols</code>) so that each thread can figure out which pixel it is responsible for.
    <div class="language-cpp highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> <span class="n">convert_rgb_to_grayscale</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">dsample_image</span><span class="p">,</span>
                                       <span class="kt">float</span> <span class="o">*</span><span class="n">dgrayscale_sample_image</span><span class="p">,</span>
                                       <span class="kt">int</span> <span class="n">rows</span><span class="p">,</span> <span class="kt">int</span> <span class="n">cols</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>Each GPU thread is assigned a unique (x, y) coordinate using its block and thread indices.</li>
  <li>It checks whether this coordinate lies within the bounds of the image to avoid illegal memory access.</li>
  <li>Using the 2D coordinate, it calculates a flat 1D index to access the RGB values of that pixel.</li>
  <li>Finally, it computes the grayscale value using a weighted sum and writes it to the output array on the GPU.
    <div class="language-cpp highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">global_x</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">global_y</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
<span class="k">if</span> <span class="p">(</span><span class="n">global_x</span> <span class="o">&lt;</span> <span class="n">cols</span> <span class="o">&amp;&amp;</span> <span class="n">global_y</span> <span class="o">&lt;</span> <span class="n">rows</span><span class="p">)</span> <span class="p">{</span>
<span class="kt">int</span> <span class="n">global_id</span> <span class="o">=</span> <span class="n">global_y</span> <span class="o">*</span> <span class="n">cols</span> <span class="o">+</span> <span class="n">global_x</span><span class="p">;</span>
<span class="kt">float</span> <span class="n">r</span> <span class="o">=</span> <span class="n">dsample_image</span><span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="n">global_id</span><span class="p">];</span>
<span class="kt">float</span> <span class="n">g</span> <span class="o">=</span> <span class="n">dsample_image</span><span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="n">global_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">];</span>
<span class="kt">float</span> <span class="n">b</span> <span class="o">=</span> <span class="n">dsample_image</span><span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="n">global_id</span> <span class="o">+</span> <span class="mi">2</span><span class="p">];</span>
<span class="n">dgrayscale_sample_image</span><span class="p">[</span><span class="n">global_id</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.144</span> <span class="o">*</span> <span class="n">r</span> <span class="o">+</span> <span class="mf">0.587</span> <span class="o">*</span> <span class="n">g</span> <span class="o">+</span> <span class="mf">0.299</span> <span class="o">*</span> <span class="n">b</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>    </div>
  </li>
  <li>We calculate how many blocks are needed in the X and Y directions to cover the entire image, rounding up to handle any leftover pixels.</li>
  <li>Each block contains a fixed number of threads defined by <code class="language-plaintext highlighter-rouge">BLOCKSIZE_X</code> and <code class="language-plaintext highlighter-rouge">BLOCKSIZE_Y</code>.</li>
  <li>These values are used to create the grid and block dimensions (<code class="language-plaintext highlighter-rouge">grid_dim</code> and <code class="language-plaintext highlighter-rouge">block_dim</code>), which tell CUDA how to organize threads for parallel execution.</li>
  <li>Finally, we launch the kernel with this configuration, passing the GPU pointers and image dimensions so each thread can process its assigned pixel.
    <div class="language-cpp highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="c1">// compute number of blocks in x and y dimensions</span>
<span class="kt">int</span> <span class="n">number_of_blocks_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">width</span> <span class="o">+</span> <span class="n">BLOCKSIZE_X</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">BLOCKSIZE_X</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">number_of_blocks_y</span> <span class="o">=</span> <span class="p">(</span><span class="n">height</span> <span class="o">+</span> <span class="n">BLOCKSIZE_Y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">BLOCKSIZE_Y</span><span class="p">;</span>
<span class="c1">// define grid dimension and block dimension for kernel launch</span>
<span class="n">dim3</span> <span class="nf">grid_dim</span><span class="p">(</span><span class="n">number_of_blocks_x</span><span class="p">,</span> <span class="n">number_of_blocks_y</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="n">dim3</span> <span class="nf">block_dim</span><span class="p">(</span><span class="n">BLOCKSIZE_X</span><span class="p">,</span> <span class="n">BLOCKSIZE_Y</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="c1">// launch the kernel</span>
<span class="n">convert_rgb_to_grayscale</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid_dim</span><span class="p">,</span> <span class="n">block_dim</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
  <span class="n">dsample_image</span><span class="p">,</span> <span class="n">dgrayscale_sample_image</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">);</span>
</code></pre></div>    </div>
  </li>
  <li>We create an empty OpenCV matrix <code class="language-plaintext highlighter-rouge">himage_grayscale</code> on the CPU to hold the grayscale image data and get a raw pointer to its data.</li>
  <li>Then, we copy the grayscale image from GPU memory back to the CPU, convert it to an 8-bit format, and save it as a PNG file using OpenCV’s <code class="language-plaintext highlighter-rouge">imwrite</code>.
    <div class="language-cpp highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="c1">// copy the grayscale image back from GPU to CPU</span>
<span class="n">cv</span><span class="o">::</span><span class="n">Mat</span> <span class="nf">himage_grayscale</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">CV_32FC1</span><span class="p">);</span>
<span class="kt">float</span> <span class="o">*</span><span class="n">himage_grayscale_data</span> <span class="o">=</span>
  <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span> <span class="o">*&gt;</span><span class="p">(</span><span class="n">himage_grayscale</span><span class="p">.</span><span class="n">data</span><span class="p">);</span>
<span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">himage_grayscale_data</span><span class="p">,</span> <span class="n">dgrayscale_sample_image</span><span class="p">,</span>
          <span class="n">width</span> <span class="o">*</span> <span class="n">height</span> <span class="o">*</span> <span class="nf">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
<span class="n">himage_grayscale</span><span class="p">.</span><span class="n">convertTo</span><span class="p">(</span><span class="n">himage_grayscale</span><span class="p">,</span> <span class="n">CV_8U</span><span class="p">,</span> <span class="mf">255.0</span><span class="p">);</span>
<span class="n">cv</span><span class="o">::</span><span class="n">imwrite</span><span class="p">(</span><span class="s">"../grayscale_sample.png"</span><span class="p">,</span> <span class="n">himage_grayscale</span><span class="p">);</span>
</code></pre></div>    </div>
  </li>
</ul>

<!-- One important concept regarding host-device communication is that the **host does not wait for the kernel execution to finish**, and moves on with the next instruction. This execution approach is known as **asynchronous**. In particular, the `host` and `device` executes independently and simulatenously. When a command is like kernel launch is issued by the host, it does not wait for the command to complete on the `device`, but simply moves on to the next instruction, while the `device` handles the requested operation in parallel.

To unerstand this better, we can change the earlier `hello_world.cu` source code, by commenting out `cudaDeviceSynchronize();`.

```cpp
#include <iostream>
#include <cuda_runtime.h>

__global__ void gpu_hello_world(){
  printf("Hello World from GPU! \n");
}

int main(){
  std::cout << "Hello World from CPU!" << std::endl;
  gpu_hello_world<<<1,1>>>();
  // comment out this line. 
  // Now the host does not wait for the device and moves on.
  // cudaDeviceSynchronize();
}
```

The output of this program will be just as follows :

```
Hello World from CPU!
```

Note, that since we removed `cudaDeviceSynchronize();`, the host launches the `gpu_hello_world` kernel and moves on to the next instruction. The exection of the host code finishes, even before the `device` completes, hence it does not print `Hello World from GPU!` onto the output buffer. This simple example highlights the separation between CPU and GPU execution — each runs independently unless explicitly synchronized. -->

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/hidden-speed-in-shared-memory/">Hidden Speed in CUDA's Shared Memory</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/a-practical-guide-to-quantization/">A practical guide to Quantization</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/down-the-cudamemory-lane/">Down the CudaMemory lane</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/quantization-explained-like-youre-five/">Quantization explained, like you are five.</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/tensorrt-meets-cpp/">TensorRT meets C++</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Sanket R. Shah. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
