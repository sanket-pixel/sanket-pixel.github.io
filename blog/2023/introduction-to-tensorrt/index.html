<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Have you met TensorRT? | Sanket Shah</title>
    <meta name="author" content="Sanket R. Shah">
    <meta name="description" content="Introduction to TensorRT in python.">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://sanket-pixel.github.io//blog/2023/introduction-to-tensorrt/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-J2Z5HX2M1E"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-J2Z5HX2M1E');
  </script>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">Sanket Shah</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Have you met TensorRT?</h1>
    <p class="post-meta">July 12, 2023</p>
    <p class="post-tags">
      <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
        ·  
        <a href="/blog/tag/nvidia">
          <i class="fas fa-hashtag fa-sm"></i> nvidia</a>  
          <a href="/blog/tag/tensorrt">
          <i class="fas fa-hashtag fa-sm"></i> tensorrt</a>  
          <a href="/blog/tag/deep-learning">
          <i class="fas fa-hashtag fa-sm"></i> deep-learning</a>  
          
        ·  
        <a href="/blog/category/edge-ai">
          <i class="fas fa-tag fa-sm"></i> edge-ai</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_1/main_photo-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_1/main_photo-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_1/main_photo-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_1/main_photo.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="have you met" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Have you met TensorRT?
</div>

<h5 id="welcome-to-our-blog-where-we-explore-the-fascinating-world-of-tensorrta-powerful-tool-that-unleashes-unparalleled-speed-and-efficiency-in-ai-in-this-first-part-well-take-a-glimpse-into-the-extraordinary-capabilities-of-tensorrt-and-its-impact-on-deep-learning-imagine-a-magical-optimization-framework-that-enhances-ai-models-enabling-them-to-process-data-at-lightning-speed-without-compromising-accuracy-its-like-giving-ai-a-boost-of-superpowers-making-it-faster-smarter-and-more-efficient-join-us-on-this-captivating-journey-as-we-uncover-the-wonders-of-tensorrt-and-its-potential-to-revolutionize-the-field-of-artificial-intelligence">Welcome to our blog, where we explore the fascinating world of TensorRT—a powerful tool that unleashes unparalleled speed and efficiency in AI. In this first part, we’ll take a glimpse into the extraordinary capabilities of TensorRT and its impact on deep learning. Imagine a magical optimization framework that enhances AI models, enabling them to process data at lightning speed without compromising accuracy. It’s like giving AI a boost of superpowers, making it faster, smarter, and more efficient. Join us on this captivating journey as we uncover the wonders of TensorRT and its potential to revolutionize the field of artificial intelligence.</h5>

<p><br></p>

<h3 id="source-from-github">Source from Github</h3>
<p>For those interested in exploring the code and gaining a deeper understanding of the concepts discussed in this blog on TensorRT and image classification, you can find the complete source code in the corresponding GitHub repository. The repository link is <a href="https://github.com/sanket-pixel/tensorrt_deeploy" rel="external nofollow noopener" target="_blank">this</a>.
, which houses an array of edge AI blogs and their source code for further exploration.</p>

<p>In particular, the source code for this specific blog, covering the fundamentals of TensorRT, image classification with PyTorch, ONNX conversion, TensorRT engine generation, and inference speedup measurement, is available in the notebook found <a href="https://github.com/sanket-pixel/tensorrt_deeploy/blob/main/1_fundamentals/python/1_fundamentals.ipynb" rel="external nofollow noopener" target="_blank">here</a>. By delving into this notebook, you can follow along with the step-by-step implementations and gain hands-on experience in harnessing the power of TensorRT for edge AI applications. Happy coding and exploring the realms of accelerated AI with PyTorch and TensorRT!</p>

<p><br></p>

<h3 id="pre-requisites-and-installation">Pre-requisites and Installation</h3>
<h5 id="1-hardware-requirements">1. Hardware requirements</h5>
<ul>
  <li>NVIDIA GPU</li>
</ul>

<h5 id="2-software-requirements">2. Software requirements</h5>
<ul>
  <li>Ubuntu &gt;= 18.04</li>
  <li>Python &gt;= 3.8</li>
</ul>

<h5 id="3-installation-guide">3. Installation Guide</h5>
<ol>
  <li>Create conda environment and install required python packages.
    <div class="language-plaintext highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>conda create -n trt python=3.8
conda activate trt
pip install -r requirements.txt
</code></pre></div>    </div>
  </li>
  <li>Install TensorRT 
Install TensorRT:</li>
</ol>

<ul>
  <li>
    <p>Download and install NVIDIA CUDA 11.4 or later following the official instructions: <a href="https://developer.nvidia.com/cuda-toolkit-archive" rel="external nofollow noopener" target="_blank">link</a></p>
  </li>
  <li>
    <p>Download and extract CuDNN library for your CUDA version (&gt;8.9.0) from: <a href="https://developer.nvidia.com/cudnn" rel="external nofollow noopener" target="_blank">link</a></p>
  </li>
  <li>
    <p>Download and extract NVIDIA TensorRT library for your CUDA version from: <a href="https://developer.nvidia.com/nvidia-tensorrt-8x-download" rel="external nofollow noopener" target="_blank">link</a>. Minimum required version is 8.5. Follow the Installation Guide for your system and ensure Python’s part is installed.</p>
  </li>
  <li>
    <p>Add the absolute path to CUDA, TensorRT, and CuDNN libs to the environment variable PATH or LD_LIBRARY_PATH.</p>
  </li>
  <li>
    <p>Install PyCUDA:</p>
    <div class="language-plaintext highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>pip install pycuda
</code></pre></div>    </div>
    <p><br></p>
  </li>
</ul>

<h3 id="introduction">Introduction</h3>

<p>TensorRT is an optimization framework designed to accelerate AI inference, making it faster and more efficient. Think of it as a performance booster for AI models, enhancing their capabilities without compromising accuracy. Imagine you have a collection of handwritten letters, each representing a unique story. You wish to organize and analyze these letters, but they are scattered and unstructured. That’s when you bring in a talented editor who transforms these letters into a beautifully composed novel, ready to be read and understood.</p>

<p>In this analogy, the handwritten letters represent a PyTorch model—an impressive piece of work but lacking the efficiency needed for real-time inference. The editor symbolizes TensorRT, refining the model and optimizing it to perform with lightning-fast speed and accuracy. Similar to how the editor transforms the letters into a coherent novel, TensorRT takes the PyTorch model and enhances it, making it highly efficient and ready to tackle complex tasks in a fraction of the time. With TensorRT’s optimization techniques, just as the editor refines the structure and language of the letters, the model undergoes a transformative process. TensorRT eliminates inefficiencies, fuses layers, and calibrates precision, resulting in an optimized model that can process data swiftly and accurately—like a beautifully composed novel ready to be enjoyed.</p>

<p>In our upcoming blog posts, we will take you on a journey where we explore practical examples of TensorRT in action. We will witness its impact on image classification, object detection, and more. Through these real-world applications, you will discover how TensorRT empowers AI practitioners to achieve remarkable performance gains, opening doors to innovative solutions and possibilities.</p>

<p>So, without further ado, let’s dive into the realm of TensorRT and witness firsthand the transformative power it holds in the field of artificial intelligence.</p>

<p><br></p>

<h3 id="step-1---hotdog-classification-using-pure-pytorch">Step 1 :  Hotdog Classification Using Pure Pytorch</h3>

<p>Now that we have familiarized ourselves with the wonders of TensorRT, let’s dive into a practical example to witness its impact firsthand. Imagine a scenario where we want to classify images of different objects, specifically determining whether an image contains a hotdog or not. To tackle this deliciously challenging task, we will leverage a pretrained PyTorch model based on ResNet architecture, which has been trained on the vast and diverse ImageNet dataset.</p>

<p>The problem at hand is intriguing yet straightforward: we aim to develop an AI model capable of differentiating between hotdogs and other objects. By utilizing the power of deep learning and the wealth of knowledge encoded within the pretrained PyTorch model, we can accomplish this with remarkable accuracy.</p>

<p>To begin, we take an image of a mouthwatering hotdog as our test subject. The pretrained PyTorch model, being a master of image recognition, will scrutinize the visual features of the hotdog and perform intricate calculations to make its classification decision. It will utilize its knowledge of patterns, shapes, and textures acquired during its training on the vast ImageNet dataset, making an educated guess as to whether the image depicts a hotdog or something else entirely.</p>

<p>This process might seem effortless to us, but behind the scenes, the AI model performs an intricate dance of calculations and computations. It analyzes the pixels of the image, extracts features, and applies complex mathematical operations to arrive at a confident prediction.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">import</span> <span class="n">cv2</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torchvision.transforms</span> <span class="kn">import</span> <span class="n">Resize</span><span class="p">,</span> <span class="n">Compose</span><span class="p">,</span> <span class="n">ToTensor</span><span class="p">,</span> <span class="n">Normalize</span>

<span class="k">def</span> <span class="nf">preprocess_image</span><span class="p">(</span><span class="n">img_path</span><span class="p">):</span>
    <span class="c1"># transformations for the input data
</span>    <span class="n">transforms</span> <span class="o">=</span> <span class="nc">Compose</span><span class="p">([</span>
        <span class="nc">ToTensor</span><span class="p">(),</span>
        <span class="nc">Resize</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
        <span class="nc">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]),</span>
    <span class="p">])</span>

    <span class="c1"># read input image
</span>    <span class="n">input_img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">imread</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
    <span class="c1"># do transformations
</span>    <span class="n">input_data</span> <span class="o">=</span> <span class="nf">transforms</span><span class="p">(</span><span class="n">input_img</span><span class="p">)</span>
    <span class="n">batch_data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch_data</span>

<span class="k">def</span> <span class="nf">postprocess</span><span class="p">(</span><span class="n">output_data</span><span class="p">):</span>
    <span class="c1"># get class names
</span>    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">../data/imagenet-classes.txt</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">.</span><span class="nf">readlines</span><span class="p">()]</span>
    <span class="c1"># calculate human-readable value by softmax
</span>    <span class="n">confidences</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">output_data</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="c1"># find top predicted classes
</span>    <span class="n">_</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">output_data</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># print the top classes predicted by the model
</span>    <span class="k">while</span> <span class="n">confidences</span><span class="p">[</span><span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]]</span> <span class="o">&gt;</span> <span class="mi">50</span><span class="p">:</span>
        <span class="n">class_idx</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
        <span class="nf">print</span><span class="p">(</span>
            <span class="sh">"</span><span class="s">class:</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">classes</span><span class="p">[</span><span class="n">class_idx</span><span class="p">],</span>
            <span class="sh">"</span><span class="s">, confidence:</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">confidences</span><span class="p">[</span><span class="n">class_idx</span><span class="p">].</span><span class="nf">item</span><span class="p">(),</span>
            <span class="sh">"</span><span class="s">%, index:</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">class_idx</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="nb">input</span> <span class="o">=</span> <span class="nf">preprocess_image</span><span class="p">(</span><span class="sh">"</span><span class="s">../data/hotdog.jpg</span><span class="sh">"</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="nf">postprocess</span><span class="p">(</span><span class="n">output</span><span class="p">)</span></code></pre></figure>

<p>In the above code, we have a script that demonstrates the usage of a pretrained ResNet-50 model from torchvision for image classification. Let’s break down the code and understand its functionality:</p>

<p>First, we import the necessary libraries, including <code class="language-plaintext highlighter-rouge">models</code> from <code class="language-plaintext highlighter-rouge">torchvision</code>, <code class="language-plaintext highlighter-rouge">cv2</code>, and <code class="language-plaintext highlighter-rouge">torch</code>, which provide the tools for working with deep learning models and image processing.</p>

<p>The script defines two important functions. The <code class="language-plaintext highlighter-rouge">preprocess_image</code> function takes an image path as input and applies a series of transformations to preprocess the image. These transformations include converting the image to a tensor, resizing it to a specific size (in this case, 224x224), and normalizing its pixel values using mean and standard deviation values commonly used in ImageNet dataset preprocessing.</p>

<p>Next, we have the <code class="language-plaintext highlighter-rouge">postprocess</code> function, which processes the output of the model. It reads the class names from a text file (<code class="language-plaintext highlighter-rouge">imagenet-classes.txt</code>), calculates the confidence scores using softmax, and sorts the output to find the top predicted classes. It then prints the class name, confidence score, and class index for each top prediction.</p>

<p>Moving on, we preprocess the input image using the <code class="language-plaintext highlighter-rouge">preprocess_image</code> function. In this case, we are using the image <code class="language-plaintext highlighter-rouge">hotdog.jpg</code>. The resulting tensor is stored in the <code class="language-plaintext highlighter-rouge">input</code> variable and is moved to the GPU (assuming it is available) using <code class="language-plaintext highlighter-rouge">.cuda()</code>.</p>

<p>We then load the ResNet-50 model using <code class="language-plaintext highlighter-rouge">models.resnet50(pretrained=True)</code>. This fetches the pretrained weights from the model zoo. The model is set to evaluation mode (<code class="language-plaintext highlighter-rouge">model.eval()</code>), and its parameters are moved to the GPU using <code class="language-plaintext highlighter-rouge">.cuda()</code>.</p>

<p>Now, we perform a forward pass through the model by passing the preprocessed input tensor (<code class="language-plaintext highlighter-rouge">input</code>) to the ResNet-50 model (<code class="language-plaintext highlighter-rouge">model</code>). This gives us the output tensor (<code class="language-plaintext highlighter-rouge">output</code>).</p>

<p>Finally, we call the <code class="language-plaintext highlighter-rouge">postprocess</code> function with the <code class="language-plaintext highlighter-rouge">output</code> tensor to interpret and display the classification results. It prints the top predicted classes along with their corresponding confidence scores and class indices.</p>

<p>By following this code, a reader can classify an input image using the pretrained ResNet-50 model, obtaining the predicted class labels and their confidence scores. This example demonstrates the power of deep learning in image classification tasks and showcases how pretrained models can be easily utilized for real-world applications.</p>

<p>If everything goes well, you should see an output similar to this :</p>

<p><code class="language-plaintext highlighter-rouge">class: hotdog, hot dog, red hot , confidence: 60.50566864013672 %, index: 934</code></p>

<p>This output represents the classification result of the input image (in this case, a hotdog image) using the pretrained ResNet-50 model. The model has predicted that the image belongs to the class “hotdog, hot dog, red hot” with a confidence score of 60.51%. The corresponding class index is 934.</p>

<p>In simpler terms, the model has successfully recognized the image as a hotdog with a relatively high level of confidence. This showcases the capability of the ResNet-50 model to accurately classify objects in images, making it a valuable tool for various computer vision tasks.</p>

<p><br></p>

<h3 id="step-2-pytorch-to-onnx-conversion">Step 2: PyTorch to ONNX Conversion</h3>
<p>In the previous section, we successfully built and utilized a PyTorch model for hotdog classification. Now, let’s take a step further and optimize the inference performance using TensorRT. In this section, we will explore the process of converting a PyTorch model to into a TensorRT engine.</p>

<p>To convert our PyTorch model to a TensorRT engine, we’ll follow a two-step process that involves the intermediate conversion to the ONNX format. This allows us to seamlessly integrate PyTorch and TensorRT, unlocking the benefits of accelerated inference.</p>

<p>The first step is to convert our PyTorch model to the ONNX format. ONNX, short for Open Neural Network Exchange, acts as a bridge between different deep learning frameworks. It provides a standardized representation of our model’s architecture and parameters, ensuring compatibility across platforms and frameworks.By exporting our PyTorch model to ONNX, we capture its structure and operations in a portable and platform-independent format. This enables us to work with the model using other frameworks, such as TensorRT, without losing important information or needing to reimplement the model from scratch.</p>

<p>To convert our PyTorch model to ONNX, we need to follow a few simple steps. First, we initialize an empty PyTorch model with the same architecture as our trained model. Then, we load the weights from our trained PyTorch model into the new model. After that, we export the model to the ONNX format using the torch.onnx.export function, specifying the input tensor shape and the desired output file name.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ONNX_FILE_PATH</span> <span class="o">=</span> <span class="sh">'</span><span class="s">../deploy_tools/resnet50.onnx</span><span class="sh">'</span>
<span class="n">torch</span><span class="p">.</span><span class="n">onnx</span><span class="p">.</span><span class="nf">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">ONNX_FILE_PATH</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">input</span><span class="sh">'</span><span class="p">],</span>
                  <span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">output</span><span class="sh">'</span><span class="p">],</span> <span class="n">export_params</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>The above lines are used to export a PyTorch model to the ONNX format. Here’s a breakdown of what each line does:</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">ONNX_FILE_PATH = '../deploy_tools/resnet50.onnx'</code>: This line defines the path and filename where the exported ONNX model will be saved. In this example, the ONNX file will be saved as “resnet50.onnx” in the “../deploy_tools” directory.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">torch.onnx.export(model, input, ONNX_FILE_PATH, input_names=['input'], output_names=['output'], export_params=True)</code>: This line exports the PyTorch model to ONNX format using the <code class="language-plaintext highlighter-rouge">torch.onnx.export</code> function. It takes several arguments:</p>

    <ul>
      <li>
<code class="language-plaintext highlighter-rouge">model</code>: This is the PyTorch model object that you want to export to ONNX.</li>
      <li>
<code class="language-plaintext highlighter-rouge">input</code>: This represents an example input tensor that will be used to trace the model. The shape and data type of this tensor should match the expected input for the model.</li>
      <li>
<code class="language-plaintext highlighter-rouge">ONNX_FILE_PATH</code>: This is the path and filename where the exported ONNX model will be saved, as defined in the previous line.</li>
      <li>
<code class="language-plaintext highlighter-rouge">input_names=['input']</code>: This specifies the names of the input nodes in the exported ONNX model. In this case, the input node will be named “input”.</li>
      <li>
<code class="language-plaintext highlighter-rouge">output_names=['output']</code>: This specifies the names of the output nodes in the exported ONNX model. In this case, the output node will be named “output”.</li>
      <li>
<code class="language-plaintext highlighter-rouge">export_params=True</code>: This indicates whether to export the parameters (weights and biases) of the model along with the model architecture. Setting it to <code class="language-plaintext highlighter-rouge">True</code> means the parameters will be included in the exported ONNX model.</li>
    </ul>
  </li>
</ul>

<p>Using the above code, the PyTorch model will be converted to the ONNX format and saved as an ONNX file at the specified location. The exported ONNX model can then be used in other frameworks or tools that support ONNX, allowing for interoperability and deployment in different runtime environments. Once we have successfully converted our PyTorch model to the ONNX format, it’s time to take a closer look at the inner workings of this intermediate representation. To gain a visual understanding of our ONNX model, we can utilize a powerful tool called Netron.</p>

<p>Netron is a user-friendly model visualization tool that allows us to explore and analyze the structure of our ONNX model. With its intuitive interface and interactive features, Netron offers a delightful experience for visualizing deep learning models. To visualize your ONNX model and confirm the success of the conversion process, you can follow these steps using the online tool Netron:</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Visit the Netron website</code>:
Go to the Netron website by using <a href="https://netron.app/" rel="external nofollow noopener" target="_blank">netron.app</a></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Load the ONNX model</code>:
Click on the “Open” button on the Netron website. This will prompt you to select your ONNX file from your local machine.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Explore the model</code>:
Once the ONNX model is loaded, Netron will display a visual representation of its structure. You can navigate through the model’s layers, examine node connections, and inspect input and output shapes.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Analyze the model</code>:
Use Netron to gain insights into the architecture and operations of your ONNX model. Verify that the conversion from PyTorch to ONNX was successful by examining the model’s structure and checking the expected input and output configurations.</p>
  </li>
</ul>

<p>Here is an example of how the onnx model visualization looks like in Netron :</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_1/onnx-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_1/onnx-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_1/onnx-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_1/onnx.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
ONNX example visualization from netron
</div>

<p><br></p>

<h3 id="step-3--building-tensorrt-engine-from-onnx">Step 3 : Building TensorRT Engine from ONNX</h3>

<p>Now that we have successfully converted our PyTorch model to the ONNX format, it’s time to take the next step towards unleashing the power of TensorRT.</p>

<p>TensorRT is a high-performance deep learning inference optimizer and runtime library developed by NVIDIA. It is designed to optimize and accelerate neural network models, taking full advantage of GPU capabilities. By converting our ONNX model to TensorRT, we can harness the exceptional speed and efficiency offered by GPUs.</p>

<p>The process of converting ONNX to TensorRT involves leveraging the TensorRT Python API. This API provides a straightforward way to generate a TensorRT engine, which is a highly optimized representation of our model for efficient inference.</p>

<p>With the TensorRT engine in hand, we can take advantage of various optimizations and techniques offered by TensorRT. These include layer fusion, precision calibration, and dynamic tensor memory management, all aimed at maximizing inference performance.</p>

<p>In our upcoming sections, we will explore the Python code required to generate the TensorRT engine from our ONNX model. By following these steps, we will unlock the immense potential of TensorRT and experience a significant boost in inference speed and efficiency.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pycuda.driver</span> <span class="k">as</span> <span class="n">cuda</span>
<span class="kn">import</span> <span class="n">pycuda.autoinit</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">tensorrt</span> <span class="k">as</span> <span class="n">trt</span>
</code></pre></div></div>
<p>In these lines, we import the necessary libraries for our script: <code class="language-plaintext highlighter-rouge">pycuda.driver</code> for CUDA operations, <code class="language-plaintext highlighter-rouge">pycuda.autoinit</code> for initializing the GPU, <code class="language-plaintext highlighter-rouge">numpy</code> for numerical computations, and <code class="language-plaintext highlighter-rouge">tensorrt</code> for working with TensorRT.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">TRT_LOGGER</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Logger</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">Logger</span><span class="p">.</span><span class="n">WARNING</span><span class="p">)</span>
</code></pre></div></div>
<p>Here, we create a TensorRT logger object (<code class="language-plaintext highlighter-rouge">TRT_LOGGER</code>) with the log level set to <code class="language-plaintext highlighter-rouge">trt.Logger.WARNING</code>. This logger is used to manage logging messages during the conversion process.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">builder</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Builder</span><span class="p">(</span><span class="n">TRT_LOGGER</span><span class="p">)</span>
</code></pre></div></div>
<p>We create a TensorRT builder object (<code class="language-plaintext highlighter-rouge">builder</code>) using the previously defined logger. The builder is responsible for building TensorRT networks.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">EXPLICIT_BATCH</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="p">(</span><span class="nb">int</span><span class="p">)(</span><span class="n">trt</span><span class="p">.</span><span class="n">NetworkDefinitionCreationFlag</span><span class="p">.</span><span class="n">EXPLICIT_BATCH</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">create_network</span><span class="p">(</span><span class="n">EXPLICIT_BATCH</span><span class="p">)</span>
</code></pre></div></div>
<p>Here, we define the <code class="language-plaintext highlighter-rouge">EXPLICIT_BATCH</code> flag, which enables explicit batch mode in TensorRT. We then create a TensorRT network using the builder, specifying the <code class="language-plaintext highlighter-rouge">EXPLICIT_BATCH</code> flag.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">parser</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">OnnxParser</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">TRT_LOGGER</span><span class="p">)</span>
<span class="n">success</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="nf">parse_from_file</span><span class="p">(</span><span class="n">ONNX_FILE_PATH</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">parser</span><span class="p">.</span><span class="n">num_errors</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">parser</span><span class="p">.</span><span class="nf">get_error</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span>
</code></pre></div></div>
<p>We create an ONNX parser object (<code class="language-plaintext highlighter-rouge">parser</code>) associated with the network and logger. The parser is responsible for parsing the ONNX file and populating the TensorRT network. We parse the ONNX model by calling <code class="language-plaintext highlighter-rouge">parser.parse_from_file(ONNX_FILE_PATH)</code>. If there are any parsing errors, we retrieve and print them using a loop.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">config</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">create_builder_config</span><span class="p">()</span>
<span class="n">config</span><span class="p">.</span><span class="nf">set_memory_pool_limit</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">MemoryPoolType</span><span class="p">.</span><span class="n">WORKSPACE</span><span class="p">,</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">22</span><span class="p">)</span> <span class="c1"># 1 MiB
</span><span class="n">config</span><span class="p">.</span><span class="nf">set_flag</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">BuilderFlag</span><span class="p">.</span><span class="n">FP16</span><span class="p">)</span>

</code></pre></div></div>
<p>We create a builder configuration object (<code class="language-plaintext highlighter-rouge">config</code>) that allows us to configure various settings. Here, we create a memory pool limit configuration, setting the workspace memory pool limit to 1 MiB (1 « 22). The <code class="language-plaintext highlighter-rouge">config.set_flag(trt.BuilderFlag.FP16)</code> sets the quantization precision to <code class="language-plaintext highlighter-rouge">FP16</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">serialized_engine</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">build_serialized_network</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
</code></pre></div></div>
<p>Using the builder and configuration, we invoke <code class="language-plaintext highlighter-rouge">builder.build_serialized_network(network, config)</code> to build the TensorRT engine and obtain the serialized engine data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">../deploy_tools/resnet.engine</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">wb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">serialized_engine</span><span class="p">)</span>
</code></pre></div></div>
<p>Finally, we open a file named “resnet50.engine” in binary write mode (<code class="language-plaintext highlighter-rouge">"wb"</code>) using a <code class="language-plaintext highlighter-rouge">with open</code> block. We write the serialized engine data to the file, saving the TensorRT engine for future inference.</p>

<p>These lines of code collectively convert the provided ONNX model into a TensorRT engine, utilizing the TensorRT Python API and its optimization capabilities. Putting it all together, the final script is as follows :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pycuda.driver</span> <span class="k">as</span> <span class="n">cuda</span>
<span class="kn">import</span> <span class="n">pycuda.autoinit</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">tensorrt</span> <span class="k">as</span> <span class="n">trt</span>

<span class="n">TRT_LOGGER</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Logger</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">Logger</span><span class="p">.</span><span class="n">WARNING</span><span class="p">)</span>
<span class="n">builder</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Builder</span><span class="p">(</span><span class="n">TRT_LOGGER</span><span class="p">)</span>

<span class="n">EXPLICIT_BATCH</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="p">(</span><span class="nb">int</span><span class="p">)(</span><span class="n">trt</span><span class="p">.</span><span class="n">NetworkDefinitionCreationFlag</span><span class="p">.</span><span class="n">EXPLICIT_BATCH</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">create_network</span><span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="nf">int</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">NetworkDefinitionCreationFlag</span><span class="p">.</span><span class="n">EXPLICIT_BATCH</span><span class="p">))</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">OnnxParser</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">TRT_LOGGER</span><span class="p">)</span>
<span class="n">success</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="nf">parse_from_file</span><span class="p">(</span><span class="n">ONNX_FILE_PATH</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">parser</span><span class="p">.</span><span class="n">num_errors</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">parser</span><span class="p">.</span><span class="nf">get_error</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">create_builder_config</span><span class="p">()</span>
<span class="n">config</span><span class="p">.</span><span class="nf">set_memory_pool_limit</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">MemoryPoolType</span><span class="p">.</span><span class="n">WORKSPACE</span><span class="p">,</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">22</span><span class="p">)</span> <span class="c1"># 1 MiB
</span><span class="n">config</span><span class="p">.</span><span class="nf">set_flag</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">BuilderFlag</span><span class="p">.</span><span class="n">FP16</span><span class="p">)</span>

<span class="n">serialized_engine</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">build_serialized_network</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">../deploy_tools/resnet50.engine</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">wb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">serialized_engine</span><span class="p">)</span>
</code></pre></div></div>

<p><br></p>

<h3 id="step-4--performing-inference-on-tensorrt-engine">Step 4 : Performing Inference on TensorRT Engine</h3>

<p>Performing inference using the TensorRT engine is a breeze! We start by loading the serialized engine, which contains all the optimizations applied by TensorRT to our deep learning model. With the engine and execution context set up, we prepare input and output buffers that will hold the data for classification and predictions. The magic lies in TensorRT’s ability to work with GPU memory, tapping into the parallel processing power of the GPU for lightning-fast inference.</p>

<p>Next, we feed our input data, like an image, to the engine. It swiftly processes the data through its optimized network, streamlining calculations and fusing operations for maximum efficiency. The outcome? Rapid and accurate predictions for our image classification task.</p>

<p>Once the engine makes predictions on the GPU, we fetch the results by transferring the output data back to the CPU memory. This step enables us to perform any further post-processing as needed for our application’s specific requirements.</p>

<p>Let’s break down the provided inference code step by step and explain what each part does:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load the serialized TensorRT engine from the file
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">../deploy_tools/resnet.engine</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">serialized_engine</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">runtime</span><span class="p">.</span><span class="nf">deserialize_cuda_engine</span><span class="p">(</span><span class="n">serialized_engine</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">engine</span><span class="p">.</span><span class="nf">create_execution_context</span><span class="p">()</span>
</code></pre></div></div>

<p>In this part of the code, we start by loading the pre-built TensorRT engine from the file <code class="language-plaintext highlighter-rouge">resnet.engine</code> using a file stream. The engine’s serialized data is read as binary and stored in the variable <code class="language-plaintext highlighter-rouge">serialized_engine</code>. Next, we use the TensorRT runtime library to deserialize this binary data into a usable engine using <code class="language-plaintext highlighter-rouge">deserialize_cuda_engine()</code> function, which is then stored in the <code class="language-plaintext highlighter-rouge">engine</code> variable. The <code class="language-plaintext highlighter-rouge">Engine</code> object represents our highly optimized deep learning model tailored for execution on NVIDIA GPUs. We create an execution context named <code class="language-plaintext highlighter-rouge">context</code>, which allows us to interact with the engine and perform inference.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Determine dimensions and create page-locked memory buffers for host inputs/outputs
</span><span class="n">h_input</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">pagelocked_empty</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="nf">volume</span><span class="p">(</span><span class="n">engine</span><span class="p">.</span><span class="nf">get_binding_shape</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">h_output</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">pagelocked_empty</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="nf">volume</span><span class="p">(</span><span class="n">engine</span><span class="p">.</span><span class="nf">get_binding_shape</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
</code></pre></div></div>
<p>In this section, we determine the dimensions of the input and output bindings required by the TensorRT engine. The <code class="language-plaintext highlighter-rouge">engine.get_binding_shape(0)</code> returns the shape (dimensions) of the input binding, and <code class="language-plaintext highlighter-rouge">engine.get_binding_shape(1)</code> returns the shape of the output binding. In general, the input and output  bindings are stored serially. This implies that if there are multiple inputs are outputs, their shapes can be accessed using their resepctive indices in the bindings. For example, if we had 2 inputs and 1 output, the first two inputs will have indices <code class="language-plaintext highlighter-rouge">(0)</code> and <code class="language-plaintext highlighter-rouge">(1)</code>, while the output will have index <code class="language-plaintext highlighter-rouge">(2)</code>. We use <code class="language-plaintext highlighter-rouge">cuda.pagelocked_empty()</code> to create page-locked (pinned) memory buffers on the host (CPU) to hold the input and output data. Page-locked memory ensures that data transfers between the host (CPU) and the device (GPU) are efficient and do not involve memory swapping.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Allocate device memory for inputs and outputs.
</span><span class="n">d_input</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">mem_alloc</span><span class="p">(</span><span class="n">h_input</span><span class="p">.</span><span class="n">nbytes</span><span class="p">)</span>
<span class="n">d_output</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">mem_alloc</span><span class="p">(</span><span class="n">h_output</span><span class="p">.</span><span class="n">nbytes</span><span class="p">)</span>
</code></pre></div></div>
<p>In this part, we allocate device memory on the GPU for storing the input and output data during inference using <code class="language-plaintext highlighter-rouge">cuda.mem_alloc()</code> function. The size of the memory buffers is determined by the size of the page-locked memory buffers <code class="language-plaintext highlighter-rouge">h_input</code> and <code class="language-plaintext highlighter-rouge">h_output</code> that we created earlier.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create a CUDA stream to perform asynchronous memory transfers and execution
</span><span class="n">stream</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nc">Stream</span><span class="p">()</span>
</code></pre></div></div>
<p>Here, we create a CUDA stream named <code class="language-plaintext highlighter-rouge">stream</code>, which allows us to perform asynchronous memory transfers and inference execution on the GPU. Asynchronous execution helps overlap data transfers and computations, improving overall performance.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Preprocess the input image and transfer it to the GPU.
</span><span class="n">host_input</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">preprocess_image</span><span class="p">(</span><span class="sh">"</span><span class="s">../data/hotdog.jpg</span><span class="sh">"</span><span class="p">).</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">)</span>
<span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_htod_async</span><span class="p">(</span><span class="n">d_input</span><span class="p">,</span> <span class="n">host_input</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
</code></pre></div></div>
<p>In this part, we preprocess the input image, which in this case is <code class="language-plaintext highlighter-rouge">hotdog.jpg</code> to prepare it for inference. The image is converted to a NumPy array and set to dtype np.float32, which matches the data type expected by the TensorRT engine. The <code class="language-plaintext highlighter-rouge">preprocess_image</code> function is used to perform any necessary transformations or normalization specific to the model’s input requirements. The preprocessed input image is then asynchronously transferred from the host (CPU) to the device (GPU) memory using <code class="language-plaintext highlighter-rouge">cuda.memcpy_htod_async()</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Run inference on the TensorRT engine.
</span><span class="n">context</span><span class="p">.</span><span class="nf">execute_async_v2</span><span class="p">(</span><span class="n">bindings</span><span class="o">=</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">d_input</span><span class="p">),</span> <span class="nf">int</span><span class="p">(</span><span class="n">d_output</span><span class="p">)],</span> <span class="n">stream_handle</span><span class="o">=</span><span class="n">stream</span><span class="p">.</span><span class="n">handle</span><span class="p">)</span>
</code></pre></div></div>
<p>Here, we execute the inference on the TensorRT engine using the execution context’s <code class="language-plaintext highlighter-rouge">execute_async_v2()</code> function. We pass the input and output bindings to the engine using the bindings parameter, which is a list containing the device memory addresses of the input and output data. The stream_handle parameter ensures that the inference runs asynchronously on the GPU.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Transfer predictions back from the GPU to the CPU.
</span><span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_dtoh_async</span><span class="p">(</span><span class="n">h_output</span><span class="p">,</span> <span class="n">d_output</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
</code></pre></div></div>
<p>After the inference is complete, the output predictions reside in the device (GPU) memory. We use <code class="language-plaintext highlighter-rouge">cuda.memcpy_dtoh_async()</code> to transfer the predictions from the GPU to the host (CPU) memory in an asynchronous manner.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Synchronize the stream to wait for the inference to finish.
</span><span class="n">stream</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
</code></pre></div></div>
<p>Before accessing the output predictions on the CPU, we synchronize the CUDA stream using <code class="language-plaintext highlighter-rouge">stream.synchronize()</code>. This ensures that all GPU computations and data transfers are complete before we proceed to post-process the predictions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Convert the output data to a Torch tensor and perform post-processing.
</span><span class="n">tensorrt_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">h_output</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">postprocess</span><span class="p">(</span><span class="n">tensorrt_output</span><span class="p">)</span>
</code></pre></div></div>

<p>Finally, we convert the output predictions from the host (CPU) memory, which are stored in the h_output buffer, into a Torch tensor. The <code class="language-plaintext highlighter-rouge">unsqueeze(0)</code> operation is used to add a batch dimension to the tensor if required. The Torch tensor output_data now contains the final predictions obtained from the TensorRT engine. Depending on the specific task, we can perform further post-processing using the postprocess function to interpret the results and present them in a human-readable format.</p>

<p>Putting it all together, the inference on TensorRT engine script looks like this</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">runtime</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Runtime</span><span class="p">(</span><span class="n">TRT_LOGGER</span><span class="p">)</span>

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">../deploy_tools/resnet.engine</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">serialized_engine</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>
    
<span class="n">engine</span> <span class="o">=</span> <span class="n">runtime</span><span class="p">.</span><span class="nf">deserialize_cuda_engine</span><span class="p">(</span><span class="n">serialized_engine</span><span class="p">)</span>

<span class="n">context</span> <span class="o">=</span> <span class="n">engine</span><span class="p">.</span><span class="nf">create_execution_context</span><span class="p">()</span>

<span class="c1"># Determine dimensions and create page-locked memory buffers (i.e. won't be swapped to disk) to hold host inputs/outputs.
</span><span class="n">h_input</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">pagelocked_empty</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="nf">volume</span><span class="p">(</span><span class="n">engine</span><span class="p">.</span><span class="nf">get_binding_shape</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">h_input</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">pagelocked_empty</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="nf">volume</span><span class="p">(</span><span class="n">engine</span><span class="p">.</span><span class="nf">get_binding_shape</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">h_output</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">pagelocked_empty</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="nf">volume</span><span class="p">(</span><span class="n">engine</span><span class="p">.</span><span class="nf">get_binding_shape</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="c1"># Allocate device memory for inputs and outputs.
</span><span class="n">d_input</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">mem_alloc</span><span class="p">(</span><span class="n">h_input</span><span class="p">.</span><span class="n">nbytes</span><span class="p">)</span>
<span class="n">d_output</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">mem_alloc</span><span class="p">(</span><span class="n">h_output</span><span class="p">.</span><span class="n">nbytes</span><span class="p">)</span>
<span class="c1"># Create a stream in which to copy inputs/outputs and run inference.
</span><span class="n">stream</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nc">Stream</span><span class="p">()</span>
<span class="c1"># read input image and preprocess
</span><span class="n">host_input</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">preprocess_image</span><span class="p">(</span><span class="sh">"</span><span class="s">../data/hotdog.jpg</span><span class="sh">"</span><span class="p">).</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">)</span>
<span class="c1"># Transfer input data to the GPU.
</span><span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_htod_async</span><span class="p">(</span><span class="n">d_input</span><span class="p">,</span> <span class="n">host_input</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
<span class="c1"># Run inference.
</span><span class="n">context</span><span class="p">.</span> <span class="nf">execute_async_v2</span><span class="p">(</span><span class="n">bindings</span><span class="o">=</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">d_input</span><span class="p">),</span> <span class="nf">int</span><span class="p">(</span><span class="n">d_output</span><span class="p">)],</span> <span class="n">stream_handle</span><span class="o">=</span><span class="n">stream</span><span class="p">.</span><span class="n">handle</span><span class="p">)</span>
<span class="c1"># Transfer predictions back from the GPU.
</span><span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_dtoh_async</span><span class="p">(</span><span class="n">h_output</span><span class="p">,</span> <span class="n">d_output</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
<span class="c1"># Synchronize the stream
</span><span class="n">stream</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
<span class="c1"># postprocess output
</span><span class="n">tensorrt_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">h_output</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">postprocess</span><span class="p">(</span><span class="n">tensorrt_output</span><span class="p">)</span>
</code></pre></div></div>

<p>With this, the inference process using the TensorRT engine is complete. The optimized engine takes advantage of GPU acceleration and sophisticated optimizations to deliver rapid and efficient predictions for our image classification model.</p>

<p><br></p>

<h3 id="time-to-retrospect">Time to Retrospect</h3>
<p><br></p>

<h4 id="consistency-validation">Consistency Validation</h4>

<p>As we draw the curtains on our exploration of PyTorch and TensorRT in the world of image classification, it’s time to reflect on the intriguing findings of our journey. One crucial aspect of this quest was comparing the output values obtained from both PyTorch and TensorRT after quantization. With a keen eye for accuracy, we scrutinized the outputs to measure any discrepancies introduced during the optimization process. Our quest for precision revealed that while quantization and optimization indeed caused minute deviations in the output values, these variations were negligible and well within acceptable limits. Thus, we could confidently establish the compatibility and reliability of TensorRT’s quantization process in preserving the essence of our image classification task.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Calculate MAE between pure torch output and TensorRT inference output
</span><span class="n">mae</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="nf">cpu</span><span class="p">()</span> <span class="o">-</span> <span class="n">tensorrt_output</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">MAE:</span><span class="sh">"</span><span class="p">,</span> <span class="n">mae</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
</code></pre></div></div>

<p>The output of this block of code looks would look something like :</p>

<p><code class="language-plaintext highlighter-rouge">MAE: 0.00590712483972311</code></p>

<p>The Mean Absolute Error between torch <code class="language-plaintext highlighter-rouge">ouput</code> and <code class="language-plaintext highlighter-rouge">tensorrt_output</code> is tending towards zero which indicates that even the post quantization results are similar ( if not equal ) to the pure pytorch output.</p>

<p><br></p>

<h4 id="latency-measurement">Latency Measurement</h4>

<p>In this analysis, we are focused on comparing the inference speed between pure PyTorch and TensorRT. To achieve this, we run each inference method for multiple iterations and measure the time it takes to process a single input image. By doing so, we gain valuable insights into the real-world performance of both approaches.</p>

<p>For the pure PyTorch inference, we employ a pre-trained ResNet model and run the image classification task multiple times, recording the time taken to process each image. The average latency is then calculated over the specified number of iterations. On the other hand, for the TensorRT inference, we have optimized the same ResNet model using TensorRT and leveraged GPU acceleration to further speed up the inference process. Once again, we run the image classification task multiple times and calculate the average latency.</p>

<p>By comparing the average latencies of both methods, we can quantitatively gauge the speedup offered by TensorRT over pure PyTorch. This performance analysis provides a clear picture of the benefits that TensorRT’s optimization and GPU acceleration bring to the table, paving the way for more efficient and rapid deployment of deep learning models in real-world applications. With these results in hand, we can confidently choose the best inference approach tailored to our specific needs, whether it’s maximum accuracy with PyTorch or lightning-fast performance with TensorRT.</p>

<p>Here is the script to measure latency of pure torch inference.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pure torch latency measurement 
</span><span class="kn">import</span> <span class="n">time</span>

<span class="c1"># Pure PyTorch Inference
</span><span class="k">def</span> <span class="nf">pytorch_inference</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>

<span class="c1"># Number of iterations
</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Run Pure PyTorch Inference for 1000 iterations
</span><span class="n">total_pytorch_latency</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">latency</span> <span class="o">=</span> <span class="nf">pytorch_inference</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
        <span class="n">total_pytorch_latency</span> <span class="o">+=</span> <span class="n">latency</span>
<span class="n">average_pytorch_latency</span> <span class="o">=</span> <span class="p">(</span><span class="n">total_pytorch_latency</span> <span class="o">/</span> <span class="n">num_iterations</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">empty_cache</span><span class="p">()</span>
</code></pre></div></div>

<p>And, now let us look at how to measure inference speedup offered by TensorRT engine.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># TensorRT latency measurement 
</span><span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="c1"># TensorRT FP16 Inference
</span><span class="k">def</span> <span class="nf">tensorrt_inference</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">d_input</span><span class="p">,</span> <span class="n">d_output</span><span class="p">,</span> <span class="n">host_input</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_htod_async</span><span class="p">(</span><span class="n">d_input</span><span class="p">,</span> <span class="n">host_input</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">context</span><span class="p">.</span><span class="nf">execute_async_v2</span><span class="p">(</span><span class="n">bindings</span><span class="o">=</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">d_input</span><span class="p">),</span> <span class="nf">int</span><span class="p">(</span><span class="n">d_output</span><span class="p">)],</span> <span class="n">stream_handle</span><span class="o">=</span><span class="n">stream</span><span class="p">.</span><span class="n">handle</span><span class="p">)</span>
    <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_dtoh_async</span><span class="p">(</span><span class="n">h_output</span><span class="p">,</span> <span class="n">d_output</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">stream</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
    <span class="n">tensorrt_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">h_output</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">tensorrt_output</span><span class="p">,</span> <span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>

<span class="c1"># Number of iterations
</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Run TensorRT FP16 Inference for 1000 iterations
</span><span class="n">total_tensorrt_latency</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">tensorrt_output</span><span class="p">,</span> <span class="n">latency</span> <span class="o">=</span> <span class="nf">tensorrt_inference</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">d_input</span><span class="p">,</span> <span class="n">d_output</span><span class="p">,</span> <span class="n">host_input</span><span class="p">)</span>
        <span class="n">total_tensorrt_latency</span> <span class="o">+=</span> <span class="n">latency</span>
<span class="n">average_tensorrt_latency</span> <span class="o">=</span> <span class="p">(</span><span class="n">total_tensorrt_latency</span> <span class="o">/</span> <span class="n">num_iterations</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>

</code></pre></div></div>

<p>Now, let’s visualize the comparison of inference latencies between pure PyTorch and TensorRT using a bar chart.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">latencies</span> <span class="o">=</span> <span class="p">[</span><span class="n">average_pytorch_latency</span><span class="p">,</span> <span class="n">average_tensorrt_latency</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Pure PyTorch</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">TensorRT</span><span class="sh">'</span><span class="p">]</span>

<span class="c1"># Create a bar chart
</span><span class="n">plt</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">latencies</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">])</span>

<span class="c1"># Add labels and title
</span><span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Inference Method</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Average Latency (ms)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Comparison of Latency: Pure PyTorch vs. TensorRT</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Show the plot
</span><span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p>We have collected the average latencies for both methods and stored them in the latencies list, while the corresponding method names, <code class="language-plaintext highlighter-rouge">Pure PyTorch</code> and <code class="language-plaintext highlighter-rouge">TensorRT</code>, are in the labels list.</p>

<p>Using the <code class="language-plaintext highlighter-rouge">matplotlib.pyplot.bar()</code> function, we create a bar chart where each bar represents one of the inference methods. The height of each bar corresponds to the average latency of that method, measured in milliseconds. We have assigned distinct colors, ‘blue’ for pure PyTorch and ‘green’ for TensorRT, making it easy to visually differentiate between the two.</p>

<p>The output plot would look as follows :</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_1/latency_compare-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_1/latency_compare-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_1/latency_compare-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_1/latency_compare.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Latency comparison between Pure Torch and TensorRT inference.
</div>

<p>In the bar chart, we have two bars representing the inference methods: ‘Pure PyTorch’ and ‘TensorRT.’ The height of each bar represents the average latency measured in milliseconds for each method. The average torch latency is approximately <code class="language-plaintext highlighter-rouge">5.50 ms</code>, while the average tensorrt latency is approximately <code class="language-plaintext highlighter-rouge">1.48 ms</code>.</p>

<p>The significant disparity between the two bars immediately catches our attention. The ‘TensorRT’ bar is remarkably shorter than the ‘Pure PyTorch’ bar, indicating that TensorRT outperforms PyTorch in terms of inference speed. The speedup offered by TensorRT can be calculated as the ratio of the average torch latency to the average tensorrt latency:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Speedup = Average Torch Latency / Average TensorRT Latency
Speedup = 5.50 ms / 1.48 ms ≈ 3.71
</code></pre></div></div>

<p>This means that TensorRT achieves an impressive speedup of approximately <strong><code class="language-plaintext highlighter-rouge">3.71 times</code></strong> faster than pure PyTorch. Such a significant improvement in inference speed can have a profound impact on real-world applications, enabling faster response times and enhancing overall system efficiency.</p>

<p><br></p>

<h3 id="conclusion">Conclusion</h3>
<p>In conclusion, our journey through image classification using PyTorch and TensorRT has been an enlightening experience. We witnessed the power of PyTorch in providing accurate and reliable classification results. However, the real revelation came when we optimized the model using TensorRT.</p>

<p>TensorRT’s quantization and GPU acceleration brought remarkable benefits to the table. We observed a negligible error in the output values after quantization, ensuring the preservation of accuracy. The speedup comparison was awe-inspiring, with TensorRT demonstrating its prowess by achieving a speedup of approximately 3.71 times faster than pure PyTorch.</p>

<p>This performance boost provided by TensorRT opens up new avenues for deploying deep learning models in real-time applications where speed and efficiency are crucial. With PyTorch for precision and TensorRT for optimization, we are equipped to tackle diverse AI challenges with unmatched accuracy and exceptional speed.</p>

<p>As we conclude this journey, we stand confident in embracing the synergistic power of PyTorch and TensorRT, paving the way for transformative advancements in the world of AI and deep learning. The road ahead beckons, and we look forward to applying these invaluable insights to usher in a new era of intelligent applications and cutting-edge innovations.</p>

<p>In the upcoming part of the blog, we will delve into the world of C++ and explore how to build the TensorRT engine and perform inference for the same image classification model. Transitioning from Python to C++ empowers us with the potential to deploy our optimized models in production environments with even greater efficiency. We will witness firsthand the seamless integration of TensorRT’s powerful optimizations and GPU acceleration with C++ code, unlocking the full potential of our deep learning model in high-performance applications. Get ready to embark on the next phase of our exciting journey into the realm of C++ and TensorRT!</p>

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/that-first-cuda-blog-3/">That First CUDA Blog I Needed :Part 3</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/that-first-cuda-blog-2/">That First CUDA Blog I Needed :Part 2</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/that-first-cuda-blog-1/">That First CUDA Blog I Needed</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/that-first-cuda-blog/">That First CUDA Blog I Needed</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/hidden-speed-in-shared-memory/">Hidden Speed in CUDA's Shared Memory</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Sanket R. Shah. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
