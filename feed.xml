<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://sanket-pixel.github.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://sanket-pixel.github.io//" rel="alternate" type="text/html" hreflang="en" /><updated>2025-02-02T05:44:38+00:00</updated><id>https://sanket-pixel.github.io//feed.xml</id><title type="html">Sanket Shah</title><subtitle></subtitle><entry><title type="html">That First CUDA Blog I Needed</title><link href="https://sanket-pixel.github.io//blog/2024/that-first-cuda-blog/" rel="alternate" type="text/html" title="That First CUDA Blog I Needed" /><published>2024-10-20T14:53:00+00:00</published><updated>2024-10-20T14:53:00+00:00</updated><id>https://sanket-pixel.github.io//blog/2024/that-first-cuda-blog</id><content type="html" xml:base="https://sanket-pixel.github.io//blog/2024/that-first-cuda-blog/"><![CDATA[<h4 id="in-this-blog-were-going-to-dive-into-one-of-the-most-critical-concepts-in-cuda-programming-shared-memory-shared-memory-is-like-the-secret-ingredient-that-can-supercharge-your-gpu-code-while-cudas-global-memory-serves-as-the-main-storage-its-often-slow-to-access-repeatedly-thats-where-shared-memory-comes-in-it-acts-as-a-customizable-fast-access-scratchpad-where-you-can-store-data-that-is-frequently-reused-by-threads-within-the-same-block-helping-you-avoid-costly-memory-transfers-well-explore-how-this-works-why-it-matters-and-how-you-can-use-it-to-make-your-cuda-programs-much-faster">In this blog, we’re going to dive into one of the most critical concepts in CUDA programming: shared memory. Shared memory is like the secret ingredient that can supercharge your GPU code. While CUDA’s global memory serves as the main storage, it’s often slow to access repeatedly. That’s where shared memory comes in. It acts as a customizable, fast-access scratchpad where you can store data that is frequently reused by threads within the same block, helping you avoid costly memory transfers. We’ll explore how this works, why it matters, and how you can use it to make your CUDA programs much faster.</h4>
<p><br /></p>

<div style="width: 80%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_8/cuda-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_8/cuda-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_8/cuda-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_8/cuda.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
   Let us begin the CUDA journey, the right way.
</div>
</div>

<!-- ``` ### 0. Prerequisites to learn CUDA``` -->

<h3 id="1-hello-world">1. Hello World</h3>
<p>A good way to learn something new, is to begin from something you already know and then connect the dots. Let us first look at a simple <code class="language-plaintext highlighter-rouge">Hello World</code> program in C++.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span><span class="kt">int</span> <span class="nf">main</span><span class="p">(){</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Hello World!"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>To execute this program on your machine, follow the following steps :</p>

<ol>
  <li>Open a terminal and navigate to the directory containing this <code class="language-plaintext highlighter-rouge">.cpp</code> file.
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> cd that_first_cuda_blog/1_hello_world
</code></pre></div>    </div>
  </li>
  <li>Compile the program using the following command
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> g++ hello_world.cpp <span class="nt">-o</span> hello_world
</code></pre></div>    </div>
  </li>
  <li>This will create an executable <code class="language-plaintext highlighter-rouge">hello_world</code> in this directory. Execute it using
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> ./hello_world
</code></pre></div>    </div>
  </li>
</ol>

<p>This should print <code class="language-plaintext highlighter-rouge">Hello World!</code> in the terminal, as expected. Here, the <strong>g++ compiler</strong> compiles the source code <code class="language-plaintext highlighter-rouge">hello_world.cpp</code> and translates it to machine code, in form of an executable file. 
The CPU then executes this machine code, to print <code class="language-plaintext highlighter-rouge">Hello World!</code> onto the temrinal.</p>

<p>If one intends to execute the same on an NVIDIA GPU, <strong>CUDA</strong> can be used.<br />
CUDA is a programming framework, that allows programmers to talk to NVIDIA GPUs via the CPU.</p>

<p><strong>TODO : TALK HERE ABOUT HOW CUDA HAS PARALLILAZATION USING THREADS</strong></p>

<p>Let us look at simple Hello World example in CUDA.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">gpu_hello_world</span><span class="p">(){</span>
  <span class="n">printf</span><span class="p">(</span><span class="s">"Hello World from GPU! </span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(){</span>
  <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Hello World from CPU!"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
  <span class="n">gpu_hello_world</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span>
  <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div>

<p>To execute this program on your machine, follow the following steps :</p>

<ol>
  <li>Open a terminal and navigate to the <code class="language-plaintext highlighter-rouge">1_hello_world</code> directory
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> cd that_first_cuda_blog/1_hello_world
</code></pre></div>    </div>
  </li>
  <li>Compile the program using the following command
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> nvcc hello_world_gpu.cu <span class="nt">-o</span> hello_world_gpu
</code></pre></div>    </div>
  </li>
  <li>This will create an executable <code class="language-plaintext highlighter-rouge">hello_world_gpu</code> in this directory. Execute it using
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> ./hello_world_gpu
</code></pre></div>    </div>
  </li>
  <li>The output should be the following
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> Hello World from CPU!
 Hello World from GPU! 
</code></pre></div>    </div>
  </li>
</ol>

<p>Now that we have our first CUDA program running, let us dissect this CUDA program, and understand how it works from first principles. <br /></p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">gpu_hello_world</span><span class="p">(){</span>
  <span class="n">printf</span><span class="p">(</span><span class="s">"Hello World from GPU! </span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The code snippet above is a function that is intended to run on the GPU. <br /> 
In the GPU jargon, such a function is called <strong>kernel</strong>. <br /></p>

<p>Kernel, specifically, is a special function, that can be invoked from the CPU, but runs only on the GPU.
CPU, is generally referred to as <code class="language-plaintext highlighter-rouge">host</code> and GPU is referred to as <code class="language-plaintext highlighter-rouge">device</code>, since the CPU hosts the GPU in some sense.
The <code class="language-plaintext highlighter-rouge">__global__</code> keyword is used to specify that this function is a <strong>kernel</strong>, in that, it can be called from the host but executed on the device.</p>

<p><code class="language-plaintext highlighter-rouge">gpu_hello_world&lt;&lt;&lt;1,1&gt;&gt;&gt;();</code> is a CUDA-specific syntax. We will discuss what <code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;1,1&gt;&gt;&gt;</code> means later in this blog. 
For now it is sufficient to understand that <code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;1,1&gt;&gt;&gt;</code>, allocates 1 thread for executing this kernel.</p>

<p>Let us first understand how this kernel launch works from first principles.</p>

<ol>
  <li>The <code class="language-plaintext highlighter-rouge">host</code> (CPU) executes instructions ( compiled lines of code ), one at a time, sequentially.</li>
  <li>When it reaches the kernel launch instruction (<code class="language-plaintext highlighter-rouge">gpu_hello_world&lt;&lt;&lt;1,1&gt;&gt;&gt;();</code>), the host launches the kernel.
Under the hood, the <strong>CUDA Runtime Library</strong> on the host, places the launch command onto a <strong>CUDA Stream</strong> which a queue mantained 
on the host.  This queue is designed to hold kernel launches, memory transfer requests and other CUDA tasks, to ensure they execute sequentially for the same <strong>CUDA Stream</strong>.
We will dissect  <strong>CUDA Streams</strong> later in this blog.</li>
  <li>The CUDA Runtime, now hands over the launch commands to the <strong>NVIDIA Driver</strong> on the <code class="language-plaintext highlighter-rouge">host</code>, which is responsible for talking to the <code class="language-plaintext highlighter-rouge">device</code> (GPU).</li>
  <li>The <strong>NVIDIA Driver</strong> pushes this launch command to the <strong>command buffer</strong> which is managed by the GPU hardware. This buffer resides on the <code class="language-plaintext highlighter-rouge">device</code> and 
holds the commands to be executed, once sufficient GPU resources are available.</li>
  <li>The GPU, once resources are available, pulls commands from the <strong>command buffer</strong> and starts executing them.</li>
  <li>The <strong>host does not wait for the kernel execution to finish</strong>, and moves on with the next instruction. This execution approach is known as <strong>asynchronous</strong>. In particular, the <code class="language-plaintext highlighter-rouge">host</code> and <code class="language-plaintext highlighter-rouge">device</code> executes independently and simulatenously. When a command is like kernel launch is issued by the host, it does not wait for the command to complete on the <code class="language-plaintext highlighter-rouge">device</code>, but simply moves on to the next instruction, while the <code class="language-plaintext highlighter-rouge">device</code> handles the requested operation in parallel.</li>
</ol>

<p>To unerstand this better, we can change the earlier <code class="language-plaintext highlighter-rouge">hello_world.cu</code> source code, by commenting out <code class="language-plaintext highlighter-rouge">cudaDeviceSynchronize();</code>.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">gpu_hello_world</span><span class="p">(){</span>
  <span class="n">printf</span><span class="p">(</span><span class="s">"Hello World from GPU! </span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(){</span>
  <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Hello World from CPU!"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
  <span class="n">gpu_hello_world</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span>
  <span class="c1">// comment out this line. </span>
  <span class="c1">// Now the host does not wait for the device and moves on.</span>
  <span class="c1">// cudaDeviceSynchronize();</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The output of this program will be just as follows :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Hello World from CPU!
</code></pre></div></div>

<p>Note, that since we removed <code class="language-plaintext highlighter-rouge">cudaDeviceSynchronize();</code>, the host launches the <code class="language-plaintext highlighter-rouge">gpu_hello_world</code> kernel and moves on to the next instruction. The exection of the host code finishes, even before the <code class="language-plaintext highlighter-rouge">device</code> completes, hence it does not print <code class="language-plaintext highlighter-rouge">Hello World from GPU!</code> onto the output buffer.</p>

<p>Let us now extend our single thread CUDA Hello World, to run it with 8 threads. We would like the GPU to repeat this same “Hello World from GPU” operation 8 times. Just one small change in our original code will make this happen.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">gpu_hello_world</span><span class="p">(){</span>
  <span class="n">printf</span><span class="p">(</span><span class="s">"Hello World from GPU! </span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(){</span>
  <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Hello World from CPU!"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
  <span class="c1">// HERE , we replace &lt;&lt;&lt;1,1&gt;&gt;&gt; with &lt;&lt;&lt;1,8&gt;&gt;&gt;.</span>
  <span class="n">gpu_hello_world</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span>
  <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The output of this code, will be one <code class="language-plaintext highlighter-rouge">Hello World from CPU!</code> and 8 <code class="language-plaintext highlighter-rouge">Hello World from GPU!</code>s. 
The main change as explained in the comment above the kernel code is replace <code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;1,1&gt;&gt;&gt;</code> with <code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;1,8&gt;&gt;&gt;</code>, which essentially
means launching the same kernel with 8 threads. The GPU runs 8 “print Hello World” operations in parallel.</p>

<p>We will understand what <code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;1,8&gt;&gt;&gt;</code> exactly means in absolute detail, but at this point, it is sufficient to understand that <code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;1,1&gt;&gt;&gt;</code>
launches one thread and <code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;1,8&gt;&gt;&gt;</code> launches 8 threads in parallel.</p>

<p>In summary, in this <strong>Hello World</strong> section, we first looked at how to print Hello World using the CPU, followed by the same using the GPU.
The major takeaway from this section is to understand what are kernels in general, and how <em>exactly</em> is a kernel launched from the <code class="language-plaintext highlighter-rouge">host</code>, to run the same operations in parallel on the <code class="language-plaintext highlighter-rouge">device</code>.</p>

<!-- TODO : ADD OS Concepts here -->

<h3 id="2-print-square-of-numbers">2. Print Square of Numbers</h3>
<p>The basic foundation is now laid and we will now lay some more foundation on top.
Let us print square of list of integers.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span>
<span class="cp">#define N 5
</span><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">print_square</span><span class="p">(){</span>
  <span class="kt">unsigned</span> <span class="n">id</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="n">printf</span><span class="p">(</span><span class="s">"%d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">id</span> <span class="o">*</span> <span class="n">id</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(){</span>
  <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">N</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">){</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">i</span> <span class="o">*</span> <span class="n">i</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="n">print_square</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span><span class="n">N</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span>
  <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="cuda" /><category term="nvidia" /><category term="cuda" /><summary type="html"><![CDATA[The ideal first blog to start learning CUDA.]]></summary></entry><entry><title type="html">Hidden Speed in CUDA’s Shared Memory</title><link href="https://sanket-pixel.github.io//blog/2024/hidden-speed-in-shared-memory-copy/" rel="alternate" type="text/html" title="Hidden Speed in CUDA’s Shared Memory" /><published>2024-09-07T14:53:00+00:00</published><updated>2024-09-07T14:53:00+00:00</updated><id>https://sanket-pixel.github.io//blog/2024/hidden-speed-in-shared-memory%20copy</id><content type="html" xml:base="https://sanket-pixel.github.io//blog/2024/hidden-speed-in-shared-memory-copy/"><![CDATA[<h4 id="in-this-blog-were-going-to-dive-into-one-of-the-most-critical-concepts-in-cuda-programming-shared-memory-shared-memory-is-like-the-secret-ingredient-that-can-supercharge-your-gpu-code-while-cudas-global-memory-serves-as-the-main-storage-its-often-slow-to-access-repeatedly-thats-where-shared-memory-comes-in-it-acts-as-a-customizable-fast-access-scratchpad-where-you-can-store-data-that-is-frequently-reused-by-threads-within-the-same-block-helping-you-avoid-costly-memory-transfers-well-explore-how-this-works-why-it-matters-and-how-you-can-use-it-to-make-your-cuda-programs-much-faster">In this blog, we’re going to dive into one of the most critical concepts in CUDA programming: shared memory. Shared memory is like the secret ingredient that can supercharge your GPU code. While CUDA’s global memory serves as the main storage, it’s often slow to access repeatedly. That’s where shared memory comes in. It acts as a customizable, fast-access scratchpad where you can store data that is frequently reused by threads within the same block, helping you avoid costly memory transfers. We’ll explore how this works, why it matters, and how you can use it to make your CUDA programs much faster.</h4>
<p><br /></p>

<div style="width: 80%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_7/shared-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_7/shared-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_7/shared-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_7/shared.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
   Sharing is Caring. 
</div>
</div>

<p>CUDA, is almost like the default framework for optimizing code on NVIDIA GPUs with parallelization. It enables us to harness the massive power of GPUs to solve complex problems at incredible speed. Whether it’s deep learning, simulations, or graphics, CUDA allows us to break down large tasks into smaller, manageable chunks and run them simultaneously on thousands of cores. It made NVIDIA, what it is today.</p>

<p>In this blog, we’re going to dive into one of the most critical concepts in CUDA programming: shared memory. Shared memory is like the secret ingredient that can supercharge your GPU code. While CUDA’s global memory serves as the main storage, it’s often slow to access repeatedly. That’s where shared memory comes in. It acts as a customizable, fast-access scratchpad where you can store data that is frequently reused by threads within the same block, helping you avoid costly memory transfers.</p>

<p>Think of it as a private workspace that all the threads in a block can share to optimize performance. We’ll explore how this works, why it matters, and how you can use it to make your CUDA programs much faster. Let’s jump into the magic of shared memory and see how it makes GPU computing even more efficient!</p>

<p>All the code used in this blog can be found <a href="https://github.com/sanket-pixel/CUDA/blob/main/shared_memory_blog/">here</a>.</p>

<p>Before we discuss the intricacies, intuition and examples of Shared Memory, let us first understand ( revisit ) the basics of CUDA and NVIDIA GPUs Memory Heirarchy.</p>

<h4 id="0-basics-of-cuda">0. Basics of CUDA</h4>

<p>At the core of CUDA programming is a hierarchical model that defines how code is executed in parallel. This model consists of three key components: threads, blocks, and grids.</p>

<ul>
  <li><strong>Thread</strong>: The smallest unit of execution in CUDA, a thread performs the same operation on different data elements in parallel.</li>
  <li><strong>Block</strong>: A group of threads that work together. Threads within a block can share data and synchronize their execution through shared memory.</li>
  <li><strong>Grid</strong>: A collection of blocks that execute the same kernel function. Each block operates independently, allowing CUDA to manage a vast number of threads efficiently.</li>
</ul>

<p>To illustrate how these components work together, let’s consider a simple example.
Imagine we have an array of 100,000 numbers, and we want to add 1 to each element.The code can be found <a href="https://github.com/sanket-pixel/CUDA/blob/main/shared_memory_blog/basics/basics.cu">here</a>.</p>

<p>In a traditional CPU-based approach, this operation would be done sequentially, but with CUDA, we can process the array elements in parallel.
Let us first look at how the operation on CPU would look like :</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">h_array</span><span class="p">[</span><span class="n">ARRAY_SIZE</span><span class="p">];</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">ARRAY_SIZE</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">h_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>This code runs on the CPU, processing each element one by one—an approach that’s inefficient for large datasets.
Now, let us do the same operation, but using parallelization on GPU using CUDA.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">add_one</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">array</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">idx</span> <span class="o">&lt;</span> <span class="n">ARRAY_SIZE</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">array</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
    <span class="c1">// allocate memory on the device (GPU)</span>
    <span class="kt">int</span><span class="o">*</span> <span class="n">d_array</span><span class="p">;</span>
    <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_array</span><span class="p">,</span> <span class="n">ARRAY_SIZE</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
    <span class="c1">// copy data from host to device</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_array</span><span class="p">,</span> <span class="n">h_array</span><span class="p">,</span> <span class="n">ARRAY_SIZE</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
    <span class="c1">// compute total blocks required</span>
    <span class="kt">unsigned</span> <span class="n">total_threads</span> <span class="o">=</span> <span class="n">ARRAY_SIZE</span><span class="p">;</span>
    <span class="kt">unsigned</span> <span class="n">total_blocks</span> <span class="o">=</span> <span class="kt">int</span><span class="p">(</span><span class="n">total_threads</span><span class="o">/</span><span class="n">BLOCK_SIZE</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">;</span>
    <span class="c1">// Launch the kernel with 1000 blocks of 128 threads each</span>
    <span class="n">add_one</span><span class="o">&lt;&lt;&lt;</span><span class="n">total_blocks</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_array</span><span class="p">);</span>  <span class="c1">// Launching 1000 blocks with 128 threads each</span>
    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
    <span class="c1">// Copy the results back to the host</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">d_array</span><span class="p">,</span> <span class="n">ARRAY_SIZE</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
    <span class="c1">// Free device memory</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_array</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The biggest takeaway from this example is how the size of the array is used to compute the required number of thread blocks and how the index of the current thread is computed within the kernel. For more details on the same, please refer to our previous blog on <a href="/blog/2023/cuda-it-be-any-faster/">CUDA it Be Any Faster?</a> where we cover the basics of CUDA.</p>

<p>Here are the key points we need to remember in order to understand the conept of shared memory, explained in this blog :</p>
<ul>
  <li>All threads are divided amongst several thread blocks, where each block contains 1024 threads.</li>
  <li>Each thread block executes independently of each other.</li>
  <li>Each thread block is further divided into warps, with each warp consisting of 32 threads.</li>
  <li>All threads within a warp execute the same intstruction at a given time.</li>
  <li>Two threads in the same thread block but different thread warp may not execute the same instruction at the same time.</li>
</ul>

<h3 id="1-basics-of-cuda-memory-heirarchy">1. Basics of CUDA Memory Heirarchy</h3>
<p>When data is copied from the CPU to the GPU using <code class="language-plaintext highlighter-rouge">cudaMemcpy</code>, it is transferred into the GPU’s RAM, referred to as <strong>Global Memory</strong>. However, modern NVIDIA GPUs are engineered for high performance and include additional memory components designed to accelerate processing. Beyond just global memory, GPUs feature specialized memory types like <strong>Shared Memory</strong>, <strong>Constant Memory</strong>, and <strong>Texture Memory</strong>, each with unique characteristics that can be leveraged to significantly boost performance. These memory types allow developers to optimize data access patterns, reduce latency, and maximize the computational power of the GPU, making them a critical part of achieving extreme performance in GPU-accelerated applications. Let us look at all these memory components in more detail.</p>

<div style="width: 70%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_7/memory-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_7/memory-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_7/memory-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_7/memory.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Memory" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
   Memory Heirarcy on NVIDIA GPUs
</div>
</div>

<p>Each <em>thread block</em> in CUDA is allocated its own <strong>Shared Memory</strong> of size <em>64KB</em>, which is an on-chip memory accessible by all threads within the block. This shared memory facilitates fast, low-latency communication and data exchange between threads in the same block. Similarly, each individual <em>thread</em> is allocated its own <strong>Register</strong>, each of size <em>4 bytes (32 bits)</em>, which is an even faster, on-chip storage used for holding temporary variables and performing computations. Registers are private to each thread, ensuring quick access and minimal latency for the thread’s operations. These pieces of information is enough, for the context of learning about leveraging Shared Memory for boosting CUDA performance. We will look at details of these types of memory in a separate blog post.</p>

<table>
  <thead>
    <tr>
      <th>Memory Type</th>
      <th>Characteristics</th>
      <th>Advantages</th>
      <th>Trade-offs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Global Memory</strong></td>
      <td>- Accessible by all threads across all blocks<br />- Largest memory space on the GPU<br />- High latency (hundreds of clock cycles)<br />- Used for storing data shared among threads or blocks</td>
      <td>- Flexibility in accessing large amounts of data</td>
      <td>- High latency for frequent access<br />- Requires careful management for data coalescing</td>
    </tr>
    <tr>
      <td><strong>Constant Memory</strong></td>
      <td>- Read-only and cached<br />- Accessible by all threads<br />- Limited size (typically 64KB)</td>
      <td>- Fast access for broadcast reads<br />- Suitable for data accessed by all threads</td>
      <td>- Small size<br />- Read-only nature limits flexibility</td>
    </tr>
    <tr>
      <td><strong>Texture Memory</strong></td>
      <td>- Read-only and cached<br />- Optimized for spatial locality and supports addressing modes and filtering<br />- Typically used for 2D data</td>
      <td>- Efficient for 2D spatial locality<br />- Supports specialized access patterns</td>
      <td>- Limited use cases<br />- Overhead for non-2D data access</td>
    </tr>
    <tr>
      <td><strong>L1/Shared Memory</strong></td>
      <td>- On-chip and shared among threads within the same block<br />- Low latency<br />- Limited size (typically 48KB per SM)</td>
      <td>- Extremely fast access for intra-block communication<br />- Reduces global memory access</td>
      <td>- Limited size<br />- Only accessible within the same block</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h3 id="2-what-is-shared-memory">2. What is Shared Memory?</h3>

<p>At the most granular level in CUDA, we have <strong>Threads</strong>. These threads are organized into <strong>Thread Blocks</strong>, and multiple Thread Blocks form a <strong>Grid</strong>.  Now, each Thread Block has its own L1 cache assigned, which is common for all the threads within that block. This L1 cache possesses a unique feature: <em>it is programmable by the user</em>. When the user decides to control what data is stored in this L1 cache ( instead of the GPU driver ), this memory is termed as <strong>Shared Memory</strong>. The term “Shared” highlights the key aspect of this memory type: the data stored in Shared Memory is accessible to all threads within the same Thread Block. Unlike the traditional cache mechanism, which automatically handles data movement between global memory and L1 cache, shared memory gives programmers direct control over what data is loaded and how it is used.</p>

<p>The design choice behind shared memory stems from the inherent limitations of standard caching. The GPU, while powerful, cannot always predict the best way to utilize cache for specific workloads. It merely attempts to copy contiguous data from global memory to L1 cache and hopes for optimal cache hits. However, real-world applications often require tailored data handling to maximize performance and minimize latency.</p>

<p>This is where shared memory shines. By allowing programmers to explicitly manage the data that gets copied to shared memory, developers can optimize performance for their unique use cases. Shared memory serves as a “scratchpad” memory, where data that is frequently accessed by threads within the same block can be stored and manipulated. In the absence of such a feature, every thread will have to perform read and write to the global memory, which is very expensive. Having frequently accessed and shared data in fast L1 cache reduces the number of costly global memory accesses, which are significantly slower compared to shared memory operations.</p>

<p>In essence, shared memory enables a more efficient collaboration among threads in a block, facilitating faster data sharing and improved performance in parallel computations. Leveraging shared memory effectively can lead to significant performance gains in applications such as image processing, matrix multiplication, and complex simulations, where data reuse and minimizing memory latency are crucial.</p>

<p>Let us understand this with a simple analogy.</p>

<h3 id="3-chefs-making-tomato-soup">3. Chefs making Tomato Soup</h3>
<p>Imagine a scenario where multiple chefs are trying to make delicious tomato soup. However, the tomatoes they need are stored in a large fridge located on the ground floor (representing Global Memory). Every time a chef needs a tomato, they have to go down to the ground floor, retrieve the tomatoes, and then return to their kitchen. This process is time-consuming, especially when many chefs are all trying to make their soups simultaneously.</p>

<div style="width: 90%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_7/tomato-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_7/tomato-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_7/tomato-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_7/tomato.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="tomato" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
   Threads in a Thread Block making Tomato Soup 
</div>
</div>

<p>To streamline the cooking process, each kitchen has its own smaller fridge (representing Shared Memory) that is accessible to all the chefs working within that kitchen (Thread Block). Instead of each chef making repeated trips to the ground floor, they can first send one chef to fetch a batch of tomatoes and store them in their shared fridge. Once the tomatoes are in the smaller fridge, all the chefs in that kitchen can easily access them without having to go back and forth to the ground floor.</p>

<p>This setup not only saves time but also enhances collaboration among the chefs in each kitchen. They can quickly share ingredients and coordinate their efforts to create the perfect soup. Just as the chefs benefit from having a shared fridge, threads within a Thread Block benefit from Shared Memory by reducing the time and effort needed to access frequently used data, ultimately leading to more efficient parallel computations in CUDA.</p>

<p>Enough talk, now its show time. It is code time.</p>

<h3 id="4-matrix-multiplication">4. Matrix Multiplication</h3>

<p>Now that we have understood the basics of CUDA, and discussed the concept of Shared Memory, let us stop speaking words, and start writing some code.
To that end, we will use Matrix Multiplication in order to understand how Shared Memory can be leveraged. This is how we will go about it.</p>

<ol>
  <li>Implement Matrix Multiplication on the CPU
    <ul>
      <li>Write a function to perform matrix multiplication using standard nested loops.</li>
      <li>Test the CPU implementation for correctness and performance.</li>
    </ul>
  </li>
  <li>Implement a Naive CUDA Kernel for Matrix Multiplication on the GPU
    <ul>
      <li>Write a basic CUDA kernel to perform matrix multiplication.</li>
      <li>Allocate memory for matrices on the GPU and transfer data from the host to the device.</li>
      <li>Launch the CUDA kernel and retrieve the result back to the host for verification.</li>
    </ul>
  </li>
  <li>Optimize Using Shared Memory
    <ul>
      <li>Modify the CUDA kernel to utilize shared memory.</li>
      <li>Test the optimized kernel for correctness and compare performance with the naive implementation.</li>
    </ul>
  </li>
</ol>

<h4 id="41-matmul-on-cpu--slow-and-steady">4.1 Matmul on CPU : Slow and Steady</h4>
<p>In this section we will discuss how Matmul can be performed on the CPU with 3 nested for loops.
This is the slowest way with zero parallelization, and will hence act as the benchmark. The code can be found <a href="https://github.com/sanket-pixel/CUDA/blob/main/shared_memory_blog/matmul/matmul.cu#L11">here</a>.</p>

<p>Let us take two 4x4 matrices A and B multiplied to give another 4x4 matrix C as shown in the figure below. Each element from the <code class="language-plaintext highlighter-rouge">ith row of A</code> is <strong>multiplied</strong> with each element of the <code class="language-plaintext highlighter-rouge">jth column of B</code>, and <strong>summed</strong> up to give the <code class="language-plaintext highlighter-rouge">element (i,j) of C</code>.</p>
<div style="width: 70%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_7/cpu_matmul-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_7/cpu_matmul-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_7/cpu_matmul-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_7/cpu_matmul.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="cpu" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
   For C[ i, j ], multiply all elements in ith row of A and jth column of B and then sum them up. Repeat this for each element of matrix C.
</div>
</div>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">matmul_cpu</span><span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;&amp;</span> <span class="n">A</span><span class="p">,</span> <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;&amp;</span> <span class="n">B</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;&amp;</span> <span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">sum</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">j</span><span class="p">];</span>
            <span class="p">}</span>
            <span class="n">C</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>
<p>When we use this function for multiplying two square matrices of size (1024x1024), and measure the latency :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Average CPU Time: 582.865ms
</code></pre></div></div>

<p>Thats more than half a second for a single matmul operation. We can definitely do better. 
Let us now look at a simple CUDA kernel and see how fast a naive implementation can get.</p>

<h4 id="42-matmul-on-gpu--the-cuda-way">4.2 Matmul on GPU : The CUDA way.</h4>

<p>The <code class="language-plaintext highlighter-rouge">matmul_cuda_naive</code> CUDA kernel performs matrix multiplication for two square matrices A and B, storing the result in matrix C. Each thread computes its unique row and column indices based on its block and thread identifiers, ensuring it operates within the bounds of the matrices. For each valid thread, the kernel initializes a sum variable and performs the dot product by iterating through the elements of the respective row of A and column of B. Finally, it stores the computed sum in the corresponding position of matrix C. The code can be found <a href="https://github.com/sanket-pixel/CUDA/blob/main/shared_memory_blog/matmul/matmul.cu#L24">here</a>.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">matmul_cuda_naive</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
        <span class="p">}</span>
        <span class="n">C</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>
<p>On using this kernel for matrix multiplication of the same (1024x1024) matrices, we obtain :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Average CUDA Naive Time: 2.42381ms
</code></pre></div></div>

<p>The CUDA matrix multiplocation is <code class="language-plaintext highlighter-rouge">240.19x</code> times faster than the CPU version. This is already pretty mind blowing, one might argue.
Lets put 240x times in perspective. If the matmul on GPU took a minute, the same operation on the CPU would take 4 hours to complete.</p>

<p>Not fast enough? Cool. Then lets make it even faster by using (no prize for guessing) <em>SHARED MEMORY</em>.</p>

<h3 id="5-matrix-multiplication-with-shared-memory">5. Matrix Multiplication with Shared Memory</h3>
<p>In this section, we will explore how Shared Memory can enhance the efficiency of Matrix Multiplication. The approach we will adopt may seem counterintuitive and could be challenging to grasp initially. Therefore, we will begin by examining the multiplication of smaller matrices, specifically the same 4x4 matrix example, using a block size of 2x2. This simplified example will provide a solid foundation, making it easier to comprehend the implementation for larger matrices later on.</p>

<h4 id="51-intuition-of-using-tiled-matmul">5.1 Intuition of using Tiled Matmul.</h4>
<p>As shown below, matrix multiplication can be done in parts, where sub-blocks <code class="language-plaintext highlighter-rouge">(i1, i2)</code> from matrix <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">(j1, j3)</code> from matrix <code class="language-plaintext highlighter-rouge">B</code> are multiplied to compute the partial result of element <code class="language-plaintext highlighter-rouge">C(i,j)</code>. The process is repeated for <code class="language-plaintext highlighter-rouge">(i3, i4)</code> and <code class="language-plaintext highlighter-rouge">(j2, j4)</code> to complete the sum for <code class="language-plaintext highlighter-rouge">C(i,j)</code>.
In summary, instead of computing all the multiplications and summing them at once, we split the work into two steps.</p>
<div style="width: 70%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_7/matmul_by_part-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_7/matmul_by_part-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_7/matmul_by_part-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_7/matmul_by_part.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="parts" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
   Matrix Multiplication split in parts.
</div>
</div>

<p>This intuition can be extended to help understand Tiled Matrix Multiplication. Instead of computing the matrix multiplication for the entire 4x4 matrix at once, we can break it down and compute the multiplication for each 2x2 block separately.</p>

<div style="width: 70%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_7/tiled_matmul-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_7/tiled_matmul-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_7/tiled_matmul-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_7/tiled_matmul.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="parts" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
   Tiled Matrix Multiplication.
</div>
</div>

<p>As shown above, when computing each <code class="language-plaintext highlighter-rouge">2x2 block</code> in matrix <code class="language-plaintext highlighter-rouge">C</code>, we can first calculate the matrix multiplication for the first 2x2 block in matrices A and B. After that, we move on to the next 2x2 block and compute its contribution. By summing these partial results, we gradually build the final result for the entire matrix.</p>

<p>This tiled approach allows us to handle smaller chunks of the matrices at a time, making it more efficient by utilizing faster memory (like shared memory) to perform intermediate computations.</p>

<h4 id="52-tiled-matmul-with-shared-memory-in-cuda">5.2 Tiled Matmul with Shared Memory in CUDA</h4>
<p>In this approach, we use shared memory to speed up the multiplication by working on small parts (tiles) of the matrices at a time. We already looked at how Tiled Matmul works for the 4x4 matrix as shown in the diagram above. Lets use the same example to understand how shared memory can be leveraged with Tiled Matrix Multiplication.</p>

<ul>
  <li><strong>Assigning Threads</strong>: For a matrix of size 4x4, we use <code class="language-plaintext highlighter-rouge">16 threads</code>, one for each element in the output <code class="language-plaintext highlighter-rouge">matrix C</code>. Each thread will be responsible for calculating one element of matrix C.</li>
</ul>
<div style="width: 40%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_7/thread-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_7/thread-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_7/thread-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_7/thread.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="parts" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
   Each element is assigned to one thread. 
</div>
</div>

<ul>
  <li>
    <p><strong>Breaking into Tiles</strong>: Instead of multiplying the entire matrix at once, we break it down into smaller 2x2 tiles. Each tile represents a section of the matrix, and this makes the problem easier to handle in smaller chunks.</p>
  </li>
  <li>
    <p><strong>Thread Block Assignment</strong>:  Each <code class="language-plaintext highlighter-rouge">2x2 tile</code> from the result <code class="language-plaintext highlighter-rouge">matrix C</code> is assigned to a thread block. The threads within that block are responsible for computing the values of this 2x2 section by multiplying corresponding 2x2 tiles from matrices <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code>.</p>
  </li>
</ul>
<div style="width: 40%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_7/block-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_7/block-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_7/block-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_7/block.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="parts" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
   Each 2x2 tile is assigned to one thread block.
</div>
</div>
<ul>
  <li><strong>Loading Data into Shared Memory</strong>: For each tile in matrix <code class="language-plaintext highlighter-rouge">C</code>, we first load the corresponding <code class="language-plaintext highlighter-rouge">2x2 tiles</code> from matrices <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code> into shared memory. <em>Shared memory</em> is fast and allows threads to access the required data quickly, avoiding the slower global memory.</li>
</ul>
<div style="width: 70%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_7/shared_copy-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_7/shared_copy-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_7/shared_copy-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_7/shared_copy.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="parts" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
   Copy relevant data to Shared Memory, and compute partial sum.
</div>
</div>

<ul>
  <li>
    <p><strong>Synchronizing Threads</strong>: Once all the threads have loaded their part of <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code> into shared memory, we <em>synchronize</em> the threads so that everyone is ready to use the data at the same time.</p>
  </li>
  <li>
    <p><strong>Performing the Partial Multiplication</strong>: : The threads in the block perform the matrix multiplication for this tile, using the data stored in shared memory. Each thread computes a partial result by multiplying corresponding elements from the current tile of A and B (just like we mentioned earlier: i1 * j1 + i2 * j2).</p>
  </li>
  <li>
    <p><strong>Repeat for Other Threads</strong>: After computing the first set of <code class="language-plaintext highlighter-rouge">2x2 tiles</code>, we move on to the next tiles from <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code>, repeating the process until all partial results for the tile in <code class="language-plaintext highlighter-rouge">C</code> are summed up.</p>
  </li>
  <li>
    <p><strong>Accumulating Results</strong>: The thread adds these partial results together. After processing all tiles, the full result for each element of <code class="language-plaintext highlighter-rouge">C</code> is accumulated.</p>
  </li>
  <li>
    <p><strong>Writing Back to Global Memory</strong>: Once all threads are done with their computations, the final values for matrix C are written back to global memory.</p>
  </li>
</ul>

<p>In summary, when performing matrix multiplication using shared memory in CUDA, we first copy the necessary data for each thread block’s 2x2 tile into shared memory and then write the results back to global memory after the computation is complete.</p>

<h4 id="53-reusing-data-from-shared-memory">5.3 Reusing Data from Shared Memory</h4>
<p>A key advantage of this approach is the efficient reuse of data. For every 2x2 tile in the result matrix C, each element from the corresponding tiles in A and B is reused twice. This reuse reduces the need for multiple accesses to global memory, as the values are already stored in shared memory after the initial copy. By reducing redundant memory accesses, we significantly improve the performance of the matrix multiplication process. The diagram you provided illustrates how the values in the blue and yellow grids are reused during each step of the multiplication.</p>

<div style="width: 60%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_7/count-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_7/count-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_7/count-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_7/count.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="parts" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
   Counting the number of times the values in A and B are used for a 2x2 matrix multiplication.
</div>
</div>

<h4 id="54-talk-is-cheap-show-me-the-code">5.4 Talk is Cheap. Show me the Code.</h4>
<p>We’ve discussed a toy example of multiplying two 4x4 matrices using 2x2 tile using shared memory to improve performance. In this section, we’ll look at the code, and extend the same concept, where the grid is divided into 32x32 blocks, each handled by a block of threads, and the computation for each element in the output matrix is handled by the respective threads in the block. The code can be found <a href="https://github.com/sanket-pixel/CUDA/blob/main/shared_memory_blog/matmul/matmul.cu#L38">here</a>.</p>

<p>Let’s dive into the code, which is essentially the same but operates on larger tiles (32x32).</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#define BLOCK_SIZE 32
</span><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">matmul_cuda_shared</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// define A and B sub matrices of size 32x32 in shared memory.</span>
    <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">Asub</span><span class="p">[</span><span class="n">BLOCK_SIZE</span><span class="p">][</span><span class="n">BLOCK_SIZE</span><span class="p">];</span>
    <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">Bsub</span><span class="p">[</span><span class="n">BLOCK_SIZE</span><span class="p">][</span><span class="n">BLOCK_SIZE</span><span class="p">];</span>

    <span class="c1">// get global x and y indices for this thread and block.</span>
    <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="c1">// variable to store sum ( stored on registers )</span>
    <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span>

    <span class="c1">// iterate over all tiles for this thread block.</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">blockIdx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">blockIdx</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="n">BLOCK_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">BLOCK_SIZE</span><span class="p">;</span> <span class="n">blockIdx</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// Load tiles into shared memory</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">&amp;&amp;</span> <span class="n">blockIdx</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span>
            <span class="n">Asub</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">blockIdx</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
        <span class="k">else</span>
            <span class="n">Asub</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">col</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">&amp;&amp;</span> <span class="n">blockIdx</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span>
            <span class="n">Bsub</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[(</span><span class="n">blockIdx</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
        <span class="k">else</span>
            <span class="n">Bsub</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
        
        <span class="c1">// Synchronize to make sure all the data is copied to shared memory.</span>
        <span class="n">__syncthreads</span><span class="p">();</span>

        <span class="c1">// Compute matrix multiplication for the tile</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">BLOCK_SIZE</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">Asub</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">Bsub</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
        <span class="p">}</span>
        <span class="n">__syncthreads</span><span class="p">();</span>
    <span class="p">}</span>
    <span class="c1">// Copy the data to global memory.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">C</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>

</code></pre></div></div>

<p>Now, let’s break this down step by step.</p>

<ol>
  <li>
    <p><strong>Block Size and Shared Memory</strong>: The <code class="language-plaintext highlighter-rouge">BLOCK_SIZE</code> is set to <code class="language-plaintext highlighter-rouge">32</code>, meaning each thread block handles a <code class="language-plaintext highlighter-rouge">32x32 tile</code>. Shared memory is allocated for storing sub-matrices (tiles) from matrix <code class="language-plaintext highlighter-rouge">A</code> and matrix <code class="language-plaintext highlighter-rouge">B</code> in Asub and Bsub. The <code class="language-plaintext highlighter-rouge">__shared__</code> keyword is key here—it allocates memory for all threads within a block to share during the computation, avoiding slower global memory accesses.</p>

    <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c1">// define A and B sub matrices of size 32x32 in shared memory.</span>
 <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">Asub</span><span class="p">[</span><span class="n">BLOCK_SIZE</span><span class="p">][</span><span class="n">BLOCK_SIZE</span><span class="p">];</span>
 <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">Bsub</span><span class="p">[</span><span class="n">BLOCK_SIZE</span><span class="p">][</span><span class="n">BLOCK_SIZE</span><span class="p">];</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Mapping Threads to Tiles</strong>: Each thread in a block is responsible for calculating <code class="language-plaintext highlighter-rouge">one element</code> in a <code class="language-plaintext highlighter-rouge">32x32</code> tile of the result matrix <code class="language-plaintext highlighter-rouge">C</code>. The thread’s position in the block corresponds to a specific <code class="language-plaintext highlighter-rouge">row</code> and <code class="language-plaintext highlighter-rouge">column</code> within the tile.
    <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c1">// get global x and y indices for this thread and block.</span>
 <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
 <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Loading Tiles into Shared Memory</strong>: In the loop, tiles from matrices <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code> are loaded into shared memory. Each thread copies one element from the global memory (the larger matrices <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code>) into the shared memory tiles (<code class="language-plaintext highlighter-rouge">Asub</code> and <code class="language-plaintext highlighter-rouge">Bsub</code>). This process is repeated for multiple tiles as we move across the matrices.</p>

    <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c1">// Load tiles into shared memory</span>
 <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">&amp;&amp;</span> <span class="n">blockIdx</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span>
     <span class="n">Asub</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">blockIdx</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
 <span class="k">else</span>
     <span class="n">Asub</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>

 <span class="k">if</span> <span class="p">(</span><span class="n">col</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">&amp;&amp;</span> <span class="n">blockIdx</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span>
     <span class="n">Bsub</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[(</span><span class="n">blockIdx</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
 <span class="k">else</span>
     <span class="n">Bsub</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
    
</code></pre></div>    </div>
  </li>
  <li><strong>Synchronization</strong>: After copying data into shared memory, <code class="language-plaintext highlighter-rouge">__syncthreads()</code> is called. This ensures that all threads in the block have completed copying their respective elements before moving on to the computation. Synchronization is crucial here because shared memory is used by all threads in the block, and we don’t want any thread to start using data that hasn’t been fully copied.
    <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c1">// Synchronize to make sure all the data is copied to shared memory.</span>
 <span class="n">__syncthreads</span><span class="p">();</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Computing the Tile</strong>: After loading a tile into shared memory, each thread computes part of the matrix multiplication for that tile. For a given row in <code class="language-plaintext highlighter-rouge">A</code> and column in <code class="language-plaintext highlighter-rouge">B</code>, the corresponding thread will compute the dot product, accumulating the result in <code class="language-plaintext highlighter-rouge">sum</code>.
    <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c1">// Compute matrix multiplication for the tile</span>
 <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">BLOCK_SIZE</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
     <span class="n">sum</span> <span class="o">+=</span> <span class="n">Asub</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">Bsub</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
 <span class="p">}</span>
 <span class="n">__syncthreads</span><span class="p">();</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Writing Back to Global Memory</strong>: Once the partial product for the tile is calculated, the results are written back to global memory in the matrix <code class="language-plaintext highlighter-rouge">C</code>. This is done after all tiles have been processed and summed.
    <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1">// Copy the data to global memory.</span>
 <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
     <span class="n">C</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
 <span class="p">}</span>
</code></pre></div>    </div>
  </li>
</ol>

<p>This code shows how to extend the concept of tiled matrix multiplication to larger blocks (32x32) using shared memory. The basic steps involve:</p>

<ul>
  <li>Copying the required sub-matrices (tiles) from global memory into shared memory for fast access.</li>
  <li>Performing matrix multiplication within each block using the shared tiles.</li>
  <li>Storing the final results back into global memory.</li>
</ul>

<p>By making effective use of shared memory, we minimize the number of slow global memory accesses, which results in significantly faster matrix multiplication.</p>

<p>In essence, the shared memory acts as a cache for data reuse within the block. For example, each value in a tile of A and B is reused 32 times during the matrix multiplication. Without shared memory, the same value would need to be loaded from global memory repeatedly, slowing down the computation.</p>

<p>Let us now compare the time improvement due to Shared Memory.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Average CPU Time: 568.871ms
Average CUDA Naive Time: 2.42451ms
Average CUDA Shared Memory Time: 1.91292ms
</code></pre></div></div>

<p>By using shared memory, this time is further reduced to <code class="language-plaintext highlighter-rouge">1.91292 ms</code>, providing about a <code class="language-plaintext highlighter-rouge">1.27x</code> speedup compared to the naive CUDA approach. This speedup comes from the efficient reuse of data within shared memory, which avoids multiple global memory accesses and significantly enhances computation speed, showcasing the advantage of shared memory in optimizing matrix multiplication.</p>

<h3 id="6-summary">6. Summary</h3>
<p>In this blog, we explored how shared memory can significantly boost the performance of CUDA programs by reducing the need for repetitive data transfers from global memory. We started by understanding the basics of thread and block organization in CUDA, and then introduced the concept of shared memory as a customizable and high-speed alternative to the default memory hierarchy. By storing frequently accessed data in shared memory, we enable threads within a block to reuse it efficiently, avoiding costly memory fetches.</p>

<p>We looked at an example of matrix multiplication using the Tiled Matmul technique, where we saw how shared memory allows us to load, compute, and reuse data efficiently. With shared memory, each thread in a block can access the data needed for computations directly from a faster, local cache rather than repeatedly querying global memory.</p>

<p>Ultimately, shared memory gives programmers more control over what data gets cached, leading to a dramatic reduction in memory latency and an overall performance boost, as seen in the 2x speedup in our example. With these concepts in mind, you’re now equipped to start leveraging shared memory in your own CUDA programs for optimized performance!</p>]]></content><author><name></name></author><category term="cuda" /><category term="nvidia" /><category term="cuda" /><summary type="html"><![CDATA[How to exactly quantize models and still not lose accuracy.]]></summary></entry><entry><title type="html">A practical guide to Quantization</title><link href="https://sanket-pixel.github.io//blog/2024/a-practical-guide-to-quantization/" rel="alternate" type="text/html" title="A practical guide to Quantization" /><published>2024-07-21T14:53:00+00:00</published><updated>2024-07-21T14:53:00+00:00</updated><id>https://sanket-pixel.github.io//blog/2024/a-practical-guide-to-quantization</id><content type="html" xml:base="https://sanket-pixel.github.io//blog/2024/a-practical-guide-to-quantization/"><![CDATA[<h4 id="in-this-blog-we-delve-into-the-practical-side-of-model-optimization-focusing-on-how-to-leverage-tensorrt-for-int8-quantization-to-drastically-improve-inference-speed-by-walking-through-the-process-step-by-step-we-compare-pure-pytorch-inference-tensorrt-optimization-and-finally-int8-quantization-with-calibration-the-results-highlight-the-incredible-potential-of-these-techniques-with-an-over-10x-speedup-in-performance-whether-youre-aiming-to-deploy-deep-learning-models-in-production-or-simply-seeking-to-enhance-your-understanding-of-model-optimization-this-blog-provides-valuable-insights-into-achieving-faster-and-more-efficient-inference">In this blog, we delve into the practical side of model optimization, focusing on how to leverage TensorRT for INT8 quantization to drastically improve inference speed. By walking through the process step-by-step, we compare pure PyTorch inference, TensorRT optimization, and finally, INT8 quantization with calibration. The results highlight the incredible potential of these techniques, with an over <code class="language-plaintext highlighter-rouge">10x speedup</code> in performance. Whether you’re aiming to deploy deep learning models in production or simply seeking to enhance your understanding of model optimization, this blog provides valuable insights into achieving faster and more efficient inference.</h4>

<p><br /></p>
<div style="width: 100%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_6/compare-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_6/compare-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_6/compare-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_6/compare.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
 A practical guide to Quantization
</div>
</div>

<p>In our previous blog post,  <a href="/blog/2024/quantization-explained-like-youre-five/">Quantization explained, like you are five</a>, we covered the theory and intuition behind quantization in detail. If you haven’t read it yet, we highly recommend doing so to gain a solid understanding of the fundamental concepts. This current post serves as a sequel, diving into the practical aspects of quantization. We will guide you through the process of applying quantization to a neural network, demonstrating how it can significantly speed up inference.</p>

<p>In this blog, we would using the resnet based image classfication model as an example. We would be using TensorRT as well for quantization. This constrains this code only for NVIDIA GPU devices. But these same concepts can be extended to other devices and tools without loss of generality. 
If you need a primer on how resnet based image classifciation can be deployed using TensorRT on nvidia devices, refer to previous blog <a href="/blog/2023/introduction-to-tensorrt/">Have you met TensorRT?</a> where we saw how to use FP16 based TensorRT engine for inference. In this blog we will take it a step further by quantizing the same model to INT8.</p>

<p>We will go about this using 3 major steps :</p>

<p><strong>1. Inference with Pure PyTorch</strong></p>
<ul>
  <li>Establish a baseline for latency and accuracy using the original PyTorch model.</li>
  <li>Measure inference time and accuracy on a sample dataset.</li>
</ul>

<p><strong>2. Inference with TensorRT Engine</strong></p>
<ul>
  <li>Convert the PyTorch model to a TensorRT engine using FP16 precision.</li>
  <li>Measure and compare the latency and accuracy to the PyTorch baseline.</li>
</ul>

<p><strong>3. Inference with TensorRT Engine (INT8, With Calibration)</strong></p>
<ul>
  <li>Perform INT8 quantization with calibration using a calibration dataset.</li>
  <li>Measure latency and accuracy, highlighting the trade-offs and benefits of quantization.</li>
</ul>

<h4 id="1-inference-with-pure-pytorch">1. Inference with Pure Pytorch</h4>

<p>To establish a baseline for both accuracy and latency, we will first run inference using the ResNet-50 model in PyTorch. This section will cover the preprocessing, inference, and postprocessing steps. The results here will serve as a comparison for the performance improvements we achieve using TensorRT and INT8 quantization in later steps.</p>

<h5 id="a-preprocessing">a. Preprocessing</h5>

<p>The preprocessing step involves preparing the input image so that it is suitable for the ResNet-50 model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">cv2</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torchvision.transforms</span> <span class="kn">import</span> <span class="n">Resize</span><span class="p">,</span> <span class="n">Compose</span><span class="p">,</span> <span class="n">ToTensor</span><span class="p">,</span> <span class="n">Normalize</span>

<span class="k">def</span> <span class="nf">preprocess_image</span><span class="p">(</span><span class="n">img_path</span><span class="p">):</span>
    <span class="c1"># Transformations for the input data
</span>    <span class="n">transforms</span> <span class="o">=</span> <span class="nc">Compose</span><span class="p">([</span>
        <span class="nc">ToTensor</span><span class="p">(),</span>
        <span class="nc">Resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)),</span>
        <span class="nc">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]),</span>
    <span class="p">])</span>
    <span class="c1"># Read input image
</span>    <span class="n">input_img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">imread</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
    <span class="c1"># Apply transformations
</span>    <span class="n">input_data</span> <span class="o">=</span> <span class="nf">transforms</span><span class="p">(</span><span class="n">input_img</span><span class="p">)</span>
    <span class="n">batch_data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch_data</span>
</code></pre></div></div>

<h5 id="b-postprocessing">b. Postprocessing</h5>
<p>After inference, the output from the model needs to be processed to extract meaningful predictions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">postprocess</span><span class="p">(</span><span class="n">output_data</span><span class="p">):</span>
    <span class="c1"># Get class names
</span>    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">data/imagenet-classes.txt</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">.</span><span class="nf">readlines</span><span class="p">()]</span>
    <span class="c1"># Calculate human-readable values by softmax
</span>    <span class="n">confidences</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">output_data</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="c1"># Find top predicted classes
</span>    <span class="n">_</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">output_data</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Print the top classes predicted by the model
</span>    <span class="k">while</span> <span class="n">confidences</span><span class="p">[</span><span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]]</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>
        <span class="n">class_idx</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
        <span class="nf">print</span><span class="p">(</span>
            <span class="sh">"</span><span class="s">class:</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">classes</span><span class="p">[</span><span class="n">class_idx</span><span class="p">],</span>
            <span class="sh">"</span><span class="s">, confidence:</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">confidences</span><span class="p">[</span><span class="n">class_idx</span><span class="p">].</span><span class="nf">item</span><span class="p">(),</span>
            <span class="sh">"</span><span class="s">%, index:</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">class_idx</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="c1"># Postprocess the output
</span><span class="nf">postprocess</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></div>
<h5 id="c-inference">c. Inference</h5>

<p>This step involves running the preprocessed image through the ResNet-50 model to obtain predictions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">import</span> <span class="n">torch.backends.cudnn</span> <span class="k">as</span> <span class="n">cudnn</span>
<span class="kn">import</span> <span class="n">time</span>
<span class="c1"># Load and prepare the model
</span><span class="n">cudnn</span><span class="p">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="c1"># Preprocess the input image
</span><span class="nb">input</span> <span class="o">=</span> <span class="nf">preprocess_image</span><span class="p">(</span><span class="sh">"</span><span class="s">data/hotdog.jpg</span><span class="sh">"</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
<span class="c1"># Perform inference
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="c1"># Warm-up the GPU
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="c1"># Measure latency
</span><span class="n">latencies</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>  <span class="c1"># Ensure that all CUDA operations are finished
</span>        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
        
        <span class="n">latency</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
        <span class="n">latencies</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">latency</span><span class="p">)</span>
<span class="nf">postprocess</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="n">average_latency</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Average Latency: </span><span class="si">{</span><span class="n">average_latency</span> <span class="o">*</span> <span class="mi">1000</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> ms</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h5 id="d-performance">d. Performance</h5>
<p>The output from pure Pytorch inference would look something like this.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class: hotdog, hot dog, red hot , confidence: 85.24353790283203 %, index: 934
Average Latency: 3.94 ms
</code></pre></div></div>

<h4 id="2-tensorrt-inference">2. TensorRT inference</h4>

<p>In this section, we demonstrate how to convert a trained PyTorch model to an ONNX format, then build a TensorRT engine, and finally perform inference using the TensorRT engine. This process significantly optimizes the inference performance on NVIDIA GPUs.</p>

<p>For more detailed explanations on converting PyTorch models to TensorRT engines, refer to our previous blog post, <a href="/blog/2023/introduction-to-tensorrt/">Have you met TensorRT?</a>.</p>

<h5 id="a-convert-pytorch-model-to-onnx">a. Convert PyTorch Model to ONNX</h5>

<p>First, we export the PyTorch model to the ONNX format. This intermediate representation serves as a bridge between different deep learning frameworks and tools like TensorRT.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ONNX_FILE_PATH</span> <span class="o">=</span> <span class="sh">'</span><span class="s">deploy_tools/resnet50.onnx</span><span class="sh">'</span>
<span class="n">torch</span><span class="p">.</span><span class="n">onnx</span><span class="p">.</span><span class="nf">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">ONNX_FILE_PATH</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">input</span><span class="sh">'</span><span class="p">],</span>
                  <span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">output</span><span class="sh">'</span><span class="p">],</span> <span class="n">export_params</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>
<h5 id="b-build-tensorrt-engine">b. Build TensorRT Engine</h5>

<p>Next, we parse the ONNX model and build the TensorRT engine, which is optimized for the target NVIDIA GPU.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pycuda.driver</span> <span class="k">as</span> <span class="n">cuda</span>
<span class="kn">import</span> <span class="n">pycuda.autoinit</span>
<span class="kn">import</span> <span class="n">tensorrt</span> <span class="k">as</span> <span class="n">trt</span>

<span class="n">TRT_LOGGER</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Logger</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">Logger</span><span class="p">.</span><span class="n">ERROR</span><span class="p">)</span>
<span class="n">builder</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Builder</span><span class="p">(</span><span class="n">TRT_LOGGER</span><span class="p">)</span>

<span class="n">EXPLICIT_BATCH</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="nf">int</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">NetworkDefinitionCreationFlag</span><span class="p">.</span><span class="n">EXPLICIT_BATCH</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">create_network</span><span class="p">(</span><span class="n">EXPLICIT_BATCH</span><span class="p">)</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">OnnxParser</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">TRT_LOGGER</span><span class="p">)</span>
<span class="n">success</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="nf">parse_from_file</span><span class="p">(</span><span class="n">ONNX_FILE_PATH</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">parser</span><span class="p">.</span><span class="n">num_errors</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">parser</span><span class="p">.</span><span class="nf">get_error</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">create_builder_config</span><span class="p">()</span>
<span class="n">config</span><span class="p">.</span><span class="nf">set_memory_pool_limit</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">MemoryPoolType</span><span class="p">.</span><span class="n">WORKSPACE</span><span class="p">,</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">30</span><span class="p">)</span>  <span class="c1"># 1 MiB
</span><span class="n">serialized_engine</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">build_serialized_network</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">deploy_tools/resnet50.engine</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">wb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">serialized_engine</span><span class="p">)</span>
</code></pre></div></div>
<p>This code snippet handles the following:</p>

<ul>
  <li><strong>Logger</strong>: Initializes the TensorRT logger to monitor errors during the engine creation.</li>
  <li><strong>Builder and Network</strong>: Creates the TensorRT builder and network definition.</li>
  <li><strong>ONNX Parser</strong>: Parses the ONNX model into the TensorRT network.</li>
  <li><strong>Engine Serialization</strong>: Serializes the TensorRT engine and saves it to a file for later use.</li>
</ul>

<h5 id="c-inference-with-tensorrt-engine">c. Inference with TensorRT Engine</h5>

<p>After building the TensorRT engine, we can load it and run inference on the input data. The process involves transferring data to the GPU, executing the engine, and retrieving the results.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">runtime</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Runtime</span><span class="p">(</span><span class="n">TRT_LOGGER</span><span class="p">)</span>

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">deploy_tools/resnet50.engine</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">serialized_engine</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="n">engine</span> <span class="o">=</span> <span class="n">runtime</span><span class="p">.</span><span class="nf">deserialize_cuda_engine</span><span class="p">(</span><span class="n">serialized_engine</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">engine</span><span class="p">.</span><span class="nf">create_execution_context</span><span class="p">()</span>

<span class="c1"># Determine dimensions and create memory buffers for inputs/outputs
</span><span class="n">h_input</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">pagelocked_empty</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="nf">volume</span><span class="p">(</span><span class="n">engine</span><span class="p">.</span><span class="nf">get_binding_shape</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">h_output</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">pagelocked_empty</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="nf">volume</span><span class="p">(</span><span class="n">engine</span><span class="p">.</span><span class="nf">get_binding_shape</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">d_input</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">mem_alloc</span><span class="p">(</span><span class="n">h_input</span><span class="p">.</span><span class="n">nbytes</span><span class="p">)</span>
<span class="n">d_output</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">mem_alloc</span><span class="p">(</span><span class="n">h_output</span><span class="p">.</span><span class="n">nbytes</span><span class="p">)</span>
<span class="n">stream</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nc">Stream</span><span class="p">()</span>

<span class="c1"># Preprocess input image
</span><span class="n">host_input</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">preprocess_image</span><span class="p">(</span><span class="sh">"</span><span class="s">data/hotdog.jpg</span><span class="sh">"</span><span class="p">).</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Warm-up the GPU
</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_htod_async</span><span class="p">(</span><span class="n">d_input</span><span class="p">,</span> <span class="n">host_input</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">context</span><span class="p">.</span><span class="nf">execute_async_v2</span><span class="p">(</span><span class="n">bindings</span><span class="o">=</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">d_input</span><span class="p">),</span> <span class="nf">int</span><span class="p">(</span><span class="n">d_output</span><span class="p">)],</span> <span class="n">stream_handle</span><span class="o">=</span><span class="n">stream</span><span class="p">.</span><span class="n">handle</span><span class="p">)</span>
    <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_dtoh_async</span><span class="p">(</span><span class="n">h_output</span><span class="p">,</span> <span class="n">d_output</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">stream</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>

<span class="c1"># Measure latency
</span><span class="n">latencies</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_htod_async</span><span class="p">(</span><span class="n">d_input</span><span class="p">,</span> <span class="n">host_input</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">context</span><span class="p">.</span><span class="nf">execute_async_v2</span><span class="p">(</span><span class="n">bindings</span><span class="o">=</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">d_input</span><span class="p">),</span> <span class="nf">int</span><span class="p">(</span><span class="n">d_output</span><span class="p">)],</span> <span class="n">stream_handle</span><span class="o">=</span><span class="n">stream</span><span class="p">.</span><span class="n">handle</span><span class="p">)</span>
    <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_dtoh_async</span><span class="p">(</span><span class="n">h_output</span><span class="p">,</span> <span class="n">d_output</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">stream</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">latency</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="n">latencies</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">latency</span><span class="p">)</span>

<span class="n">average_latency</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Average Latency: </span><span class="si">{</span><span class="n">average_latency</span> <span class="o">*</span> <span class="mi">1000</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> ms</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Postprocess and display results
</span><span class="n">tensorrt_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">h_output</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">postprocess</span><span class="p">(</span><span class="n">tensorrt_output</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">empty_cache</span><span class="p">()</span>
</code></pre></div></div>

<p>In this step:</p>

<ul>
  <li><strong>Runtime and Engine</strong>: We deserialize the saved TensorRT engine and create an execution context.</li>
  <li><strong>Memory Allocation</strong>: Allocate memory for inputs and outputs on both host (CPU) and device (GPU).</li>
  <li><strong>Inference</strong>: Perform inference using the TensorRT engine, measuring the latency over multiple iterations.</li>
  <li><strong>Postprocessing</strong>: Convert the output back to a PyTorch tensor for easier postprocessing and interpretation of the results.</li>
</ul>

<h5 id="d-performance-1">d. Performance</h5>
<p>The output from TensorRT inference is as follows ( may vary on different GPUs)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Average Latency: 2.73 ms
class: hotdog, hot dog, red hot , confidence: 85.29612731933594 %, index: 934
</code></pre></div></div>
<p>With just basic TensorRT engine conversion, we achieved a <code class="language-plaintext highlighter-rouge">1.44x</code> speedup. Now lets convert it to a quantized INT8 TensorRT engine and see how much speedup we can achieve.</p>

<h4 id="3-inference-with-int8-quantization">3. Inference with INT8 Quantization</h4>

<p><br /></p>
<div style="width: 100%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_6/quantization-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_6/quantization-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_6/quantization-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_6/quantization.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Steps for Quantization
</div>
</div>

<p>In this section, we’ll dive into the heart of the optimization process: calibration and quantization. This process is essential for converting a model to INT8 precision, which can significantly boost inference speed while maintaining a good level of accuracy.</p>

<h5 id="a-implementing-a-custom-calibration-class">a. Implementing a Custom Calibration Class</h5>

<p>Before we can convert our model to INT8, we need to calibrate it. Calibration is the process where we run a representative dataset through the model to collect statistical information about its activations. This data is crucial for accurately mapping the floating-point values to INT8 values. To do this in TensorRT, we implement a custom calibration class that extends from the base trt.IInt8EntropyCalibrator2 class.</p>

<p>Here’s how we set up the calibration process:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ImageCalibrator</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">IInt8EntropyCalibrator2</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">image_dir_path</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ImageCalibrator</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">current_index</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">image_paths</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">load_images_paths</span><span class="p">(</span><span class="n">image_dir_path</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_samples</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">image_paths</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">device_image</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">mem_alloc</span><span class="p">(</span><span class="mi">1</span><span class="o">*</span><span class="mi">3</span><span class="o">*</span><span class="mi">224</span><span class="o">*</span><span class="mi">224</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">load_images_paths</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">image_dir_path</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="n">image_dir_path</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_batch_size</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">batch_size</span>

    <span class="k">def</span> <span class="nf">get_batch</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">names</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">current_index</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">num_samples</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">None</span>
        <span class="n">image_path</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">image_paths</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">current_index</span><span class="p">]</span>
        <span class="n">image_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">data/calibration_dataset/</span><span class="sh">"</span> <span class="o">+</span> <span class="n">image_path</span>
        <span class="n">image</span> <span class="o">=</span> <span class="nf">preprocess_image</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>
        <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_htod</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device_image</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">current_index</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="n">batch_size</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">device_image</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">read_calibration_cache</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">calibration.cache</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">rb</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>
        <span class="k">except</span> <span class="nb">FileNotFoundError</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">write_calibration_cache</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
        <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">calibration.cache</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">wb</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">cache</span><span class="p">)</span>
</code></pre></div></div>

<p>What’s Happening Here?</p>

<p><strong>Initialization</strong>: The class is initialized with the directory path containing calibration images. These images are loaded into memory as the calibration process iterates through them.</p>

<p><strong>Image Preprocessing</strong>: Each image from the calibration dataset is preprocessed and then copied to the GPU’s memory for use in calibration. The batch size is set to 1 for simplicity, but this can be adjusted based on your needs.</p>

<p><strong>Handling the Calibration Cache</strong>: TensorRT can cache the calibration data, so you don’t have to recalibrate the model every time you run this process. The cache is read from and written to the disk, allowing for faster reuse of calibration data in future runs.</p>

<p>When calibrating other models, you’ll need to implement a similar class tailored to the specific preprocessing and data handling requirements of your model.</p>

<h5 id="b-building-the-tensorrt-engine-with-int8-quantization">b. Building the TensorRT Engine with INT8 Quantization</h5>

<p>Once we’ve set up our calibrator, we’re ready to build the TensorRT engine with INT8 precision. The steps involve parsing the model from an ONNX file and configuring the builder to use the INT8 calibration data.</p>

<p>Here’s how it’s done:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ONNX_FILE_PATH</span> <span class="o">=</span> <span class="sh">"</span><span class="s">deploy_tools/resnet50.onnx</span><span class="sh">"</span>
<span class="n">image_dir</span> <span class="o">=</span> <span class="sh">"</span><span class="s">data/calibration_dataset</span><span class="sh">"</span>
<span class="n">TRT_LOGGER</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Logger</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">Logger</span><span class="p">.</span><span class="n">ERROR</span><span class="p">)</span>
<span class="n">builder</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Builder</span><span class="p">(</span><span class="n">TRT_LOGGER</span><span class="p">)</span>
<span class="n">EXPLICIT_BATCH</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="nf">int</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">NetworkDefinitionCreationFlag</span><span class="p">.</span><span class="n">EXPLICIT_BATCH</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">create_network</span><span class="p">(</span><span class="n">EXPLICIT_BATCH</span><span class="p">)</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">OnnxParser</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">TRT_LOGGER</span><span class="p">)</span>
<span class="n">success</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="nf">parse_from_file</span><span class="p">(</span><span class="n">ONNX_FILE_PATH</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">parser</span><span class="p">.</span><span class="n">num_errors</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">parser</span><span class="p">.</span><span class="nf">get_error</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">create_builder_config</span><span class="p">()</span>
<span class="n">config</span><span class="p">.</span><span class="nf">set_memory_pool_limit</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">MemoryPoolType</span><span class="p">.</span><span class="n">WORKSPACE</span><span class="p">,</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">30</span><span class="p">)</span>  <span class="c1"># 1 GiB
</span>
<span class="c1"># Assign the custom calibrator
</span><span class="n">config</span><span class="p">.</span><span class="n">int8_calibrator</span> <span class="o">=</span> <span class="nc">ImageCalibrator</span><span class="p">(</span><span class="n">image_dir</span><span class="p">)</span>
<span class="n">config</span><span class="p">.</span><span class="nf">set_flag</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">BuilderFlag</span><span class="p">.</span><span class="n">INT8</span><span class="p">)</span>

<span class="c1"># Build and serialize the INT8 TensorRT engine
</span><span class="n">serialized_engine</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">build_serialized_network</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">deploy_tools/resnet_int8.engine</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">wb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">serialized_engine</span><span class="p">)</span>
</code></pre></div></div>

<p>Key Points:</p>

<ul>
  <li>
    <p><strong>ONNX Parser</strong>: The model is parsed from the ONNX format, and the network is created using TensorRT.</p>
  </li>
  <li>
    <p><strong>INT8 Calibration</strong>: The custom calibrator class we defined earlier is assigned to the builder configuration. The INT8 flag is set, instructing TensorRT to use the calibration data for quantizing the model.</p>
  </li>
  <li>
    <p><strong>Engine Serialization</strong>: Once the engine is built with INT8 precision, it is serialized and saved to disk. This engine is now optimized for fast inference using 8-bit integer operations.</p>
  </li>
</ul>

<p>For more details on the general process of building TensorRT engines, refer to our previous post, “Have you met TensorRT?”.</p>

<h5 id="c-running-inference-with-the-int8-optimized-engine">c. Running Inference with the INT8-Optimized Engine</h5>

<p>Finally, we can run inference using the INT8-optimized TensorRT engine. Here’s how it’s done:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">runtime</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Runtime</span><span class="p">(</span><span class="n">TRT_LOGGER</span><span class="p">)</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">deploy_tools/resnet_int8.engine</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">serialized_engine</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">runtime</span><span class="p">.</span><span class="nf">deserialize_cuda_engine</span><span class="p">(</span><span class="n">serialized_engine</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">engine</span><span class="p">.</span><span class="nf">create_execution_context</span><span class="p">()</span>

<span class="c1"># Allocate memory for inputs and outputs
</span><span class="n">h_input</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">pagelocked_empty</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="nf">volume</span><span class="p">(</span><span class="n">engine</span><span class="p">.</span><span class="nf">get_binding_shape</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">h_output</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">pagelocked_empty</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="nf">volume</span><span class="p">(</span><span class="n">engine</span><span class="p">.</span><span class="nf">get_binding_shape</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">d_input</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">mem_alloc</span><span class="p">(</span><span class="n">h_input</span><span class="p">.</span><span class="n">nbytes</span><span class="p">)</span>
<span class="n">d_output</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">mem_alloc</span><span class="p">(</span><span class="n">h_output</span><span class="p">.</span><span class="n">nbytes</span><span class="p">)</span>
<span class="n">stream</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nc">Stream</span><span class="p">()</span>

<span class="c1"># Preprocess and load the input image
</span><span class="n">host_input</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">preprocess_image</span><span class="p">(</span><span class="sh">"</span><span class="s">data/hotdog.jpg</span><span class="sh">"</span><span class="p">).</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Warm-up the GPU
</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_htod_async</span><span class="p">(</span><span class="n">d_input</span><span class="p">,</span> <span class="n">host_input</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">context</span><span class="p">.</span><span class="nf">execute_async_v2</span><span class="p">(</span><span class="n">bindings</span><span class="o">=</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">d_input</span><span class="p">),</span> <span class="nf">int</span><span class="p">(</span><span class="n">d_output</span><span class="p">)],</span> <span class="n">stream_handle</span><span class="o">=</span><span class="n">stream</span><span class="p">.</span><span class="n">handle</span><span class="p">)</span>
    <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_dtoh_async</span><span class="p">(</span><span class="n">h_output</span><span class="p">,</span> <span class="n">d_output</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">stream</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>

<span class="c1"># Measure latency for INT8 inference
</span><span class="n">iterations</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">latencies</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_htod_async</span><span class="p">(</span><span class="n">d_input</span><span class="p">,</span> <span class="n">host_input</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">context</span><span class="p">.</span><span class="nf">execute_async_v2</span><span class="p">(</span><span class="n">bindings</span><span class="o">=</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">d_input</span><span class="p">),</span> <span class="nf">int</span><span class="p">(</span><span class="n">d_output</span><span class="p">)],</span> <span class="n">stream_handle</span><span class="o">=</span><span class="n">stream</span><span class="p">.</span><span class="n">handle</span><span class="p">)</span>
    <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_dtoh_async</span><span class="p">(</span><span class="n">h_output</span><span class="p">,</span> <span class="n">d_output</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">stream</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">latencies</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>

<span class="c1"># Calculate average latency
</span><span class="n">average_latency</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Average Latency for INT8 Inference: </span><span class="si">{</span><span class="n">average_latency</span> <span class="o">*</span> <span class="mi">1000</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> ms</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Post-process the output
</span><span class="n">tensorrt_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">h_output</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">postprocess</span><span class="p">(</span><span class="n">tensorrt_output</span><span class="p">)</span>
</code></pre></div></div>

<p>What’s Happening Here?</p>

<ul>
  <li>
    <p><strong>Engine Deserialization</strong>: The INT8 engine is loaded from disk and deserialized into memory.</p>
  </li>
  <li>
    <p><strong>Memory Management</strong>: We allocate memory for inputs and outputs on both the host and device, setting up everything needed to run inference.</p>
  </li>
  <li>
    <p><strong>Inference Execution</strong>: After warming up the GPU, inference is run multiple times to measure latency. Using INT8 should provide a significant reduction in latency compared to FP32 inference.</p>
  </li>
  <li>
    <p><strong>Output Post-processing</strong>: Finally, the output is converted back to a PyTorch tensor and processed just like we did with the FP32 model.</p>
  </li>
</ul>

<p>This step demonstrates the power of quantization. With the INT8 engine, we can achieve faster inference times, which is crucial for deploying models in real-time applications.</p>

<h5 id="d-performance-2">d. Performance</h5>
<p>The output from INT8 quantized TensorRT engine is as follows :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Average Latency for INT8 Inference: 0.39 ms
class: hotdog, hot dog, red hot , confidence: 93.55050659179688 %, index: 934
</code></pre></div></div>

<p>As we can see, INT8 quantization achieved a whooping <code class="language-plaintext highlighter-rouge">10.25x speedup</code> as compared to pure Pytorch inference. All the struggle we went through for calibration and quantization was indeed worth it. ;)</p>

<h4 id="summary">Summary</h4>

<p>In this blog, we explore the practical application of quantization using TensorRT to significantly speed up inference on a ResNet-based image classification model. We begin with a baseline comparison, demonstrating inference using pure PyTorch, which provides a foundational understanding of the model’s performance. Next, we transition to using a TensorRT engine, showcasing the initial speed improvements by optimizing the model for NVIDIA GPUs. Finally, we delve into INT8 quantization, applying calibration techniques to maximize efficiency. The results are striking, with a speedup of over 10x compared to the original PyTorch inference, illustrating the power of quantization in real-world scenarios.</p>]]></content><author><name></name></author><category term="cuda" /><category term="nvidia" /><category term="tensorrt" /><category term="deep-learning" /><summary type="html"><![CDATA[How to exactly quantize models and still not lose accuracy.]]></summary></entry><entry><title type="html">Down the CudaMemory lane</title><link href="https://sanket-pixel.github.io//blog/2024/down-the-cudamemory-lane/" rel="alternate" type="text/html" title="Down the CudaMemory lane" /><published>2024-07-14T19:53:00+00:00</published><updated>2024-07-14T19:53:00+00:00</updated><id>https://sanket-pixel.github.io//blog/2024/down-the-cudamemory-lane</id><content type="html" xml:base="https://sanket-pixel.github.io//blog/2024/down-the-cudamemory-lane/"><![CDATA[<h4 id="in-the-fast-paced-landscape-of-deep-learning-deployment-where-performance-optimization-is-critical-understanding-the-foundational-principles-behind-memory-management-and-data-transfer-between-cpus-and-gpus-is-essential-this-blog-aims-to-demystify-these-concepts-by-starting-from-first-principles-well-explore-how-memory-allocation-operates-on-cpus-using-malloc-and-on-gpus-using-cudamalloc-shedding-light-on-their-distinct-functionalities-additionally-well-unravel-the-complexities-of-cudamemcpy-a-crucial-function-for-seamless-data-exchange-between-these-processing-units-by-clarifying-these-fundamental-concepts-we-empower-developers-to-strategically-optimize-their-applications-for-maximum-efficiency-and-speed-in-gpu-accelerated-environments">In the fast-paced landscape of deep learning deployment, where performance optimization is critical, understanding the foundational principles behind memory management and data transfer between CPUs and GPUs is essential. This blog aims to demystify these concepts by starting from first principles. We’ll explore how memory allocation operates on CPUs using <code class="language-plaintext highlighter-rouge">malloc</code> and on GPUs using <code class="language-plaintext highlighter-rouge">cudaMalloc</code>, shedding light on their distinct functionalities. Additionally, we’ll unravel the complexities of <code class="language-plaintext highlighter-rouge">cudaMemcpy</code>, a crucial function for seamless data exchange between these processing units. By clarifying these fundamental concepts, we empower developers to strategically optimize their applications for maximum efficiency and speed in GPU-accelerated environments.</h4>

<p><br /></p>
<div style="width: 95%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_5/memory-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_5/memory-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_5/memory-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_5/memory.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
 Data Transfers Between CPU and GPU 
</div>
</div>

<p>We will understand the fundamentals of CPU and GPU memory manipulations and writing and reading data from the same in the following sections. We first look at how to allocate memory on CPU and GPU, following which we look at how to copy data from the CPU to the GPU and vice versa.</p>

<h3 id="1-understanding-memory-allocation">1. Understanding Memory Allocation</h3>
<p>Efficient memory allocation is fundamental to optimizing computational tasks, particularly when working with CPU and GPU resources. This section delves into the specifics of <code class="language-plaintext highlighter-rouge">malloc</code> and <code class="language-plaintext highlighter-rouge">cudaMalloc</code>, the primary functions used for memory allocation in C++ and CUDA respectively, and highlights the key differences between CPU and GPU memory allocation.</p>

<h4 id="11-malloc">1.1 malloc</h4>
<p><code class="language-plaintext highlighter-rouge">malloc</code>, short for “memory allocation,” is a standard library function in C and C++ used to allocate a block of memory on the heap. The memory allocated by malloc is uninitialized and is typically used for dynamic data structures such as arrays and linked lists.</p>

<div style="width: 80%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_5/malloc-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_5/malloc-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_5/malloc-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_5/malloc.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="malloc" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Malloc
</div>
</div>

<p><strong>Example Usage</strong></p>

<p>In this example, we allocate memory for an array of integers of size 10.</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">size_t</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="nf">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span> <span class="c1">// Allocate memory for 10 integers</span>
</code></pre></div></div>
<p>We first compute the size of the memory that needs to be allocated on the CPU in bytes, since the malloc function accepts the size parameter in bytes. To that end, we multiply the desired array size ( 10 in this case ) with the bytesize of the desired data type ( integer in this case ). The <code class="language-plaintext highlighter-rouge">sizeof</code> function returns the size of the datatype in bytes. Just for information, this results in 40 ( 10 * 4) since each integer has the size of 4 bytes.</p>

<p>Next, we use the all important <code class="language-plaintext highlighter-rouge">malloc()</code> function to allocate the memory of the size computed before.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="o">*</span><span class="n">array</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span> <span class="n">malloc</span><span class="p">(</span><span class="n">size</span><span class="p">);</span>
</code></pre></div></div>
<p>The malloc function returns a pointer pointing to the memory address in the CPU where the allocation has been done. In particular, it returns a <code class="language-plaintext highlighter-rouge">void*</code> type pointer. Since we are trying to create an array of integers, we type case this <code class="language-plaintext highlighter-rouge">void*</code> to <code class="language-plaintext highlighter-rouge">int*</code>.</p>

<p>A good question that should arise at this point is, what difference does this typecasting have on the actual CPU memory. The memory block allocated by <code class="language-plaintext highlighter-rouge">malloc</code> is simply a contiguous block of raw bytes. The casting does not alter this raw memory. The casting simply lets the compiler know that this memory address points to an array of integers. The advantages are as follows :</p>

<p><strong>a. Pointer Arthematic</strong></p>

<p>With an <code class="language-plaintext highlighter-rouge">int*</code> pointer, pointer arithmetic operates in units of <code class="language-plaintext highlighter-rouge">int</code>. For example, if array is an <code class="language-plaintext highlighter-rouge">int*</code>, then array + 1 points to the next int in the array (4 bytes away if sizeof(int) is 4).</p>

<p><strong>b. Assigning Elements</strong></p>

<p>After type typecasting to <code class="language-plaintext highlighter-rouge">int*</code> integer values can be assigned by simply passing indexm like a regular integer array.</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">i</span><span class="p">;</span> <span class="c1">// Store the square of i in the allocated array</span>
<span class="p">}</span>
</code></pre></div></div>

<p><strong>c. Accessing Elements</strong></p>

<p><code class="language-plaintext highlighter-rouge">array[i]</code> accesses the i-th int in the allocated memory block.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span> <span class="c1">// Print the values stored in the array</span>
<span class="p">}</span>
</code></pre></div></div>
<p>To summarize, here is the code for allocating memory on the CPU for an array of 10 integers.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">size_t</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="nf">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span> <span class="c1">// Allocate memory for 10 integers</span>
<span class="kt">int</span> <span class="o">*</span><span class="n">array</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span> <span class="n">malloc</span><span class="p">(</span><span class="n">size</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">array</span> <span class="o">==</span> <span class="nb">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">"Failed to allocate memory"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">return</span> <span class="n">EXIT_FAILURE</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Use the allocated memory</span>
<span class="n">free</span><span class="p">(</span><span class="n">array</span><span class="p">);</span> <span class="c1">// Free the allocated memory when done</span>

</code></pre></div></div>
<h4 id="12-cudamalloc">1.2 cudaMalloc</h4>
<p><code class="language-plaintext highlighter-rouge">cudaMalloc</code> is a function provided by CUDA for allocating memory on the GPU. This function is analogous to malloc but is specifically designed for GPU memory, allowing developers for accessing and manipulating GPU memory.</p>

<div style="width: 80%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_5/cudamalloc-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_5/cudamalloc-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_5/cudamalloc-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_5/cudamalloc.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="malloc" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
CudaMalloc
</div>
</div>

<p><strong>Example Usage</strong></p>

<p>Let us stick with the same example as we used for malloc(), but this time we allocate the array of integers of size 10 on the GPU.
For starters, just like before, lets compute the number of bytes needed for an integer array of size 10.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">size_t</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="nf">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span> <span class="c1">// Allocate memory for 10 integers on GPU</span>
</code></pre></div></div>
<p>Now, we declare a pointer to the start of this integer array, just like we would for the CPU.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="o">*</span><span class="n">d_array</span><span class="p">;</span>
</code></pre></div></div>
<p>But there is a small catch here.</p>

<p><strong><em>Alhthough we declare the <code class="language-plaintext highlighter-rouge">int *d_array</code> on the CPU, this pointer is intended to point to a memory location on the GPU.</em></strong></p>

<p>We will understand this in further detail, once we complete the memory allocation on the GPU.
To that end, we now add the final missing piece which is calling <code class="language-plaintext highlighter-rouge">cudaMalloc()</code></p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cudaError_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_array</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
</code></pre></div></div>

<p>As shown above, the <code class="language-plaintext highlighter-rouge">cudaMalloc()</code> function, expects <strong>a pointer to a pointer</strong> as input. In other words, we first declare a device pointer <code class="language-plaintext highlighter-rouge">int *d_array</code>, and then pass the address of this pointer on the CPU <code class="language-plaintext highlighter-rouge">(&amp;d_array)</code> to the cudaMalloc function.
We then cast this pointer to a pointer to type <code class="language-plaintext highlighter-rouge">(void**)</code> and pass that as input along with the size of intended memory allocation in bytes.</p>

<p>This function then allocates the expected size of memory onto the GPU starting from memory location <code class="language-plaintext highlighter-rouge">d_array</code>. Let us understand this visually.</p>

<div style="width: 80%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_5/deviceptr-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_5/deviceptr-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_5/deviceptr-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_5/deviceptr.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="malloc" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Intuition of a Device Pointer. The CPU location 4 stores the allocated GPU memory location C.
</div>
</div>

<p>In this example, for the sake of simplicity, lets assume that the CPU memory address is indexed by numbers 1,2,3… and so on. 
Furthermore, the GPU is indexed by A,B,C.. and so on.  In the first step, when <code class="language-plaintext highlighter-rouge">int *d_array</code> is declared, the memory location 4 on the CPU
is allocated, to store the address of the GPU.</p>

<p>Then after <code class="language-plaintext highlighter-rouge">cudaMalloc</code>, this address stores the value C, which is the memory location allocated on the GPU.
In this context, <code class="language-plaintext highlighter-rouge">*d_array</code> is <strong>C</strong> (on the GPU) and <code class="language-plaintext highlighter-rouge">&amp;(*d_array)</code> is <strong>4</strong> ( on the CPU).
To summarize, here is the code for allocating memory for 10 integers on the GPU.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">size_t</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="nf">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span> <span class="c1">// Allocate memory for 10 integers on GPU</span>
<span class="kt">int</span> <span class="o">*</span><span class="n">d_array</span><span class="p">;</span>
<span class="n">cudaError_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_array</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">err</span> <span class="o">!=</span> <span class="n">cudaSuccess</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">"Failed to allocate GPU memory: "</span> <span class="o">&lt;&lt;</span> <span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">return</span> <span class="n">EXIT_FAILURE</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Now that we have understood memory allocation, lets look at how to copy data within the CPU, GPU and between CPU and GPU.</p>

<h3 id="2-copying-explained-from-first-principals">2. Copying explained from first principals</h3>

<p>Now that we have understood memory allocation on both the CPU and GPU, let us understand how to transfer data between them.
We will use the same example of an integer array of size 10, to keep life simple, and focus on understanding the underlying concepts from first principals.</p>

<p>In this example, we will perform the following operations :</p>

<ol>
  <li>Allocate memory on the CPU for integer array of size 10.</li>
  <li>Allocate memory on the GPU for integer array of size 10.</li>
  <li>Initialize the CPU array with squares of integers from 1 to 10.</li>
  <li>Copy data from the CPU to the GPU.</li>
  <li>Copy the data back from the GPU to the CPU.</li>
  <li>Print values to verify it all works.</li>
</ol>

<p>Lets look at each step, one at a time.</p>

<h4 id="step-1--allocate-memory-on-the-cpu">STEP 1 : Allocate memory on the CPU</h4>

<p>As already explained, we first compute the size in bytes and allocate appropriate memory using <code class="language-plaintext highlighter-rouge">malloc()</code>.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">size_t</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="nf">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span>
<span class="c1">// Allocate memory on the CPU (host)</span>
<span class="kt">int</span> <span class="o">*</span><span class="n">h_array</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span> <span class="n">malloc</span><span class="p">(</span><span class="n">size</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">h_array</span> <span class="o">==</span> <span class="nb">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">"Failed to allocate CPU memory"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">return</span> <span class="n">EXIT_FAILURE</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<h4 id="step-2---allocate-memory-on-the-gpu">STEP 2 : : Allocate Memory on the GPU</h4>
<p>We first compute the size of allocation in bytes and use <code class="language-plaintext highlighter-rouge">cudaMalloc</code> as explained above.</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Allocate memory on the GPU (device)</span>
<span class="kt">int</span> <span class="o">*</span><span class="n">d_array</span><span class="p">;</span>
<span class="n">cudaError_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_array</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">err</span> <span class="o">!=</span> <span class="n">cudaSuccess</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">"Failed to allocate GPU memory: "</span> <span class="o">&lt;&lt;</span> <span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="n">free</span><span class="p">(</span><span class="n">h_array</span><span class="p">);</span> <span class="c1">// Free CPU memory if GPU allocation fails</span>
    <span class="k">return</span> <span class="n">EXIT_FAILURE</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<h4 id="step-3-initialize-the-cpu-array">Step 3: Initialize the CPU Array</h4>

<p>We initialize the CPU array with the squares of integers from 1 to 10.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Initialize CPU memory with data</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">h_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// Squares of integers from 1 to 10</span>
<span class="p">}</span>
</code></pre></div></div>

<h4 id="step-4-copy-data-from-the-cpu-to-the-gpu">Step 4: Copy Data from the CPU to the GPU</h4>
<p>We now copy this data from the CPU to GPU in the allocated memory.
Here we take a small stop, and first understand <code class="language-plaintext highlighter-rouge">cudaMemcpy</code> in detail.</p>

<h5 id="41-cudamemcpy-explained">4.1 cudaMemcpy Explained</h5>
<p>The <code class="language-plaintext highlighter-rouge">cudaMemcpy</code> function is essential for transferring data between the <code class="language-plaintext highlighter-rouge">host</code> (CPU) and the <code class="language-plaintext highlighter-rouge">device</code> (GPU) in CUDA programming. Let’s break down this function and understand it intuitively from first principles. First, the syntax is as shown in the figure below.</p>

<div style="width: 80%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_5/cudamemcpy-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_5/cudamemcpy-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_5/cudamemcpy-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_5/cudamemcpy.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="malloc" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Syntax of cudaMemcpy.
</div>
</div>

<p>The function takes in the destination pointer ( the memory location where the data needs to be copied to), source pointer (the memory location from where the data needs to be copied from ). Furhtermore, it also takes in the number of bytes that need to copied from. It is important to note, that these destination and source pointers may be on the CPU and GPU, depending on the direction of copying provided in the final parameter <code class="language-plaintext highlighter-rouge">cudaMemcpyKind</code>.
It is pretty obvious that there can only be 4 possible direction of copying data in this context.</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">cudaMemcpyHostToHost</code>: Copy data from host to host.</li>
  <li><code class="language-plaintext highlighter-rouge">cudaMemcpyHostToDevice</code>: Copy data from host to device.</li>
  <li><code class="language-plaintext highlighter-rouge">cudaMemcpyDeviceToHost</code>: Copy data from device to host.</li>
  <li><code class="language-plaintext highlighter-rouge">cudaMemcpyDeviceToDevice</code>: Copy data from device to device.</li>
</ol>

<p>Now that we have the syntax and intuition of <code class="language-plaintext highlighter-rouge">cudaMemcpy</code> covered, let us continue with our example and use this function.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Copy data from CPU to GPU</span>
<span class="n">err</span> <span class="o">=</span> <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_array</span><span class="p">,</span> <span class="n">h_array</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">err</span> <span class="o">!=</span> <span class="n">cudaSuccess</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">"Failed to copy data from CPU to GPU: "</span> <span class="o">&lt;&lt;</span> <span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_array</span><span class="p">);</span> <span class="c1">// Free GPU memory if copy fails</span>
    <span class="n">free</span><span class="p">(</span><span class="n">h_array</span><span class="p">);</span>     <span class="c1">// Free CPU memory</span>
    <span class="k">return</span> <span class="n">EXIT_FAILURE</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>
<p>Here, we use <code class="language-plaintext highlighter-rouge">cudaMemcpy</code> to copy the data from the CPU (<code class="language-plaintext highlighter-rouge">h_array</code>) to the GPU (<code class="language-plaintext highlighter-rouge">d_array</code>). We specify <code class="language-plaintext highlighter-rouge">cudaMemcpyHostToDevice</code> to indicate the direction of the copy.</p>

<h4 id="step-5--copy-data-from-the-gpu-to-the-cpu">Step 5:  Copy Data from the GPU to the CPU</h4>
<p>We now copy the data back to the CPU from the GPU. As expected the source pointer now becomes the GPU pointer and destination becomes the CPU pointer.
Furthermore the direction of copying is from device to host so the <code class="language-plaintext highlighter-rouge">cudaMemcpyKind</code> will be <code class="language-plaintext highlighter-rouge">cudaMemcpyDeviceToHost</code>.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">err</span> <span class="o">=</span> <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">h_array</span><span class="p">,</span> <span class="n">d_array</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">err</span> <span class="o">!=</span> <span class="n">cudaSuccess</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">"Failed to copy data from GPU to CPU: "</span> <span class="o">&lt;&lt;</span> <span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_array</span><span class="p">);</span> <span class="c1">// Free GPU memory if copy fails</span>
    <span class="n">free</span><span class="p">(</span><span class="n">h_array</span><span class="p">);</span>     <span class="c1">// Free CPU memory</span>
    <span class="k">return</span> <span class="n">EXIT_FAILURE</span><span class="p">;</span>
<span class="p">}</span>

</code></pre></div></div>

<h4 id="step-6--verify-consistency-after-copying">Step 6 : Verify consistency after copying</h4>

<p>Just for completeness we now verify if the data copied back from the GPU to CPU is as expected by simply printing the host array.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Print the data copied back from GPU to CPU</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">h_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

<span class="c1">// Free the allocated memory</span>
<span class="n">cudaFree</span><span class="p">(</span><span class="n">d_array</span><span class="p">);</span>
<span class="n">free</span><span class="p">(</span><span class="n">h_array</span><span class="p">);</span>

</code></pre></div></div>

<p>In the end, we free the GPU and CPU memory using <code class="language-plaintext highlighter-rouge">cudaFree</code> and <code class="language-plaintext highlighter-rouge">free</code> respectively,</p>

<p>Let us look at the entire code to recap.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
    <span class="c1">// Step 1: Allocate Memory on the CPU for an Integer Array of Size 10</span>
    <span class="kt">size_t</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span>

    <span class="c1">// Allocate memory on the CPU (host)</span>
    <span class="kt">int</span> <span class="o">*</span><span class="n">h_array</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span> <span class="n">malloc</span><span class="p">(</span><span class="n">size</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">h_array</span> <span class="o">==</span> <span class="nb">nullptr</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">"Failed to allocate CPU memory"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
        <span class="k">return</span> <span class="n">EXIT_FAILURE</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// Step 2: Allocate Memory on the GPU for an Integer Array of Size 10</span>
    <span class="kt">int</span> <span class="o">*</span><span class="n">d_array</span><span class="p">;</span>
    <span class="n">cudaError_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_array</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">err</span> <span class="o">!=</span> <span class="n">cudaSuccess</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">"Failed to allocate GPU memory: "</span> <span class="o">&lt;&lt;</span> <span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
        <span class="n">free</span><span class="p">(</span><span class="n">h_array</span><span class="p">);</span> <span class="c1">// Free CPU memory if GPU allocation fails</span>
        <span class="k">return</span> <span class="n">EXIT_FAILURE</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// Step 3: Initialize the CPU Array with Squares of Integers from 1 to 10</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">h_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// Squares of integers from 1 to 10</span>
    <span class="p">}</span>

    <span class="c1">// Step 4: Copy Data from the CPU to the GPU</span>
    <span class="n">err</span> <span class="o">=</span> <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_array</span><span class="p">,</span> <span class="n">h_array</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">err</span> <span class="o">!=</span> <span class="n">cudaSuccess</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">"Failed to copy data from CPU to GPU: "</span> <span class="o">&lt;&lt;</span> <span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
        <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_array</span><span class="p">);</span> <span class="c1">// Free GPU memory if copy fails</span>
        <span class="n">free</span><span class="p">(</span><span class="n">h_array</span><span class="p">);</span>     <span class="c1">// Free CPU memory</span>
        <span class="k">return</span> <span class="n">EXIT_FAILURE</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// Step 5: Copy the Data Back from the GPU to the CPU</span>
    <span class="n">err</span> <span class="o">=</span> <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">h_array</span><span class="p">,</span> <span class="n">d_array</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">err</span> <span class="o">!=</span> <span class="n">cudaSuccess</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">"Failed to copy data from GPU to CPU: "</span> <span class="o">&lt;&lt;</span> <span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
        <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_array</span><span class="p">);</span> <span class="c1">// Free GPU memory if copy fails</span>
        <span class="n">free</span><span class="p">(</span><span class="n">h_array</span><span class="p">);</span>     <span class="c1">// Free CPU memory</span>
        <span class="k">return</span> <span class="n">EXIT_FAILURE</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// Step 6: Print Values to Verify It All Works</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">h_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

    <span class="c1">// Free the allocated memory</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_array</span><span class="p">);</span>
    <span class="n">free</span><span class="p">(</span><span class="n">h_array</span><span class="p">);</span>

    <span class="k">return</span> <span class="n">EXIT_SUCCESS</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<h3 id="3-conclusion">3. Conclusion</h3>

<p>In this blog, we explored the essential concepts of memory allocation and data transfer between the CPU and GPU, which are key to optimizing performance in GPU-accelerated applications. We began by examining how memory is allocated on the CPU using malloc, which reserves a block of memory for use in programs. We then discussed GPU memory allocation with cudaMalloc, which functions similarly but is used for allocating memory on the GPU.</p>

<p>Following memory allocation, we demonstrated how to transfer data between the CPU and GPU using cudaMemcpy. We highlighted the process of copying data from the CPU to the GPU, performing computations, and then copying the results back to the CPU. This seamless data transfer enables efficient use of the GPU’s processing power, allowing for faster and more powerful computations. Understanding these fundamental operations is crucial for anyone looking to leverage GPU acceleration in their applications.</p>]]></content><author><name></name></author><category term="quantization" /><category term="cuda" /><category term="nvidia" /><summary type="html"><![CDATA[Data Transfers Between CPU and GPU]]></summary></entry><entry><title type="html">Quantization explained, like you are five.</title><link href="https://sanket-pixel.github.io//blog/2024/quantization-explained-like-youre-five/" rel="alternate" type="text/html" title="Quantization explained, like you are five." /><published>2024-03-29T19:53:00+00:00</published><updated>2024-03-29T19:53:00+00:00</updated><id>https://sanket-pixel.github.io//blog/2024/quantization-explained-like-youre-five</id><content type="html" xml:base="https://sanket-pixel.github.io//blog/2024/quantization-explained-like-youre-five/"><![CDATA[<h4 id="in-the-era-of-extravagence-where-models-casually-cross-100b-parameters-devouring-over-500gb-of-gpu-memory-and-costing-millions-of-dollars-for-a-single-training-session-quantization-comes-in-as-a-prudent-accountantit-ensures-that-models-refrain-from-indulging-in-excessive-memory-consumption-while-minimizing-any-loss-in-model-quality---in-this-blog-post-we-aim-to-demystify-this-potent-mathematical-framework-using-intuitive-explanations-relatable-examples-and-accessible-language-we-will-also-delve-into-the-fancy-jargons-and-the-ugly-math-that-come-along-with-quantization-just-deeply-enough-to-allow-readers-to-nagivate-research-papers-and-documentation-on-quantization-libraries-the-objective-is-to-make-these-esoteric-concepts-more-approachable-and-less-daunting-so-buckle-up-as-we-embark-on-this-journey-as-we-learn-how-to-take-mammoth-ml-models-and-prune-them-down-to-preserve-only-the-essential">In the era of extravagence, where models casually cross 100B parameters, devouring over 500GB of GPU memory and costing millions of dollars for a single training session, quantization comes in as a prudent accountant.It ensures that models refrain from indulging in excessive memory consumption while minimizing any loss in model quality.   In this blog post, we aim to demystify this potent mathematical framework using intuitive explanations, relatable examples, and accessible language. We will also delve into the fancy jargons and the ugly math that come along with quantization, just deeply enough to allow readers to nagivate research papers and documentation on quantization libraries. The objective is to make these esoteric concepts more approachable and less daunting. So buckle up as we embark on this journey, as we learn how to take mammoth ML models, and prune them down to preserve only the essential.</h4>

<p><br /></p>
<div style="width: 95%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_4/michael-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_4/michael-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_4/michael-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_4/michael.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
  Explained so simply, even Michael Scott would get it.
</div>
</div>

<h3 id="why-bother-learning-about-quantization">Why bother learning about Quantization?</h3>
<p>In recent years, both language models and computer vision models have undergone a significant evolution, with newer models boasting unprecedented sizes and complexities. For instance, language models like GPT-3 and computer vision models like EfficientNet have reached staggering parameter counts, with GPT-3 having 175 billion parameters and EfficientNet surpassing billions of parameters across its variants.</p>

<p>However, the sheer size of these models presents practical challenges, particularly in terms of deployment on everyday devices. Consider a language model like GPT-3—its inference alone demands extensive computational resources, with estimates suggesting the need for multiple high-performance GPUs. Similarly, for computer vision tasks, deploying models like EfficientNet on resource-constrained devices can be daunting due to their computational and memory requirements.</p>

<p>To overcome these hurdles, techniques such as quantization have emerged as indispensable tools. By compressing the parameters of these large models into lower precision formats, such as INT8, quantization offers a pathway to significantly reduce memory footprint and computational demands without compromising performance. This is crucial for making these cutting-edge models accessible and deployable across a diverse range of devices, from smartphones to edge devices.</p>

<p>To summarize, learning about quantization, will help you deploy large models, on relatively small devices, while also consuming less power. But before we delve into the core quantization concepts, let us first look at the common data types used in Machine Learning.</p>

<h3 id="0-common-data-types-in-machine-learning">0. Common Data Types in Machine Learning</h3>

<p>In machine learning, data types, or precision, play a vital role in model performance and efficiency. The most common data types used include float32, float16, bfloat16, and int8.
<br /></p>
<div style="width: 70%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_4/types-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_4/types-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_4/types-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_4/types.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="quantization steps" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
  Figure 0. Common Data Types in Machine Learning
</div>
</div>
<ul>
  <li>
    <p><strong>Float32 (FP32):</strong> This 32-bit floating point representation offers a wide range of numbers, with 8 bits for the exponent, 23 bits for the mantissa, and 1 bit for the sign. FP32 provides high precision and dynamic range suitable for most computations.</p>
  </li>
  <li>
    <p><strong>Float16 (FP16):</strong> With 5 bits for the exponent and 10 bits for the mantissa, FP16 offers lower precision compared to FP32. While efficient for many computations, FP16’s limited dynamic range can lead to overflow and underflow issues.</p>
  </li>
  <li>
    <p><strong>Bfloat16 (BF16):</strong> BF16 addresses the limitations of FP16 by allocating 8 bits for the exponent and 7 bits for the fraction. This allows it to maintain the dynamic range of FP32 while sacrificing some precision.</p>
  </li>
  <li>
    <p><strong>Int8 (INT8):</strong> Int8 consists of an 8-bit representation capable of storing 2^8 different values. While it offers lower precision than floating point formats, INT8 is often used in quantization to reduce model size and improve inference efficiency.</p>
  </li>
</ul>

<p>In the context of quantization, FP32 is considered full precision, while FP16, BF16, and INT8 are referred to as reduced precision formats. During training, a mixed precision approach may be employed, where FP32 weights are used as a reference, while computations are performed in lower precision formats to enhance training speed and resource efficiency. This allows for efficient utilization of computational resources while maintaining model accuracy.</p>

<h3 id="1-what-is-quantization">1. What is Quantization?</h3>
<p>Quantization, simply put, is the art of converting all decimal numbers ( ex. <code class="language-plaintext highlighter-rouge">float32</code>) in your data, into whole numbers within a fixed range (ex. <code class="language-plaintext highlighter-rouge">int8</code>), with a mathematical framework, that still allows us to recover the original decimal number from the whole number when needed. This is ofcourse an oversimplicatoin, if there ever was one. While the description might seem straightforward, quantization is more of an art than a rigid science. The conversion methodology is not set in stone and lacks strict determinism, which is what makes it fun, and blog worthy.</p>

<p>Let’s envision a scenario where we have a full HD image of a cat, occupying a hefty 8 megabytes of memory. Now, through the magic of quantization, we pixelate this image just enough to lose some fine details while retaining the essence of the feline subject. As a result, the memory storage required to represent the image diminishes significantly, perhaps to just a fraction of its original size. This trade-off between fidelity and memory efficiency encapsulates the essence of quantization, where the reduction in granularity leads to tangible benefits in terms of resource optimization. Just as pixelation preserves the overall identity of the cat in our image, quantization ensures that our deep learning models maintain their performance while operating within constrained memory environments.</p>

<h3 id="2-fundamentals-of-quantization">2. Fundamentals of Quantization</h3>
<p>Before we get into the ugly math and fancy jarons, let us understand the intuition. Quantization (however fancy it may sound) is just converting decimal values ( eg. <code class="language-plaintext highlighter-rouge">float32</code> ) into whole numbers (eg. <code class="language-plaintext highlighter-rouge">int8</code> ), in a way that it is feasible to recover the decimal number back from the whole number. Lets say we want to quantize a list of float values that are all between -4.0 and +4.0. And we want to represnet these float values in a universe that just consists of integers between -127 and 127. We would like to have a mechanism, wherein, the minimum value -4.0 maps to -127, and the maximum value +4.0, maps to 127. This would allow us to caputure the essence of all the float values in this only-integer universe of ours. Let us understand how we quantize our list of decimal values, one step at a time.</p>

<p><br /></p>
<div style="width: 70%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_4/quantize-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_4/quantize-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_4/quantize-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_4/quantize.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="quantization steps" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
  Figure 1. Steps in Quantization
</div>
</div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Original float array to be quantized :
[-2.3, -1.1, 0.0, 1.5, 2.8, 4.0]

STEP 1 : Find the maximum value in the array 
max([-2.3, -1.1, 0.0, 1.5, 2.8, 4.0]) = 4.0

STEP 2 : Calculate the scaling constant  
s = 127/4.0 = 31.75

STEP 3 : Multiply all values in array by scaling constant
=  [-2.3, -1.1, 0.0, 1.5, 2.8, 4.0] * 31.75
=  [ -73.025, -34.925, 0, 47.625, 88.9, 127.0]

STEP 4 : Round all numbers to obtain integer representation. 
         Clamp all numbers between (-127,127)
= [-73, -35, 0, 47, 127]                    &lt;---- Quantized Values

</code></pre></div></div>

<p><br /></p>
<div style="width: 60%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_4/dequantization-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_4/dequantization-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_4/dequantization-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_4/dequantization.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="quantization steps" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
  Figure 2. Steps in Dequantization to recover original values
</div>
</div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Dequantization for recovering original values :

STEP 5 : Recover original number by dividing by scaling constant.
= [-2.299, -1.1023, 0, 1.5118, 2.8031, 4.0] &lt;---- Dequantized Values

</code></pre></div></div>

<p>As we can see, after division by scaling constant, we were able to reasonably approximate the orignal float values from the quantized value. We will refer to <code class="language-plaintext highlighter-rouge">STEP 3</code> and <code class="language-plaintext highlighter-rouge">STEP 4</code> as <strong>Quantization</strong> and <code class="language-plaintext highlighter-rouge">STEP 5</code> as <strong>Dequantization</strong>.</p>

<p>Okay, now as promised, presenting the same ideas explained above, but with the <em>ugly math</em> and fancy jargons. The fancy jargons, we shall now deal with include :</p>

<ol>
  <li>Range Mapping
    <ul>
      <li>Scale Quantization</li>
      <li>Affine Quantization</li>
    </ul>
  </li>
  <li>Tensor Quantization Granularity
    <ul>
      <li>Per Tensor Granularity</li>
      <li>Per Element Granularity.</li>
      <li>Per Row/Column/Channel Granularity</li>
    </ul>
  </li>
  <li>Calibration</li>
</ol>

<p>Lets understand them one at a time.</p>

<h4 id="21-range-mapping">2.1 Range Mapping</h4>
<p>Range mapping, as the names suggest, is the mechanism for transforming continuous float values into discrete integers. Let’s denote the chosen range of representable real values as \([\beta, \alpha]\), similar to -4 and +4 in earlier example. Let the signed integer space be restricted to the bit-width b. In earlier example, since the signed integer universe had range of {-128,127}, the bit width was 8.  The process of quantization involves mapping an input value \(x ∈ [β, α]\) to reside within the range \([−2^{b-1}, 2^{b-1} - 1]\). This mapping can either be an affine transformation \(f(x) = s.x + z\) or, its special case \(f(x) = s.x\) where \(x, s, z ∈ R\). We refer to these mappins as <em>affine mapping</em> and <em>scale mapping</em> respectively.</p>

<h5 id="211-affine-mapping">2.1.1 Affine Mapping</h5>
<p>This mapping usually takes place using multiplication (scaling) with a <strong>scaling factor <em>s</em></strong>, \(f (x) = s · x\).  Another variant of this mapping can be <em>affine mapping</em>. Affine mapping, is just scaling, along with addition of  a constant called <strong>zero point \(z\)</strong>,  \(f(x) = s · x + z\). The constant in affine mapping \(z\) is called <strong>zero point</strong> because it represents the value in the quantized integer space, that corresponds to the zero in the float space. Now, let’s delve into the math that underlies these transformations and understand how they bring about the crucial conversion from continuous to discrete representations.
<br /></p>
<div style="width: 60%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_4/affine-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_4/affine-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_4/affine-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_4/affine.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="affine" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
  Figure 3. Affine Mapping
</div>
</div>
<p>Affine quantization serves as the bridge that maps a real value \(x \in \mathbb{R}\) to a \(b\)-bit signed integer \(x_p \in \{-2^{(n-1)}, -2^{(n-1)} + 1, ..., 2^{(n-1)} - 1\}\). The transformation function \(f(x) = s \cdot x + z\) is defined by:</p>

\[s = \frac{2^{(b - 1)}}{\alpha - \beta}\]

\[z = - \text{round}(\beta \cdot s) - 2^{(b-1)}\]

<p>Here, \(s\) is the scale factor, and \(z\) is the zero-point - the integer value to which the real value zero is mapped. For an 8-bit representation \(b = 8\), \(s = \frac{255}{\alpha - \beta}\) and \(z = - \text{round}(\beta \cdot s) - 128\). The quantize operation, described by the equations below, involves clipping the result to the specified range:</p>

\[x_p = \text{quantize}(x, b, s, z) = \text{clip}(\text{round}(s \cdot x + z), -2^{(b-1)}, 2^{(b-1)} - 1)\]

<p>The dequantize function provides an approximation of the original real-valued input \(x\):</p>

\[x̂ = \text{dequantize}(x_p, s, z) = \frac{1}{s} (x_p - z)\]

<p>This transformation ensures the mapping of real values to int8 representation with affine quantization, where \(s\) represents the ratio of the integer-representable range to the chosen real range.</p>

<h5 id="212-scale-mapping">2.1.2 Scale Mapping</h5>

<p>Scale quantization performs range mapping with only a scale transformation, using multiplication (scaling) with a <strong>scaling factor <em>s</em></strong>, \(f (x) = s · x\). We focus on the symmetric variant of scale quantization, where the input range and integer range are symmetric around zero. In this case, for int8, the integer range is {−127, 127}, avoiding the value -128 in favor of symmetry. Here, the zero point in the float space, maps to the zero point in the integer space.</p>

<p><br /></p>
<div style="width: 60%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_4/scale-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_4/scale-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_4/scale-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_4/scale.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="scale" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
  Figure 4. Scale Mapping
</div>
</div>

<p>We define the scale quantization of a real value \(x\), with a chosen representable range \([−α, α]\), producing a \(b\)-bit integer value, \(x_q\) as follows,</p>

\[s=\frac{2^{b−1}−1}{α}\]

\[x_q=quantize(x,b,s)=clip(round(s⋅x),−2^{b−1}+1,2^{b−1}−1)\]

<p>We recover the approximate original value using the dequantize operation for scale quantization as follows.</p>

\[x̂=dequantize(x_q,s) = \frac{1}{s}{x_q}\]

<p>The scale mapping is similar to the example we looked at earlier, where we quantize by multiplying with a constant, and dequantize by dividing with the same constant.</p>

<h4 id="22-quantization-granularity">2.2 Quantization Granularity</h4>
<p>The scaling factor \(s,z\) are referred to as quantization parameters. The effectiveness of quantization, is entirely dependent of the choice of these parameters. While performing quantization for a neural network graph, we want to quantize the inputs tensors, ( activations ) and also the corresponding weights before performing the operation. The term <em>quantization granularity</em> refers to the level at which quantization parameters are shared among tensor elements. Here are the common choices for quantization granularity:
<br /></p>
<div style="width: 80%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_4/granularity-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_4/granularity-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_4/granularity-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_4/granularity.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="granularity" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
  Figure 5. Quanization Granularity Mapping
</div>
</div>
<ol>
  <li>
    <p><strong>Per Tensor Granularity</strong>: In this approach, the same quantization parameters are shared by all elements in the entire tensor. It is the coarsest granularity and implies that the entire tensor is treated as a single entity during quantization and all elements have the same scaling factor \(s\) and zero point \(z\).</p>
  </li>
  <li>
    <p><strong>Per Element Granularity</strong>: At the finest granularity, each element in the tensor has its individual quantization parameters. This means that each element is quantized independently.</p>
  </li>
  <li>
    <p><strong>Per Row/Column/Channel Granularity</strong>: For 2D matrices or 3D tensors (like images), quantization parameters can be shared over various dimensions. For example, quantization parameters may be shared per row or per column in 2D matrices, or per channel in 3D tensors.</p>
  </li>
</ol>

<p>The choice of quantization granularity affects how quantization parameters are applied to the elements of the tensor, and it provides flexibility in adapting quantization to different structures within the data. Here is a general rule guide for choice of granularity.</p>
<ol>
  <li>
    <p><strong>Weights</strong> : Use per column granularity for weights tensor. All elements in a column of weights tensor should have same quantization parameters.</p>
  </li>
  <li>
    <p><strong>Activations/Inputs</strong> : Use per tensor granularity for activations or inputs to the network. All elements of the entire input tensor should have same quantization paramters.</p>
  </li>
</ol>

<h4 id="23-calibration">2.3 Calibration</h4>
<p>Quantization, as we’ve learned, is the process of converting continuous numerical values into a discrete representation. Calibration, in this context, is the art of carefully choosing the parameters that guide this conversion. Let’s dive into the intuition behind calibration before exploring three calibration methods.</p>

<p>In the quantization process, we aim to squeeze a broad spectrum of real-numbered values into a limited integer space. Calibration ensures we choose the right boundaries for this squeeze, allowing us to maintain the essence of our data while mapping it to a more compact form. Think of it as finding the sweet spot that captures the diversity of values in our model without losing critical information.
<br /></p>
<div style="width: 70%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_4/calibration-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_4/calibration-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_4/calibration-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_4/calibration.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="quantization steps" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
  Figure 6.Types of calibration
</div>
</div>

<p>There are three major strategies to find the correct quantization paramters given the distribution of the values being quantized :</p>
<ol>
  <li>
    <p><strong>Max Calibration</strong>: Simple yet effective, this method sets the range based on the maximum absolute value observed during calibration. It’s like saying, “Let’s make sure we cover the extremes.”</p>
  </li>
  <li>
    <p><strong>Entropy Calibration</strong>: This method leverages KL divergence to minimize information loss between original floating-point values and their quantized counterparts. It’s a nuanced approach, aiming to preserve the distribution of information.</p>
  </li>
  <li>
    <p><strong>Percentile Calibration</strong>: Tailored to the data distribution, this method involves setting the range to a percentile of the absolute values seen during calibration. For instance, a 99% calibration clips the largest 1% of magnitude values.</p>
  </li>
</ol>

<p>Each method brings its own flavor to the calibration process, ensuring that the quantized model not only fits the data but also does so intelligently, preserving crucial details. Calibration becomes the bridge that connects the continuous world of real numbers to the discrete universe of quantization.</p>

<h3 id="3-conclusion">3. Conclusion</h3>
<p>In conclusion, we’ve explored the fundamentals of quantization, ranging from simple examples of quantization and dequantization to more advanced topics such as range mapping, tensor quantization granularity, and calibration. By delving into concepts like scale quantization, affine quantization, and different granularities of tensor quantization, we’ve gained a deeper understanding of how quantization optimizes model memory and computational efficiency without sacrificing performance. In the next blog post, we’ll dive into concrete Python and PyTorch examples to illustrate how these concepts translate into practice, empowering readers to implement quantization techniques effectively in their machine learning workflows. Stay tuned as we continue our journey into the realm of quantization and its transformative impact on machine learning models.</p>]]></content><author><name></name></author><category term="quantization" /><category term="deep-learning" /><category term="quantization" /><summary type="html"><![CDATA[Explaining the intuition behind quantization]]></summary></entry><entry><title type="html">TensorRT meets C++</title><link href="https://sanket-pixel.github.io//blog/2023/tensorrt-meets-cpp/" rel="alternate" type="text/html" title="TensorRT meets C++" /><published>2023-08-08T19:53:00+00:00</published><updated>2023-08-08T19:53:00+00:00</updated><id>https://sanket-pixel.github.io//blog/2023/tensorrt-meets-cpp</id><content type="html" xml:base="https://sanket-pixel.github.io//blog/2023/tensorrt-meets-cpp/"><![CDATA[<h4 id="building-upon-the-foundations-laid-in-our-previous-post-have-you-met-tensorrt-where-we-embarked-on-a-journey-into-the-world-of-basic-concepts-using-python-we-now-delve-into-the-exciting-realm-of-c-by-seamlessly-integrating-tensorrt-with-c-this-blog-unlocks-the-potential-for-readers-to-effortlessly-transition-their-pytorch-models-into-a-c-environment-we-present-an-illustrative-example-of-image-classification-utilizing-the-familiar-model-from-our-earlier-exploration-as-the-blog-unfolds-the-power-of-this-integration-becomes-evidentreaders-will-learn-how-to-read-an-input-image-using-opencv-copy-it-to-the-gpu-perform-inference-to-get-the-output-and-copy-the-output-back-to-the-cpu-this-sets-a-strong-foundation-for-utilizing-this-pipeline-with-any-standard-pytorch-model-this-blog-empowers-readers-with-the-knowledge-to-bridge-the-gap-between-two-domains-ultimately-enabling-them-to-harness-the-capabilities-of-tensorrt-in-the-world-of-c">Building upon the foundations laid in our previous post, “Have you met TensorRT?,” where we embarked on a journey into the world of basic concepts using Python, we now delve into the exciting realm of C++. By seamlessly integrating TensorRT with C++, this blog unlocks the potential for readers to effortlessly transition their PyTorch models into a C++ environment. We present an illustrative example of image classification, utilizing the familiar model from our earlier exploration. As the blog unfolds, the power of this integration becomes evident—readers will learn how to read an input image using OpenCV, copy it to the GPU, perform inference to get the output, and copy the output back to the CPU. This sets a strong foundation for utilizing this pipeline with any standard PyTorch model. This blog empowers readers with the knowledge to bridge the gap between two domains, ultimately enabling them to harness the capabilities of TensorRT in the world of C++.</h4>
<p><br /></p>
<div style="width: 100%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_3/pt_cpp.drawio-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_3/pt_cpp.drawio-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_3/pt_cpp.drawio-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_3/pt_cpp.drawio.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
  TensorRT meets C++
</div>
</div>

<p>Welcome to our blog series where the worlds of TensorRT and C++ converge to revolutionize the AI landscape. In our previous installment, <a href="/blog/2023/introduction-to-tensorrt/">Have you met TensorRT?</a>, we embarked on an exciting exploration of the fundamental concepts, laying the groundwork for the cutting-edge journey we’re about to embark upon. Now, with “TensorRT meets C++”,  we usher you into a realm of possibilities where the seamless integration of these technologies has profound implications for AI, particularly in the context of robotics and edge computing.</p>

<h4 id="unveiling-the-power-of-integration">Unveiling the Power of Integration</h4>

<p>The significance of this integration cannot be understated. While our prior post introduced you to TensorRT’s prowess in Python, this blog takes you a step further. By blending TensorRT with the power of C++, we equip you with the skills to transition your PyTorch models seamlessly into a C++ environment. This transition isn’t just about speed—although the enhanced inference speed is undoubtedly thrilling—it’s about more. It’s about delving into the heart of memory management, understanding the intricacies of processor operations, and acquiring a deeper comprehension of how your models interact with the hardware.</p>

<h4 id="the-challenge-of-copying-navigating-the-gpu-highway">The Challenge of Copying: Navigating the GPU Highway</h4>

<p>With great power comes great responsibility. The marriage of TensorRT and C++ introduces a pivotal challenge: the accurate transfer of data between the CPU and GPU. As we embark on this journey, we delve into the problem of copying data correctly onto the GPU and navigating the intricacies of memory transfers. We dissect this challenge, peeling back the layers to understand how to harmonize these distinct processing units and deliver seamless data flow.</p>

<h4 id="unveiling-a-new-frontier-from-vision-to-robotics">Unveiling a New Frontier: From Vision to Robotics</h4>

<p>Our blog isn’t just about one aspect of AI—it’s about a journey that spans diverse domains. This integration is the key that unlocks possibilities beyond image classification. From lidar-based perception to planning-based decision-making, from text-based sentiment analysis to complex deep reinforcement learning, the door to countless applications opens. All these, in the realm of robotics and edge AI where C++ reigns supreme. As you delve into this blog, you’re not merely mastering a technology; you’re bridging the chasm between your Jupyter notebooks and the robotics that can wield your models.</p>

<h4 id="source-from-github">Source from Github</h4>
<p>For those interested in exploring the code and gaining a deeper understanding of the concepts discussed in this blog on TensorRT and image classification, you can find the complete source code in the corresponding GitHub repository. The repository link is <a href="https://github.com/sanket-pixel/tensorrt_cpp">this</a>.</p>

<h4 id="pre-requisites-and-installation">Pre-requisites and Installation</h4>
<p><strong>1. Hardware requirements</strong></p>
<ul>
  <li>NVIDIA GPU</li>
</ul>

<p><strong>2. Software requirements</strong></p>
<ul>
  <li>Ubuntu &gt;= 18.04</li>
  <li>Python &gt;= 3.8</li>
</ul>

<p><strong>3. Installation Guide</strong></p>
<ol>
  <li>Create conda environment and install required python packages.
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create -n trt python=3.8
conda activate trt
pip install -r requirements.txt
</code></pre></div>    </div>
  </li>
  <li>Install TensorRT 
Install TensorRT:</li>
</ol>

<ul>
  <li>
    <p>Download and install NVIDIA CUDA 11.4 or later following the official instructions: <a href="https://developer.nvidia.com/cuda-toolkit-archive">link</a></p>
  </li>
  <li>
    <p>Download and extract CuDNN library for your CUDA version (&gt;8.9.0) from: <a href="https://developer.nvidia.com/cudnn">link</a></p>
  </li>
  <li>
    <p>Download and extract NVIDIA TensorRT library for your CUDA version from: <a href="https://developer.nvidia.com/nvidia-tensorrt-8x-download">link</a>. Minimum required version is 8.5. Follow the Installation Guide for your system and ensure Python’s part is installed.</p>
  </li>
  <li>
    <p>Add the absolute path to CUDA, TensorRT, and CuDNN libs to the environment variable PATH or LD_LIBRARY_PATH.</p>
  </li>
  <li>
    <p>Install PyCUDA:</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install pycuda
</code></pre></div>    </div>
  </li>
</ul>

<p><br /></p>

<h4 id="the-road-ahead-sections-to-explore">The Road Ahead: Sections to Explore</h4>

<p>As we journey through “TensorRT meets C++,” we’ll traverse several vital sections:</p>

<ol>
  <li>
    <p><strong>Intuition</strong>: Lay the groundwork by understanding the synergy between TensorRT and C++.</p>
  </li>
  <li>
    <p><strong>Inside Deep Dive</strong>: Embark on an exploration of the inner workings, understanding the harmony of memory and processing.</p>
  </li>
  <li>
    <p><strong>Latency and Consistency</strong>: Quantify the gains—measure the reduced latency that TensorRT offers.</p>
  </li>
  <li>
    <p><strong>Conclusion</strong>: Weave together the threads of knowledge, leaving you ready to wield the integration with confidence.</p>
  </li>
</ol>

<p>Prepare to witness the fusion of AI and robotics—a fusion that empowers you to take your models beyond the notebook and into the real world, where C++ is the language of choice. Let’s embark on this transformative journey together.</p>

<h2 id="1-intuition">1. Intuition</h2>
<p>In the journey of merging the powerful capabilities of TensorRT with the versatile landscape of C++, we embark on a comprehensive exploration of intuitive concepts that form the bedrock of seamless model inference. This section of the blog delves into four key intuitions, each unraveling a layer of understanding that enriches our grasp of the integration process. From traversing the path from a Torch model to C++ inference to uncovering the mechanics where TensorRT and C++ converge, and from unraveling the intricacies of memory operations to mapping RGB channels for perfect alignment—the intuitions explored herein shed light on the intricate dance between theory and application. These insights, rooted in both practical implementation and theoretical foundations, propel us toward mastering the harmonious symphony that is “TensorRT meets C++.”</p>

<h4 id="11-from-torch-model-to-c-inference">1.1. From Torch Model to C++ Inference</h4>

<p>In the realm of AI and machine learning, the journey from a PyTorch model within the cozy confines of a Jupyter notebook to real-world applications demands a bridge—a bridge that harmonizes the worlds of deep learning and robust, high-performance C++ execution. Enter TensorRT, the catalyst that transforms this transition from an ambitious leap to a smooth stride. Let us look at an overview of how we make this transition.</p>

<p>The diagram below captures the essence of this journey, illustrating how the torch model transforms into an optimized TensorRT engine and finds its home in the world of C++.</p>

<div style="width: 100%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_3/pt_cpp.drawio-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_3/pt_cpp.drawio-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_3/pt_cpp.drawio-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_3/pt_cpp.drawio.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
  Going from Pytorch model to Inference in C++
</div>
</div>

<p><br /></p>

<p><strong>1: The Torch-to-ONNX Conversion</strong></p>

<p>At the heart of this transformation lies the conversion of the PyTorch model into the Open Neural Network Exchange (<code class="language-plaintext highlighter-rouge">ONNX</code>) format. This universal format serves as the lingua franca between frameworks, unlocking the potential to bridge the gap between PyTorch and TensorRT. The Torch-to-ONNX conversion encapsulates the model’s architecture and parameters, setting the stage for seamless integration into the C++ landscape.</p>

<p><strong>2: The ONNX-to-TensorRT Metamorphosis</strong></p>

<p>With the ONNX representation in hand, in form of an <code class="language-plaintext highlighter-rouge">.onnx</code> file, we traverse the second leg of our journey—the ONNX-to-TensorRT transformation. Here, the ONNX model metamorphoses into a high-performance TensorRT engine, optimized to harness the prowess of modern GPUs. TensorRT’s meticulous optimization techniques prune the neural network, leveraging the inherent parallelism of GPUs for expedited inference without compromising accuracy. This can happen via the <code class="language-plaintext highlighter-rouge">trtexec</code> tool provided by TensorRT, via the Python API or through the C++ API. Having already covered the Python API implementation in the previous blog, we shall see the C++ API execution later in this blog.</p>

<p><strong>3: Unveiling C++ Inference</strong></p>

<p>And now, with the TensorRT engine prepared, in form of a <code class="language-plaintext highlighter-rouge">.engine</code> file, we navigate the final stretch of our voyage—the integration of the engine into C++ for inference. Armed with the TensorRT-powered engine, C++ becomes the stage where our models perform with astounding efficiency. Leveraging C++’s capabilities, we create a pipeline to read input data, offload it onto the GPU, execute inference, and retrieve the output—effortlessly spanning the distance between deep learning models and real-world applications.</p>

<h4 id="12-the-fusion-where-tensorrt-meets-c">1.2. The Fusion: Where TensorRT Meets C++</h4>

<p>In the intricate choreography between TensorRT and C++, a harmonious integration unfolds, paving the way for seamless model inference. As we transition from a TensorRT engine to the dynamic landscape of C++, a meticulous orchestration of steps ensures a blend of precision, speed, and efficiency that bridges the gap between these two powerful realms.</p>

<div style="width: 100%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_3/memory-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_3/memory-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_3/memory-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_3/memory.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
  Memory Transfers during inference.
</div>
</div>

<p><br /></p>

<p><strong>1. Setting the Stage: Input and Output Shapes</strong></p>

<p>Our journey begins by laying a foundation of understanding—the dimensions of the engine’s input and output tensors. These dimensions become the architectural blueprints for memory allocation and data manipulation. Armed with this knowledge, we traverse the intricate memory pathways that interconnect the CPU and GPU.</p>

<p><strong>2. Memory Allocation: The CPU-GPU Ballet</strong></p>

<p>With dimensions in hand, the stage shifts to memory allocation. The CPU takes center stage, employing <code class="language-plaintext highlighter-rouge">malloc()</code> to reserve space for the input and output tensors (<code class="language-plaintext highlighter-rouge">host_input</code> and <code class="language-plaintext highlighter-rouge">host_output</code>). Simultaneously, the GPU claims its spot, using <code class="language-plaintext highlighter-rouge">cudaMalloc()</code> to allocate memory for the tensors (<code class="language-plaintext highlighter-rouge">device_input</code> and <code class="language-plaintext highlighter-rouge">device_output</code>). This synchronization lays the groundwork for the fluid movement of data between the CPU and GPU.</p>

<p><strong>3. Orchestrating Data Flow: Copy and Serialize</strong></p>

<p>As the memory symphony unfolds, we shift our focus to the heart of data manipulation. The input—an image in this case—is transformed into a flattened array and stored in (<code class="language-plaintext highlighter-rouge">host_input</code>) that succinctly encapsulates the image data in a 1D structure. This array, a language understood by both the CPU and GPU, prepares for its leap onto the GPU memory stage.</p>

<p><strong>4. The Leap to GPU: Copy and Sync</strong></p>

<p>The synchronization is executed with precision. Our flattened array, now embodied as <code class="language-plaintext highlighter-rouge">host_input</code>, takes its leap from CPU memory to the GPU’s allocated memory (<code class="language-plaintext highlighter-rouge">device_input</code>). This transition is elegantly facilitated by <code class="language-plaintext highlighter-rouge">cudaMemcpy()</code>, as the image data makes its home within GPU memory, seamlessly bridging the chasm between CPU and GPU.</p>

<p><strong>5. The Grand Performance: Execution and Output</strong></p>

<p>The pinnacle of our journey arrives with the TensorRT engine poised on the stage. Equipped with the memory addresses of the input and output residing within the GPU (<code class="language-plaintext highlighter-rouge">device_input</code> and <code class="language-plaintext highlighter-rouge">device_output</code>), the engine’s inference function takes the command, orchestrating a high-speed performance where calculations unfurl on the parallel prowess of the GPU. The outcome—a meticulously calculated inference output—is etched onto the GPU’s at <code class="language-plaintext highlighter-rouge">device_output</code>.</p>

<p><strong>6. The Final Flourish: Retrieving the Output</strong></p>

<p>As the GPU’s performance concludes, it’s time to unveil the masterpiece—the inference output <code class="language-plaintext highlighter-rouge">device_output</code>. Guided by <code class="language-plaintext highlighter-rouge">cudaMemcpy()</code>, the <code class="language-plaintext highlighter-rouge">device_output</code> elegantly navigates the CPU-GPU bridge, returning to the CPU at <code class="language-plaintext highlighter-rouge">host_output</code>. Here, the versatile arms of C++ embrace it, ready to be presented to the world—bridging the divide between a PyTorch model and real-world applications.</p>

<p>This symphony, an interplay of dimensions, memory, and orchestration, encapsulates the essence of how TensorRT seamlessly converges with C++ for inference. As we explore each note of this symphony, we peel back the layers to reveal the intricate mechanics that underpin the fusion of “TensorRT meets C++.”</p>

<p><br /></p>

<h4 id="13-understanding-memory-operations-malloc-cudamalloc-and-cudamemcpy">1.3. Understanding Memory Operations: Malloc, cudaMalloc, and cudaMemcpy</h4>

<p>As we delve into the mechanics of TensorRT and C++ integration, let’s illuminate the roles of memory operations—<code class="language-plaintext highlighter-rouge">malloc</code>, <code class="language-plaintext highlighter-rouge">cudaMalloc</code>, and <code class="language-plaintext highlighter-rouge">cudaMemcpy</code>—through clear examples that illustrate their significance in data manipulation.</p>

<p><strong>1. <code class="language-plaintext highlighter-rouge">malloc</code>: CPU Memory Allocation</strong></p>

<p>Our journey begins with <code class="language-plaintext highlighter-rouge">malloc</code>, a venerable method for memory allocation in C++. This operation reserves memory space on the CPU, where data can be stored and manipulated. But here’s the catch—<code class="language-plaintext highlighter-rouge">malloc</code> operates in the realm of bytes. It expects the size of memory required in bytes. For instance, if we’re working with an array of integers, allocating space for 10 integers would involve requesting <code class="language-plaintext highlighter-rouge">10 * sizeof(int)</code> bytes. This allocated memory is crucial for accommodating data like input and output tensors (<code class="language-plaintext highlighter-rouge">host_input</code> and <code class="language-plaintext highlighter-rouge">host_output</code>) within the CPU’s memory space.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span><span class="o">*</span> <span class="n">host_input</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="mi">10</span> <span class="o">*</span> <span class="nf">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
</code></pre></div></div>
<p>Here, <code class="language-plaintext highlighter-rouge">host_input</code> becomes an array of 10 integers, ready to hold data in the CPU’s memory space.</p>

<p><strong>2. <code class="language-plaintext highlighter-rouge">cudaMalloc</code>: Alloacting GPU Memory</strong></p>

<p>On the GPU’s side of the stage, <code class="language-plaintext highlighter-rouge">cudaMalloc</code> steps in as the protagonist. But there’s a twist—<code class="language-plaintext highlighter-rouge">cudaMalloc</code> needs a pointer on the CPU that stores the address of the allocated memory space on the GPU. This introduces the concept of a pointer to a pointer, often referred to as a double-pointer. Along with the size, you pass the address of the double-pointer where <code class="language-plaintext highlighter-rouge">cudaMalloc</code> will store the GPU memory’s address. This synchronization between the CPU and GPU is pivotal, as it aligns the allocated memory on both sides, preparing for data flow orchestrated by the memory maestro, <code class="language-plaintext highlighter-rouge">cudaMemcpy</code>.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">float</span><span class="o">*</span> <span class="n">device_input</span><span class="p">;</span>
<span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">device_input</span><span class="p">,</span> <span class="mi">10</span> <span class="o">*</span> <span class="nf">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">device_input</code> becomes a pointer to GPU memory, reserved for 10 integers.</p>

<p><strong>3. <code class="language-plaintext highlighter-rouge">cudaMemcpy</code>: Bridging CPU and GPU</strong></p>

<p>As the memory symphony crescendos, <code class="language-plaintext highlighter-rouge">cudaMemcpy</code> takes the conductor’s baton. This operation orchestrates the harmonious movement of data between the CPU and GPU. With <code class="language-plaintext highlighter-rouge">cudaMemcpy</code>, data—like the serialized input image in our journey—travels seamlessly from the CPU’s memory (<code class="language-plaintext highlighter-rouge">host_input</code>) to the GPU’s realm (<code class="language-plaintext highlighter-rouge">device_input</code>). This synchronization ensures that the data exists in both places, primed for the GPU’s parallel calculations and the impending inference. Transferring an array of integers from CPU (<code class="language-plaintext highlighter-rouge">host_input</code>) to GPU (<code class="language-plaintext highlighter-rouge">device_input</code>):</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">device_input</span><span class="p">,</span> <span class="n">host_input</span><span class="p">,</span> <span class="mi">10</span> <span class="o">*</span> <span class="nf">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</code></pre></div></div>
<p>This synchronizes the data, making <code class="language-plaintext highlighter-rouge">device_input</code> on the GPU a reflection of <code class="language-plaintext highlighter-rouge">host_input</code> on the CPU.</p>

<p>With a deepened understanding of these memory operations, we unveil the intricacies of how memory becomes the thread that stitches together the CPU and GPU, forming the cohesive fabric that powers the integration of TensorRT and C++. The mechanics of <code class="language-plaintext highlighter-rouge">malloc</code>, <code class="language-plaintext highlighter-rouge">cudaMalloc</code>, and <code class="language-plaintext highlighter-rouge">cudaMemcpy</code> set the stage for the symphony of operations that unfolds in the TensorRT-powered C++ inference process.</p>

<p><br /></p>

<h4 id="14-mapping-rgb-channels-aligning-image-data-for-inference">1.4. Mapping RGB Channels: Aligning Image Data for Inference</h4>
<p>In the realm of integrating TensorRT with C++, the subtle yet pivotal process of aligning RGB channels when preparing an image for inference serves as an essential bridge between the image’s raw data and the expectations of the TensorRT engine. This alignment transforms the conventional RGB channel order (R1, G1, B1, R2, G2, B2, …) into a sequential arrangement that TensorRT comprehends (R1, R2, R3, G1, G2, G3, B1, B2, B3), ensuring that the spatial relationships within the image’s channels are preserved. The opencv image is stored in memory as <code class="language-plaintext highlighter-rouge">(H,W,C)</code> while most deep learning based torch models take input image as <code class="language-plaintext highlighter-rouge">(C,H,W)</code>. Hence, this reordering is needed. This reordering not only ensures the engine’s calculations are accurate but also reflects the way neural networks interpret and extract features from images. This seemingly subtle transformation thus becomes a critical step in bridging the gap between image data and the intricacies of deep learning-powered inference.</p>

<div style="width: 80%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_3/ocv_cpp-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_3/ocv_cpp-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_3/ocv_cpp-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_3/ocv_cpp.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
 Transforming input image from OpenCV to TensorRT to compensate for (C,H,W) to (H,C,W)
</div>
</div>

<p><br /></p>

<p>As we wrap up our exploration of these fundamental intuitions, we find ourselves equipped with a solid foundation that bridges the gap between theoretical understanding and practical implementation. From comprehending the journey from a Torch model to C++ inference, to diving into the seamless fusion of TensorRT and C++, understanding memory operations, and aligning image data for optimal inference—we have peeled back the layers that compose the intricate symphony of “TensorRT meets C++.”</p>

<h2 id="2-inside-the-code">2. Inside the Code</h2>

<p>In this section, we will take a comprehensive journey through the heart of our project, diving deep into the codebase and uncovering its intricacies. Our exploration will be structured into three phases: understanding the project’s folder structure, executing the project on your machine, and delving into an in-depth explanation of the crucial code components.</p>
<h3 id="21-folder-structure">2.1 Folder Structure</h3>

<p>Here, we unveil the architectural framework that underpins our C++ inference codebase, ensuring a structured and organized approach to integrating TensorRT into the C++ environment. Our code repository encompasses various directories and files, each with a distinct role in facilitating the intricate dance of “TensorRT meets C++.”</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>├── build
├── CMakeLists.txt
├── data
│   ├── hotdog.jpg
│   └── imagenet-classes.txt
├── deploy_tools
│   ├── resnet.engine
│   └── resnet.onnx
├── include
│   ├── inference.hpp
│   ├── postprocessor.hpp
│   └── preprocessor.hpp
├── main.cpp
├── README.md
├── src
│   ├── inference.cpp
│   ├── postprocess.cpp
│   └── preprocessor.cpp
├── tools
│   ├── environment.sh
│   ├── run.sh
│   └── torch_inference.py
└── torch_stuff
    ├── latency.txt
    └── output.txt
</code></pre></div></div>

<ul>
  <li><strong>src</strong>: Source code for inference, pre-processing, and post-processing.</li>
  <li><strong>include</strong>: Header files for communication between project components.</li>
  <li><strong>deploy_tools</strong>: Serialized TensorRT engine and original ONNX model.</li>
  <li><strong>data</strong>: Input data like images and class labels.</li>
  <li><strong>tools</strong>: Utilities for setup, execution, and PyTorch inference.</li>
  <li><strong>build</strong>: Build artifacts and configuration files.</li>
  <li><strong>CMakeLists.txt</strong>: Build configuration.</li>
  <li><strong>main.cpp</strong>: Entry point for the application.</li>
  <li><strong>README.md</strong>: Comprehensive guide.</li>
  <li><strong>torch_stuff</strong>: Pytorch Latency and Output</li>
</ul>

<h3 id="22-project-setup-and-execution">2.2 Project Setup and Execution</h3>

<p>To set up and run the project on your machine, follow these steps:</p>

<ol>
  <li>Open the <code class="language-plaintext highlighter-rouge">tools/environment.sh</code> script and adjust the paths for <code class="language-plaintext highlighter-rouge">TensorRT</code> and <code class="language-plaintext highlighter-rouge">CUDA</code> libraries as per your system configuration:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nb">export </span><span class="nv">TensorRT_Lib</span><span class="o">=</span>/path/to/TensorRT/lib
 <span class="nb">export </span><span class="nv">TensorRT_Inc</span><span class="o">=</span>/path/to/TensorRT/include
 <span class="nb">export </span><span class="nv">TensorRT_Bin</span><span class="o">=</span>/path/to/TensorRT/bin

 <span class="nb">export </span><span class="nv">CUDA_Lib</span><span class="o">=</span>/path/to/CUDA/lib64
 <span class="nb">export </span><span class="nv">CUDA_Inc</span><span class="o">=</span>/path/to/CUDA/include
 <span class="nb">export </span><span class="nv">CUDA_Bin</span><span class="o">=</span>/path/to/CUDA/bin
 <span class="nb">export </span><span class="nv">CUDA_HOME</span><span class="o">=</span>/path/to/CUDA

 <span class="nb">export </span><span class="nv">MODE</span><span class="o">=</span>inference

 <span class="nb">export </span><span class="nv">CONDA_ENV</span><span class="o">=</span>tensorrt
</code></pre></div>    </div>
    <p>Set the <code class="language-plaintext highlighter-rouge">MODE</code> to <code class="language-plaintext highlighter-rouge">build_engine</code> for building the TensorRT engine or make it <code class="language-plaintext highlighter-rouge">inference</code> for running inference on the sample image with the engine.</p>
  </li>
  <li>
    <p>Run the <code class="language-plaintext highlighter-rouge">tools/run.sh</code> script to execute the PyTorch inference and save its output:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> bash tools/run.sh
</code></pre></div>    </div>

    <p>Upon executing the above steps, you’ll observe an informative output similar to the one below, detailing both PyTorch and TensorRT C++ inference results:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> ===============================================================
 ||  MODE: inference
 ||  TensorRT: /path/to/TensorRT/lib
 ||  CUDA: /path/to/CUDA
 ===============================================================
 Configuration done!
 =================== STARTING PYTORCH INFERENCE===============================
 class: hotdog, hot dog, red hot, confidence: 60.50566864013672 %, index: 934
 Saved Pytorch output in torch_stuff/output.txt
 Average Latency for 10 iterations: 5.42 ms
 =============================================================================
 -- Configuring done
 -- Generating done
 -- Build files have been written to: /path/to/project/build
 =================== STARTING C++ TensorRT INFERENCE==========================
 class: hotdog, hot dog, red hot, confidence: 59.934%, index: 934
 Mean Absolute Difference in Pytorch and TensorRT C++: 0.0121075
 Average Latency for 10 iterations: 2.19824 ms
 =====================================SUMMARY=================================
 Pytorch Latency: 5.42 ms
 TensorRT in C++ Latency: 2.19824 ms
 Speedup by Quantization: 2.46561x
 Mean Absolute Difference in Pytorch and TensorRT C++: 0.0121075
 =============================================================================
</code></pre></div>    </div>

    <p>With this guide, you can effortlessly set up and run the project on your local machine, leveraging the power of TensorRT in C++ inference and comparing it with PyTorch’s results.</p>
  </li>
</ol>

<h3 id="23-code-deep-dive">2.3 Code Deep Dive</h3>
<p>In this section, we’ll take a deep dive into the core concepts of running inference with a TensorRT engine in C++. We’ll dissect the essential components of our project’s codebase and explore how they come together to enable efficient model inference. While we’ll primarily focus on the key functions and concepts, we’ll also provide an intuitive overview of the supporting functions to ensure a comprehensive understanding of the process.</p>

<h4 id="a-exploring-the-inference-class">A. Exploring the Inference Class:</h4>
<p>The heart of our project lies in the <code class="language-plaintext highlighter-rouge">Inference</code> class defined in <code class="language-plaintext highlighter-rouge">inference.hpp</code> and <code class="language-plaintext highlighter-rouge">inference.cpp</code>, which orchestrates the entire inference process. To understand how the magic happens, we’ll focus on the two pivotal functions: <code class="language-plaintext highlighter-rouge">initialize_inference</code> and <code class="language-plaintext highlighter-rouge">do_inference</code>. While we’ll provide a high-level overview of other functions for context, these two functions encapsulate the most critical aspects of model loading, memory management, and inference execution. Let’s break down how these functions work together to achieve accurate and speedy inference results.</p>

<p><strong>inference.hpp:</strong></p>

<ol>
  <li>
    <p><strong>Header Inclusions:</strong> The header includes necessary libraries, such as Nvidia TensorRT headers, OpenCV, CUDA runtime API, and others, for building and running the TensorRT inference engine.</p>
  </li>
  <li>
    <p><strong>Parameters Struct:</strong> The <code class="language-plaintext highlighter-rouge">Params</code> struct holds various parameters needed for configuring the inference process, such as file paths, engine attributes, and model parameters.</p>
  </li>
  <li>
    <p><strong>InferLogger Class:</strong> This class derives from <code class="language-plaintext highlighter-rouge">nvinfer1::ILogger</code> and is used to handle log messages generated during inference. It’s specialized to only print error messages.</p>
  </li>
  <li>
    <p><strong>Inference Class:</strong> This class encapsulates the entire inference process. It has member functions for building the engine, initializing inference, performing inference, and other helper functions for pre-processing, post-processing, and more.</p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">build()</code>: Constructs a TensorRT engine by parsing the ONNX model, creating the network, and serializing the engine to a file.</li>
      <li><code class="language-plaintext highlighter-rouge">buildFromSerializedEngine()</code>: Loads a pre-built serialized engine from a file, sets up the TensorRT runtime, and creates an execution context.</li>
      <li><code class="language-plaintext highlighter-rouge">initialize_inference()</code>: Allocates memory for input and output buffers on the GPU, prepares input and output bindings.</li>
      <li><code class="language-plaintext highlighter-rouge">do_inference()</code>: Reads an input image, preprocesses it, populates the input buffer, performs inference, and processes the output to get class predictions.</li>
    </ul>
  </li>
  <li>
    <p><strong>Helper Functions:</strong> Some inline helper functions are defined for convenience, such as <code class="language-plaintext highlighter-rouge">getElementSize</code> to determine the size of different data types.</p>
  </li>
</ol>

<p><strong>inference.cpp:</strong></p>

<ol>
  <li>
    <p><strong>constructNetwork():</strong> This function is responsible for constructing the TensorRT network by parsing the ONNX model. It configures builder, network, and parser based on user-defined parameters.</p>
  </li>
  <li>
    <p><strong>build():</strong> This function constructs the TensorRT engine by creating a builder, network, and parser, and then serializing the engine to a file.</p>
  </li>
  <li>
    <p><strong>buildFromSerializedEngine():</strong> This function loads a pre-built serialized engine from a file, sets up the runtime, and creates an execution context.</p>
  </li>
  <li>
    <p><strong>read_image():</strong> Reads an input image using OpenCV.</p>
  </li>
  <li>
    <p><strong>preprocess():</strong> Preprocesses the input image by resizing and normalizing it.</p>
  </li>
  <li>
    <p><strong>enqueue_input():</strong> Takes the preprocessed image and flattens the RGB channels into the input buffer in a specific order.</p>
  </li>
  <li>
    <p><strong>initialize_inference():</strong> Allocates GPU memory for input and output buffers, and sets up input and output bindings for the execution context.</p>
  </li>
  <li>
    <p><strong>do_inference():</strong> Reads an image, preprocesses it, enqueues input, performs inference, calculates latency, and processes the output predictions.</p>
  </li>
</ol>

<p>These files encapsulate the core functionality of loading an ONNX model, building a TensorRT engine, performing inference, and processing the results, with necessary pre-processing and post-processing steps. This structure enables you to easily integrate and run the inference process in a C++ environment. Now let us look at the two pivotal functions <code class="language-plaintext highlighter-rouge">initialize_inference()</code> and <code class="language-plaintext highlighter-rouge">do_inference()</code> in detail.</p>

<p><strong>initialize_inference():</strong></p>

<p>This function is responsible for setting up the necessary memory allocations and configurations required for running inference using the TensorRT engine. Let’s break down the code step by step:</p>

<ol>
  <li><strong>Input and Output Buffer Sizes:</strong>
    <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">input_idx</span> <span class="o">=</span> <span class="n">mEngine</span><span class="o">-&gt;</span><span class="n">getBindingIndex</span><span class="p">(</span><span class="s">"input"</span><span class="p">);</span>
<span class="k">auto</span> <span class="n">input_dims</span> <span class="o">=</span> <span class="n">mContext</span><span class="o">-&gt;</span><span class="n">getBindingDimensions</span><span class="p">(</span><span class="n">input_idx</span><span class="p">);</span>
<span class="n">nvinfer1</span><span class="o">::</span><span class="n">DataType</span> <span class="n">input_type</span> <span class="o">=</span> <span class="n">mEngine</span><span class="o">-&gt;</span><span class="n">getBindingDataType</span><span class="p">(</span><span class="n">input_idx</span><span class="p">);</span>
<span class="kt">size_t</span> <span class="n">input_vol</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">input_dims</span><span class="p">.</span><span class="n">nbDims</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">input_vol</span> <span class="o">*=</span> <span class="n">input_dims</span><span class="p">.</span><span class="n">d</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
<span class="n">input_size_in_bytes</span> <span class="o">=</span> <span class="n">input_vol</span> <span class="o">*</span> <span class="nf">getElementSize</span><span class="p">(</span><span class="n">input_type</span><span class="p">);</span>
</code></pre></div>    </div>
    <p>This block of code calculates the size of the input buffer based on the input dimensions and data type defined in the TensorRT engine.</p>
  </li>
  <li><strong>Memory Allocation for Input Buffer:</strong>
    <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">device_input</span><span class="p">,</span> <span class="n">input_size_in_bytes</span><span class="p">);</span>
<span class="n">host_input</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">input_size_in_bytes</span><span class="p">);</span>
<span class="n">bindings</span><span class="p">[</span><span class="n">input_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">device_input</span><span class="p">;</span>
</code></pre></div>    </div>
    <p>The input buffer is allocated on the GPU using <code class="language-plaintext highlighter-rouge">cudaMalloc</code>, and corresponding host memory is allocated using <code class="language-plaintext highlighter-rouge">malloc</code>. The <code class="language-plaintext highlighter-rouge">bindings</code> array is updated with the device input buffer.</p>
  </li>
  <li><strong>Output Buffer Setup:</strong>
Similar steps are performed for the output buffer:
    <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">output_idx</span> <span class="o">=</span> <span class="n">mEngine</span><span class="o">-&gt;</span><span class="n">getBindingIndex</span><span class="p">(</span><span class="s">"output"</span><span class="p">);</span>
<span class="k">auto</span> <span class="n">output_dims</span> <span class="o">=</span> <span class="n">mContext</span><span class="o">-&gt;</span><span class="n">getBindingDimensions</span><span class="p">(</span><span class="n">output_idx</span><span class="p">);</span>
<span class="n">nvinfer1</span><span class="o">::</span><span class="n">DataType</span> <span class="n">output_type</span> <span class="o">=</span> <span class="n">mEngine</span><span class="o">-&gt;</span><span class="n">getBindingDataType</span><span class="p">(</span><span class="n">output_idx</span><span class="p">);</span>
<span class="kt">size_t</span> <span class="n">output_vol</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">output_dims</span><span class="p">.</span><span class="n">nbDims</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">output_vol</span> <span class="o">*=</span> <span class="n">output_dims</span><span class="p">.</span><span class="n">d</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
<span class="n">output_size_in_bytes</span> <span class="o">=</span> <span class="n">output_vol</span> <span class="o">*</span> <span class="nf">getElementSize</span><span class="p">(</span><span class="n">output_type</span><span class="p">);</span>

<span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">device_output</span><span class="p">,</span> <span class="n">output_size_in_bytes</span><span class="p">);</span>
<span class="n">host_output</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">output_size_in_bytes</span><span class="p">);</span>
<span class="n">bindings</span><span class="p">[</span><span class="n">output_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">device_output</span><span class="p">;</span>
</code></pre></div>    </div>
    <p>The output buffer size is calculated, memory is allocated on the GPU and host, and the <code class="language-plaintext highlighter-rouge">bindings</code> array is updated.</p>
  </li>
</ol>

<p><strong>do_inference():</strong></p>

<p>This function performs the actual inference using the configured TensorRT engine. Let’s delve into the detailed explanation:</p>

<ol>
  <li><strong>Read and Preprocess Input Image:</strong>
    <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cv</span><span class="o">::</span><span class="n">Mat</span> <span class="n">img</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="n">mParams</span><span class="p">.</span><span class="n">ioPathsParams</span><span class="p">.</span><span class="n">image_path</span><span class="p">);</span>
<span class="n">cv</span><span class="o">::</span><span class="n">Mat</span> <span class="n">preprocessed_image</span><span class="p">;</span>
<span class="n">preprocess</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">preprocessed_image</span><span class="p">);</span>
</code></pre></div>    </div>
    <p>The input image is read using the <code class="language-plaintext highlighter-rouge">read_image</code> function, and then preprocessed using the <code class="language-plaintext highlighter-rouge">preprocess</code> function.</p>
  </li>
  <li><strong>Enqueue Input Data:</strong>
    <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">enqueue_input</span><span class="p">(</span><span class="n">host_input</span><span class="p">,</span> <span class="n">preprocessed_image</span><span class="p">);</span>
<span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">device_input</span><span class="p">,</span> <span class="n">host_input</span><span class="p">,</span> <span class="n">input_size_in_bytes</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</code></pre></div>    </div>
    <p>The preprocessed image data is enqueued into the input buffer using <code class="language-plaintext highlighter-rouge">enqueue_input</code>, and then the input data is copied from the host to the GPU using <code class="language-plaintext highlighter-rouge">cudaMemcpy</code>.</p>
  </li>
  <li><strong>Perform Inference:</strong>
    <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">bool</span> <span class="n">status_0</span> <span class="o">=</span> <span class="n">mContext</span><span class="o">-&gt;</span><span class="n">executeV2</span><span class="p">(</span><span class="n">bindings</span><span class="p">);</span>
</code></pre></div>    </div>
    <p>The inference is executed using the <code class="language-plaintext highlighter-rouge">executeV2</code> method of the execution context.</p>
  </li>
  <li><strong>Copy Output Data to Host:</strong>
    <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">host_output</span><span class="p">,</span> <span class="n">device_output</span><span class="p">,</span> <span class="n">output_size_in_bytes</span><span class="p">,</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
</code></pre></div>    </div>
    <p>The output data from the GPU is copied back to the host using <code class="language-plaintext highlighter-rouge">cudaMemcpy</code>.</p>
  </li>
  <li><strong>Calculate Latency:</strong>
    <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">auto</span> <span class="n">end_time</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>
<span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">duration</span><span class="o">&lt;</span><span class="kt">float</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">milli</span><span class="o">&gt;</span> <span class="n">duration</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">;</span>
<span class="n">latency</span> <span class="o">=</span> <span class="n">duration</span><span class="p">.</span><span class="n">count</span><span class="p">();</span>
</code></pre></div>    </div>
    <p>The execution time is calculated to determine the inference latency.</p>
  </li>
  <li><strong>Post-process Output and Classify:</strong>
    <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">float</span><span class="o">*</span> <span class="n">class_flattened</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">host_output</span><span class="p">);</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">predictions</span><span class="p">(</span><span class="n">class_flattened</span><span class="p">,</span> <span class="n">class_flattened</span> <span class="o">+</span> <span class="n">mParams</span><span class="p">.</span><span class="n">modelParams</span><span class="p">.</span><span class="n">num_classes</span><span class="p">);</span>
<span class="n">mPostprocess</span><span class="p">.</span><span class="n">softmax_classify</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">verbose</span><span class="p">);</span>
</code></pre></div>    </div>
    <p>The output data is processed to calculate class predictions using softmax, and the <code class="language-plaintext highlighter-rouge">softmax_classify</code> function is called from the <code class="language-plaintext highlighter-rouge">mPostprocess</code> object.</p>
  </li>
</ol>

<h4 id="b-exploring-the-preprocess-and-postprocess-class">B. Exploring the Preprocess and Postprocess Class</h4>
<p>In this section, we’ll dive into the preprocessor and postprocessor classes. These classes are vital for preparing input data and interpreting model outputs. We’ll see how the preprocessor class resizes and normalizes images, while the postprocessor class calculates softmax probabilities and identifies top predicted classes. Understanding these components sheds light on the core data transformations and result analysis in your inference workflow.</p>

<p><strong>Preprocessor Class:</strong></p>

<p>The <code class="language-plaintext highlighter-rouge">preprocessor</code> class handles image preprocessing tasks before feeding them into the neural network for inference. It consists of two main functions: <code class="language-plaintext highlighter-rouge">resize</code> and <code class="language-plaintext highlighter-rouge">normalization</code>.</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">resize</code>:
This function takes an input image and resizes it to a predefined size using OpenCV’s <code class="language-plaintext highlighter-rouge">cv::resize</code> function. The resized image is stored in the <code class="language-plaintext highlighter-rouge">output_image</code> parameter. This step ensures that the input image has consistent dimensions that match the requirements of the neural network.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">normalization</code>:
The normalization function standardizes the pixel values of the resized image to be suitable for the neural network. It performs the following steps:</p>
    <ol>
      <li>Converts the input image to a floating-point representation and scales pixel values to the range [0, 1].</li>
      <li>Subtracts mean values from each channel (RGB) of the image.</li>
      <li>Divides the subtracted image by the standard deviation values.
The normalized image is stored in the <code class="language-plaintext highlighter-rouge">output_image</code> parameter. These preprocessing steps help ensure that the neural network receives inputs in a consistent format.</li>
    </ol>
  </li>
</ul>

<p><strong>Postprocessor Class:</strong></p>

<p>Now let’s move on to the <code class="language-plaintext highlighter-rouge">postprocessor</code> class, which handles processing the model outputs after inference.</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Postprocessor</code> Constructor:
The constructor of the <code class="language-plaintext highlighter-rouge">postprocessor</code> class initializes an instance of the class and accepts a path to a file containing class names. This file is used to map class indices to their human-readable names.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">softmax_classify</code>:
The <code class="language-plaintext highlighter-rouge">softmax_classify</code> function performs post-processing on the model’s output probabilities. It calculates softmax probabilities from the raw output values and prints the top predicted classes along with their confidences. Here’s a breakdown of the steps:</p>
    <ol>
      <li>The function reads class names from the provided class file and stores them in the <code class="language-plaintext highlighter-rouge">classes</code> vector.</li>
      <li>The softmax probabilities are calculated from the raw output values using the exponential function and normalized.</li>
      <li>The function sorts the predicted classes based on their confidences.</li>
      <li>If the <code class="language-plaintext highlighter-rouge">verbose</code> flag is enabled, the top predicted classes with confidences greater than 50% are printed to the console.</li>
    </ol>
  </li>
</ul>

<p>Overall, the <code class="language-plaintext highlighter-rouge">postprocessor</code> class helps interpret the model’s output probabilities and provides human-readable class predictions along with confidence values.</p>

<h4 id="c-exploring-the-main-code-maincpp">C. Exploring the Main Code: main.cpp</h4>

<p>The <code class="language-plaintext highlighter-rouge">main.cpp</code> file is the heart of our inference application, where we orchestrate the entire process of building and using the TensorRT engine for inference. Let’s break down the key parts of this code to understand how it works.</p>

<ol>
  <li>
    <p><strong>Command-line Arguments:</strong> The program accepts command-line arguments to determine its behavior. If no arguments are provided or if <code class="language-plaintext highlighter-rouge">--help</code> is used, a help message is displayed.</p>
  </li>
  <li>
    <p><strong>Initializing Inference:</strong> The <code class="language-plaintext highlighter-rouge">Params</code> struct is used to configure various parameters for the inference process. We create an instance of the <code class="language-plaintext highlighter-rouge">Inference</code> class, passing these parameters.</p>
  </li>
  <li>
    <p><strong>Command-line Argument Processing:</strong> We iterate through the provided command-line arguments. If <code class="language-plaintext highlighter-rouge">--build_engine</code> is passed, the TensorRT engine is built using the ONNX model. If <code class="language-plaintext highlighter-rouge">--inference</code> is passed, the built engine is used for inference.</p>
  </li>
  <li><strong>Performing Inference:</strong> When <code class="language-plaintext highlighter-rouge">--inference</code> is specified, the following steps occur:
    <ul>
      <li>We build the TensorRT engine from the serialized engine file.</li>
      <li>We initialize the inference context and memory buffers.</li>
      <li>We perform inference using the <code class="language-plaintext highlighter-rouge">do_inference()</code> method from the <code class="language-plaintext highlighter-rouge">Inference</code> class.</li>
      <li>We read the Python-generated output from a file and calculate the mean absolute difference between the Python output and C++ TensorRT output.</li>
    </ul>
  </li>
  <li>
    <p><strong>Measuring Latency and Speedup:</strong> We measure the average latency of the TensorRT inference over multiple iterations. We also read the Python-generated latency from a file. By comparing the latencies, we calculate the speedup achieved by the TensorRT inference.</p>
  </li>
  <li><strong>Displaying Summary:</strong> Finally, we display a summary of the results, including the PyTorch latency, TensorRT latency, speedup, and mean absolute difference in predictions.</li>
</ol>

<p>This <code class="language-plaintext highlighter-rouge">main.cpp</code> file demonstrates how we can effectively build, deploy, and analyze the performance and accuracy of our TensorRT-based inference system.</p>

<h4 id="d-building-the-project-with-cmake-cmakeliststxt">D. Building the Project with CMake: CMakeLists.txt</h4>

<p>The <code class="language-plaintext highlighter-rouge">CMakeLists.txt</code> file is crucial for configuring the build process of your project. It defines how to compile the code and link various libraries. Let’s dive into the key components of this CMake configuration.</p>

<ol>
  <li>
    <p><strong>Minimum Required Version:</strong> Specify the minimum required CMake version for your project.</p>
  </li>
  <li>
    <p><strong>Project Configuration:</strong> Set the project name and C++ standard version.</p>
  </li>
  <li>
    <p><strong>Setting RPATH:</strong> Configure the runtime path for your executable using <code class="language-plaintext highlighter-rouge">$ORIGIN</code>.</p>
  </li>
  <li>
    <p><strong>Include Directories:</strong> Define the include directories for your project, including OpenCV and CUDA.</p>
  </li>
  <li>
    <p><strong>Find OpenCV:</strong> Use the <code class="language-plaintext highlighter-rouge">find_package</code> command to locate and configure OpenCV.</p>
  </li>
  <li>
    <p><strong>Find CUDA:</strong> Use the <code class="language-plaintext highlighter-rouge">find_package</code> command to locate and configure CUDA.</p>
  </li>
  <li>
    <p><strong>Enable CUDA Support:</strong> Set the CUDA compiler and flags to enable CUDA support.</p>
  </li>
  <li>
    <p><strong>Linking Preprocessor and Postprocessor Libraries:</strong> Build and link the <code class="language-plaintext highlighter-rouge">pre_processor</code> and <code class="language-plaintext highlighter-rouge">post_processor</code> libraries. These libraries contain the preprocessing and postprocessing logic.</p>
  </li>
  <li>
    <p><strong>Building the Inference Library:</strong> Create the <code class="language-plaintext highlighter-rouge">Inference</code> shared library by compiling and linking the <code class="language-plaintext highlighter-rouge">inference.cpp</code> file. This library is used to perform inference using TensorRT.</p>
  </li>
  <li>
    <p><strong>Linking Dependencies:</strong> Link the <code class="language-plaintext highlighter-rouge">Inference</code> library with the <code class="language-plaintext highlighter-rouge">pre_processor</code>, <code class="language-plaintext highlighter-rouge">post_processor</code>, and necessary libraries such as <code class="language-plaintext highlighter-rouge">nvinfer</code>, <code class="language-plaintext highlighter-rouge">nvonnxparser</code>, <code class="language-plaintext highlighter-rouge">stdc++fs</code>, CUDA libraries, and OpenCV.</p>
  </li>
  <li>
    <p><strong>Creating the Executable:</strong> Build the <code class="language-plaintext highlighter-rouge">main</code> executable, which serves as the entry point for your application. Link it with the <code class="language-plaintext highlighter-rouge">Inference</code> library.</p>
  </li>
</ol>

<p>This <code class="language-plaintext highlighter-rouge">CMakeLists.txt</code> file defines how your project will be built, linking various libraries and ensuring proper configuration for successful compilation and execution.</p>

<h2 id="3-latency-and-consistency">3. Latency and Consistency</h2>

<p>In this section, we will delve into the results obtained from our TensorRT-powered inference and compare them with PyTorch. The performance improvements achieved through TensorRT are significant, showcasing both speedup and consistency in inference latency.</p>

<h3 id="31-speedup-by-quantization">3.1 Speedup by Quantization</h3>

<p>Quantization is a technique that reduces the memory and computational requirements of neural network models. When comparing PyTorch’s latency to that of our C++ TensorRT-powered model, we observe a remarkable speedup. PyTorch exhibits a latency of 5.42 ms, while the C++ TensorRT combination achieves an impressive latency of 2.19824 ms. This translates to a speedup of approximately <code class="language-plaintext highlighter-rouge">2.46561 times.</code></p>

<h3 id="32-minimal-output-difference">3.2 Minimal Output Difference</h3>

<p>Although the inference latencies have changed, it’s crucial to ensure that the actual outputs remain consistent between the original PyTorch model and the C++ TensorRT implementation. Fortunately, our analysis reveals that the Mean Absolute Difference between the two is merely <code class="language-plaintext highlighter-rouge">0.0121075</code>. This minor variation in output demonstrates the reliability and accuracy of the TensorRT-powered inference.</p>

<p>The combined benefits of reduced latency and minimal output differences make the integration of TensorRT into the project a powerful optimization, ensuring efficient and consistent real-time inferencing for various applications.</p>

<h2 id="4-conclusion">4. Conclusion</h2>

<p>In this project, we embarked on a journey to optimize neural network inference using TensorRT, a high-performance deep learning inference optimizer and runtime library. By integrating TensorRT into our C++ application, we achieved remarkable improvements in inference speed and consistency, enhancing the overall efficiency of our model.</p>

<p>Throughout the exploration, we dissected the intricacies of the TensorRT engine, delving into core concepts such as building, initializing, and executing the engine for both PyTorch and C++ implementations. We gained insights into preprocessing and postprocessing techniques to ensure accurate input and output handling. Our journey was enriched by understanding the integration of CUDA and OpenCV libraries, which are essential for seamless GPU acceleration and image processing.</p>

<p>By combining the power of TensorRT, CUDA, and C++, we witnessed a substantial reduction in inference latency. The speedup achieved—showcased through a quantified speedup factor—highlighted the importance of optimizing deep learning models for real-time applications. Additionally, our evaluation revealed a minimal Mean Absolute Difference between the outputs of PyTorch and the C++ TensorRT implementation, affirming the reliability of our optimization strategy.</p>

<p>In conclusion, our project underscores the significance of leveraging TensorRT in tandem with C++ for neural network inference. This integration paves the way for enhanced performance, making it a pivotal solution for real-time applications across various domains. Through this exploration, we’ve gained valuable insights into the world of deep learning optimization, setting the stage for further innovations in the field.</p>]]></content><author><name></name></author><category term="edge-ai" /><category term="nvidia" /><category term="tensorrt" /><category term="deep-learning" /><summary type="html"><![CDATA[TensorRT inference in C++]]></summary></entry><entry><title type="html">CUDA it Be Any Faster?</title><link href="https://sanket-pixel.github.io//blog/2023/cuda-it-be-any-faster/" rel="alternate" type="text/html" title="CUDA it Be Any Faster?" /><published>2023-08-05T19:53:00+00:00</published><updated>2023-08-05T19:53:00+00:00</updated><id>https://sanket-pixel.github.io//blog/2023/cuda-it-be-any-faster</id><content type="html" xml:base="https://sanket-pixel.github.io//blog/2023/cuda-it-be-any-faster/"><![CDATA[<h4 id="experience-the-exhilarating-world-of-nvidias-cuda-programming-as-we-revolutionize-point-cloud-processing-in-computer-vision-and-robotics-voxelization-the-process-of-converting-3d-points-into-discrete-voxels-has-faced-challenges-with-traditional-cpu-based-methods-limiting-groundbreaking-innovations-but-fear-not-as-we-harness-the-immense-power-of-parallelization-for-a-monumental-leap-of-over-580x-times-over-traditional-cpu-dive-into-cudas-awe-inspiring-realm-where-each-point-gets-its-own-thread-enabling-lightning-fast-voxelization-and-opening-the-doors-to-real-time-applications-join-us-on-this-thrilling-ride-and-witness-the-magic-of-cuda-as-we-rewrite-the-future-of-point-cloud-processing-lets-embrace-the-sheer-power-of-cuda-together-and-change-the-game">Experience the exhilarating world of NVIDIA’s CUDA programming as we revolutionize point cloud processing in computer vision and robotics. Voxelization, the process of converting 3D points into discrete voxels, has faced challenges with traditional CPU-based methods, limiting groundbreaking innovations. But fear not, as we harness the immense power of parallelization for a monumental leap of over 580x times over traditional CPU! Dive into CUDA’s awe-inspiring realm, where each point gets its own thread, enabling lightning-fast voxelization and opening the doors to real-time applications. Join us on this thrilling ride and witness the magic of CUDA as we rewrite the future of point cloud processing. Let’s embrace the sheer power of CUDA together and change the game!</h4>

<p><br /></p>

<div style="width: 100%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_2/voxelize.gif-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_2/voxelize.gif-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_2/voxelize.gif-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_2/voxelize.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
   Input Point Cloud on the Left and Voxelized Point Cloud using CUDA programming on right. 
</div>
</div>

<h2 id="why-should-you-read-this-blog">Why Should You Read This Blog?</h2>

<p>Buckle up as we’re about to embark on an exhilarating journey into the world of NVIDIA GPU programming and CUDA wizardry. So, grab your favorite caffeinated beverage and prepare for some adrenaline-pumping excitement!</p>

<p>Now, you might be wondering, “Why should I read this blog? Is it going to be another dull and dreary technical piece?” Well, fear not! This blog is anything but ordinary. We’re here to show you the mind-blowing power of CUDA programming on NVIDIA GPUs, and we’ve got a jaw-dropping example to demonstrate just that: voxelization!</p>

<p>Okay, you might be thinking, “Voxelization? What in the world is that? Sounds like a made-up word from a sci-fi movie!” Well, in a way, it kind of is! Voxelization is the magical process of taking a ginormous LiDAR point cloud and breaking it down into tiny cubic chunks called voxels. Think of it as pixelating a 3D world, but on a whole new level!</p>

<p>Now, hold on to your seat because here comes the fun part: we’re going to do all this voxelization stuff using the mighty NVIDIA GPUs and CUDA. That’s right, the same GPUs used in gaming rigs to unleash stunning graphics and epic frame rates are going to help us crunch numbers like there’s no tomorrow.  Together, we’ll venture into the depths of parallel programming, where our algorithms will run at supersonic speeds, leaving ordinary CPUs gasping for breath. Instead of using the age old for-loops to process every point one after another, we will process these points, all at once, by using a seperate thread for every point in the point cloud. This way, we make every thread count, squeezing out every ounce of performance, that the NVIDIA GPU has to offer. These concepts of parallelization that we will use for point-cloud processing can be seamlessly used for any other realm where parallelization is an option.</p>

<p>But beware! As we venture further into the realm of CUDA programming, things might get a bit hairy. New concepts and parallel programming lingo might make you scratch your head in confusion. But hey, don’t give up just yet! Remember, the path to greatness is often paved with challenges. And in this case, the reward at the end of the tunnel is an unreal, mind-boggling, jaw-dropping <code class="language-plaintext highlighter-rouge">580x speedup</code>!</p>

<p>Yes, you heard that right! By the time we’re done here, you’ll have a GPU-based voxelization algorithm that can chew through mountains of LiDAR data at an incredible speed, leaving your CPU-bound counterparts in the dust. It’s like having a supersonic jet compared to a horse-drawn carriage!</p>

<p>So, stick with us, and we promise it’ll be worth it. Sure, there might be moments when you feel like you’re lost in a maze of CUDA syntax or buried under an avalanche of parallel processing concepts. But fear not, brave adventurer! We’re here to guide you, step by step, through the intricacies of GPU programming.</p>

<p>So, are you ready for the ride of a lifetime? Strap on your GPU-powered jetpack, and let’s dive into the mind-bending universe of CUDA programming and voxelization. Together, we’ll unlock the secrets of parallel processing and witness the awe-inspiring 580x speedup that will leave you marveling at the wonders of modern technology!</p>

<p><br /></p>

<h2 id="1-what-is-voxelization">1. What is Voxelization?</h2>

<p>Voxelization is a fundamental process in computer graphics and 3D data analysis that involves converting continuous 3D space into a discrete representation using small cubic units known as “voxels.” The term <code class="language-plaintext highlighter-rouge">voxel</code> is a combination of <code class="language-plaintext highlighter-rouge">volume</code> and <code class="language-plaintext highlighter-rouge">pixel</code>, and it serves as the 3D equivalent of a 2D pixel.</p>

<p>Imagine we have a 3D object or scene that we want to represent digitally. Voxelization begins by enclosing this continuous 3D space within an imaginary 3D grid. This grid subdivides the entire space into a series of equally-sized cubic voxels, and each cell is a voxel. Similar to pixels in a 2D image, each voxel corresponds to a specific region within the 3D space.</p>

<p>The next step is to analyze the contents of each voxel and determine whether it is occupied or empty. This process is often referred to as <code class="language-plaintext highlighter-rouge">filling</code> the voxels. To fill the voxels, we examine the objects or entities within the 3D space and determine which voxels they intersect or occupy.</p>

<p>For example, consider a 3D point cloud obtained from a LiDAR sensor. Each point in the point cloud represents a 3D coordinate in space. During voxelization, the points falling within a voxel’s boundaries are considered as <code class="language-plaintext highlighter-rouge">occupied</code> while the voxels without any points are <code class="language-plaintext highlighter-rouge">empty</code>.</p>

<p>By analyzing the presence or absence of objects in each voxel, we create a binary representation of the 3D space. The voxels that are occupied are marked with a value of 1, while the empty voxels are assigned a value of 0. This binary representation forms a digital 3D model, often referred to as a <code class="language-plaintext highlighter-rouge">voxel grid</code> or <code class="language-plaintext highlighter-rouge">voxel map</code>.</p>

<p>In the context of this blog, <code class="language-plaintext highlighter-rouge">voxelization</code> refers to the initial step of assigning individual 3D points from a LiDAR point cloud to discrete voxels within a 3D grid. Each voxel represents a small cubic region in the 3D space, and the process of voxelization categorizes points based on their spatial location. Points falling within a voxel’s boundaries are considered “occupied,” while voxels without any points are labeled as “empty.”</p>

<p>After the voxelization step, the blog proceeds to <code class="language-plaintext highlighter-rouge">feature extraction</code>. In this step, we compute the average of certain features for all the points that belong to each voxel. The features of interest in our case are the x, y, z coordinates, and the intensity of the LiDAR points. By averaging these features for all the points within a voxel, we obtain a condensed representation of the original point cloud data.</p>

<p>The voxelization and feature extraction steps are essential in processing large-scale LiDAR point cloud data efficiently and effectively. They provide several key benefits:</p>

<p><strong>Data Summarization</strong>: Voxelization enables us to partition the complex and continuous point cloud data into smaller, discrete units, represented by voxels. This categorization allows us to summarize the content of the entire point cloud and simplifies subsequent analysis.</p>

<p><strong>Reduced Computational Complexity</strong>: The process of assigning points to voxels significantly reduces the computational burden by processing points at a voxel level rather than individually. This reduction in complexity is crucial when dealing with massive point cloud datasets, as it enables faster processing and analysis.</p>

<p><strong>Compact Representation</strong>: Feature extraction by averaging the x, y, z coordinates, and intensity within each voxel results in a compact representation of the original point cloud. Instead of dealing with millions of individual points, we obtain summarized information for each voxel, making it more manageable for downstream tasks.</p>

<p><strong>Efficient Task-Specific Analysis</strong>: The condensed representation obtained through feature extraction provides valuable insights for various 3D tasks such as object detection, moving object segmentation, velocity estimation, and more. By processing voxel-level information, these tasks become computationally tractable and more efficient.</p>

<h2 id="2-primer-on-cuda-programming">2. Primer on CUDA Programming</h2>

<p>Before we embark on the exhilarating journey of GPU-powered voxelization, let’s take a moment to familiarize ourselves with the essence of CUDA programming. If you’re new to this realm, fear not, for we shall unravel the key concepts step by step, and soon, you’ll be navigating through the seas of parallel processing with confidence.</p>

<p><em>Understanding CUDA: The Power of Parallelism</em></p>

<p>At its core, CUDA is NVIDIA’s powerful parallel computing platform and programming model. It harnesses the raw computational might of NVIDIA GPUs to tackle intricate problems at breathtaking speeds. Imagine orchestrating a symphony where hundreds or thousands of independent operations perform in harmony, each contributing to the grand performance.</p>

<p><em>The Building Blocks of CUDA: Threads, Blocks, and Grids</em></p>

<p>CUDA operates on the principles of threads, blocks, and grids. Threads are the elemental units of computation, executing independently on the GPU. They group together into blocks, and these blocks, in turn, form a grid. This orchestrated arrangement of threads, blocks, and grids conducts the seamless symphony of parallelism.</p>

<p><em>A Simple CUDA Example: Adding Numbers in Parallel</em></p>

<p>Let’s commence our CUDA journey with a classic example: adding numbers in parallel. This seemingly simple task is a profound introduction to the world of parallel processing.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span>
<span class="c1">// CUDA kernel to add two arrays in parallel</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">addNumbers</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">b</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">c</span><span class="p">,</span> <span class="kt">int</span> <span class="n">numElements</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">&lt;</span> <span class="n">numElements</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">c</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">tid</span><span class="p">];</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
    <span class="c1">// Array size and memory allocation</span>
    <span class="kt">int</span> <span class="n">numElements</span> <span class="o">=</span> <span class="mi">1000000</span><span class="p">;</span>
    <span class="kt">size_t</span> <span class="n">size</span> <span class="o">=</span> <span class="n">numElements</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">h_a</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">size</span><span class="p">);</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">h_b</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">size</span><span class="p">);</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">h_c</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">size</span><span class="p">);</span>

    <span class="c1">// Initialize arrays with some values</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">numElements</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">h_a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
        <span class="n">h_b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// Allocate GPU memory</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">d_a</span> <span class="o">=</span> <span class="nb">nullptr</span><span class="p">;</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">d_b</span> <span class="o">=</span> <span class="nb">nullptr</span><span class="p">;</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">d_c</span> <span class="o">=</span> <span class="nb">nullptr</span><span class="p">;</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_a</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_b</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_c</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>

    <span class="c1">// Copy input arrays from host to GPU memory</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_a</span><span class="p">,</span> <span class="n">h_a</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_b</span><span class="p">,</span> <span class="n">h_b</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

    <span class="c1">// Launch the CUDA kernel with 256 threads per block</span>
    <span class="kt">int</span> <span class="n">blockSize</span> <span class="o">=</span> <span class="mi">256</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">numBlocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">numElements</span> <span class="o">+</span> <span class="n">blockSize</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">blockSize</span><span class="p">;</span>
    <span class="n">addNumbers</span><span class="o">&lt;&lt;&lt;</span><span class="n">numBlocks</span><span class="p">,</span> <span class="n">blockSize</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_a</span><span class="p">,</span> <span class="n">d_b</span><span class="p">,</span> <span class="n">d_c</span><span class="p">,</span> <span class="n">numElements</span><span class="p">);</span>

    <span class="c1">// Copy the result from GPU memory back to host</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">h_c</span><span class="p">,</span> <span class="n">d_c</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

    <span class="c1">// Free GPU memory and host arrays</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_a</span><span class="p">);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_b</span><span class="p">);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_c</span><span class="p">);</span>
    <span class="n">free</span><span class="p">(</span><span class="n">h_a</span><span class="p">);</span>
    <span class="n">free</span><span class="p">(</span><span class="n">h_b</span><span class="p">);</span>
    <span class="n">free</span><span class="p">(</span><span class="n">h_c</span><span class="p">);</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p><strong>Code Explanation:</strong></p>

<ol>
  <li>
    <p><code class="language-plaintext highlighter-rouge">__global__</code> Function: The <code class="language-plaintext highlighter-rouge">addNumbers</code> function is marked as a <code class="language-plaintext highlighter-rouge">__global__</code> function. This indicates that it’s a GPU kernel that will be executed on the GPU.</p>
  </li>
  <li>
    <p>Kernel Launch: The kernel is launched with the <code class="language-plaintext highlighter-rouge">&lt;&lt;&lt;numBlocks, blockSize&gt;&gt;&gt;</code> syntax, specifying the number of blocks (<code class="language-plaintext highlighter-rouge">numBlocks</code>) and the number of threads per block (<code class="language-plaintext highlighter-rouge">blockSize</code>). Each block contains multiple threads, and the threads execute the kernel function in parallel.</p>
  </li>
  <li>
    <p>Thread Indexing: The <code class="language-plaintext highlighter-rouge">tid</code> (thread ID) is calculated using the <code class="language-plaintext highlighter-rouge">blockDim.x</code>, <code class="language-plaintext highlighter-rouge">blockIdx.x</code>, and <code class="language-plaintext highlighter-rouge">threadIdx.x</code> built-in variables. Each thread knows its global ID, allowing it to access the corresponding elements in arrays <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code>, and <code class="language-plaintext highlighter-rouge">c</code>.</p>
  </li>
  <li>
    <p>Array Initialization: Three arrays <code class="language-plaintext highlighter-rouge">h_a</code>, <code class="language-plaintext highlighter-rouge">h_b</code>, and <code class="language-plaintext highlighter-rouge">h_c</code> are allocated in host memory (<code class="language-plaintext highlighter-rouge">malloc</code>) and initialized with some values. These arrays represent the input arrays <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code>, and the output array <code class="language-plaintext highlighter-rouge">c</code>.</p>
  </li>
  <li>
    <p>Memory Allocation and Data Transfer: Memory is allocated on the GPU using <code class="language-plaintext highlighter-rouge">cudaMalloc</code>, and the input arrays <code class="language-plaintext highlighter-rouge">h_a</code> and <code class="language-plaintext highlighter-rouge">h_b</code> are copied to the GPU memory using <code class="language-plaintext highlighter-rouge">cudaMemcpy</code>.</p>
  </li>
  <li>
    <p>Kernel Execution: The <code class="language-plaintext highlighter-rouge">addNumbers</code> kernel is launched with the specified number of blocks and threads per block. Each thread computes the sum of the corresponding elements of <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code>, storing the result in array <code class="language-plaintext highlighter-rouge">c</code> on the GPU.</p>
  </li>
  <li>
    <p>Result Retrieval: The result is copied back from the GPU memory to the host array <code class="language-plaintext highlighter-rouge">h_c</code> using <code class="language-plaintext highlighter-rouge">cudaMemcpy</code>.</p>
  </li>
  <li>
    <p>Memory Deallocation: GPU memory is freed using <code class="language-plaintext highlighter-rouge">cudaFree</code>, and host arrays are freed using <code class="language-plaintext highlighter-rouge">free</code>.</p>
  </li>
</ol>

<p><strong>Compiling the CUDA Code with CMake</strong></p>

<p>To compile the CUDA code, we can use CMake, a popular build system that supports CUDA projects. Below is a minimal CMakeLists.txt file to build the CUDA example:</p>

<div class="language-cmake highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cmake_minimum_required</span><span class="p">(</span>VERSION 3.10<span class="p">)</span>
<span class="nb">project</span><span class="p">(</span>CUDAExample<span class="p">)</span>

<span class="c1"># Find CUDA</span>
<span class="nb">find_package</span><span class="p">(</span>CUDA REQUIRED<span class="p">)</span>

<span class="c1"># Set C++ version</span>
<span class="nb">set</span><span class="p">(</span>CMAKE_CXX_STANDARD 14<span class="p">)</span>

<span class="c1"># Include CUDA headers</span>
<span class="nb">include_directories</span><span class="p">(</span><span class="si">${</span><span class="nv">CUDA_INCLUDE_DIRS</span><span class="si">}</span><span class="p">)</span>

<span class="c1"># Add the CUDA source file and executable</span>
<span class="nf">cuda_add_executable</span><span class="p">(</span>cuda_example main.cu<span class="p">)</span>
</code></pre></div></div>

<p><strong>Executing the CUDA Code</strong></p>

<p>Once the CMakeLists.txt file is prepared, follow these steps to execute the CUDA example:</p>

<ol>
  <li>Create a directory (e.g., build) to build the project:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nb">mkdir </span>build
  <span class="nb">cd </span>build
</code></pre></div>    </div>
  </li>
  <li>Generate the Makefile using CMake:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  cmake ..
</code></pre></div>    </div>
  </li>
  <li>Build the project:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  make
</code></pre></div>    </div>
  </li>
  <li>Run the CUDA executable:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./cuda_example
</code></pre></div>    </div>
  </li>
</ol>

<p>And with that, we cover the basics of CUDA ,and  we are now ready to witness how these concepts are harnessed to implement highly efficient voxelization using the parallel processing capabilities of GPUs. Before we embark on this exciting journey into the world of GPU-accelerated voxelization, let’s delve deeper into the intuition and mechanics of this process. Get ready to experience the wonders of voxelization and the incredible performance boost it brings!</p>

<p><br /></p>

<h2 id="3-intuition">3. Intuition</h2>
<p>In this section, we will embark on an intuitive exploration of the point cloud voxelization process using CUDA. To that end, let’s first set the stage by taking a look at a simple 2D grid. We’ll create a 3x3 grid and randomly select 14 points with most points within its boundaries. Some points will also lie slightly outside the grid to make the example more interesting. The voxels here are analogous to voxels. Visualizing this grid and its sample points, we get the following plot:</p>

<div style="width: 60%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_2/points-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_2/points-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_2/points-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_2/points.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
   Example 2D grid with 14 points. The voxels are analogous to voxels.
</div>
</div>

<p>Now, let us understand one last detail before we begin the actual processing. In the figure shown below, on the left, we have a 2D grid representing the voxels with their corresponding indices ranging from (0, 0) to (2, 2). Each voxel in the grid is identified by its x and y coordinates, starting from the bottom-left corner and progressing towards the top-right corner. However, from now on, we will refer to this serialized integer index as <code class="language-plaintext highlighter-rouge">voxel_offset</code>,  which uniquely represents each voxel in a sequential order from 0 to 8, as shown on the right side of the figure.</p>
<div style="width: 70%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_2/serialize-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_2/serialize-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_2/serialize-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_2/serialize.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
   Left : 2D indices of the voxels (i,j), the x and ycoordinate respectively. Right : Flattened out 1D indices.
</div> 
</div>
<p>This <code class="language-plaintext highlighter-rouge">voxel_offset</code> plays a crucial role in the upcoming CUDA-based voxelization process, allowing us to efficiently access and process voxel data in a linear manner. By representing the grid in this serialized format, we can easily map each voxel’s position to its corresponding voxel ID, making the hash map implementation more streamlined and intuitive. These concepts will get clear later.</p>

<p>The process of converting a point cloud to voxels using CUDA involves three main steps: <em>hash map building</em>, <em>voxelization</em>, and <em>feature extraction</em> as shown in the Figure below. Hash map building efficiently stores information about unique voxels that contain points, eliminating the need to process all grid voxels. Voxelization assigns each point to its corresponding voxel, creating a serialized array that stores point features for all voxels. Finally, feature extraction calculates the average features for each voxel, resulting in an efficient representation of point cloud features. In all of these steps, CUDA programming helps in parallelization, in that, every point or every voxel is processed on a seperate thread in parallel. In the following section we will understand each of these steps intuitively using our toy example before we delve into the real deal.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_2/steps-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_2/steps-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_2/steps-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_2/steps.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    The key steps involved in CUDA based Voxelization
</div>
<p><br /></p>

<h3 id="a-build-hashmaps">A. Build Hashmaps</h3>
<p>Now, let’s explore the first step of building hash maps. Before diving into the intricacies of building hash maps, it’s essential to understand why they are necessary. In our 3x3 grid example, we observed that out of the 9 voxels, only 6 voxels contain points, while the remaining 3 voxels are entirely empty (as marked in red in the figure below). This situation presents a compelling opportunity for optimization, as processing all 9x9 voxels would be highly inefficient and computationally wasteful. That’s where hash maps comes into play.</p>

<div style="width: 70%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_2/empty_voxels-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_2/empty_voxels-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_2/empty_voxels-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_2/empty_voxels.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
     Why use hash maps? To process only non-empty voxels.
</div> 
</div>

<p>Hash maps offer an efficient way to store and access data by associating each voxel’s position with its corresponding information. By directly mapping the unique voxel positions as keys( <code class="language-plaintext highlighter-rouge">voxel_offset</code> as described earlier to their respective <code class="language-plaintext highlighter-rouge">voxel_id</code>s ( will be explained soon) as values, we can efficiently eliminate the need to process all the empty voxels. This approach drastically reduces memory consumption and processing time, making voxelization of point clouds significantly faster and more resource-efficient. We perform several operations on every point in the point cloud, for building the hash map. All these operation is performed for each point on a seperate thread in parallel using CUDA. Let’s now understand how we go from our scattered 2D points, with certain empty voxels, to an efficient and compact hashmap :</p>
<ol>
  <li><strong>Filtering Points</strong>: We begin by filtering out all the points that lie outside the defined boundary. These points are not relevant for our voxelization and can be safely excluded from further processing.</li>
  <li>
    <p><strong>Voxel Computation</strong> : We take each remaining point from the filtered set and calculate its corresponding voxel indices (voxel_x, voxel_y) using a general point-to-voxel clamping technique. This process ensures that each point is precisely associated with a specific voxel in the 2D grid.</p>

    <p>Let’s consider an example point from our 2D grid with coordinates <code class="language-plaintext highlighter-rouge">(1.8, 2.5)</code>. To convert this point into a voxel, we essentially apply <code class="language-plaintext highlighter-rouge">floor</code> operation to both the coordinates of the points.
 The actual computation is a little more involved, but we will cover that in later section.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   Original Point:[1.8, 2.5]
   Clamped Point: [floor(1.8), floor(2.5)] =[1, 2]
</code></pre></div>    </div>

    <p>Now, the clamped point <code class="language-plaintext highlighter-rouge">(1, 2)</code> represents the voxel indices<code class="language-plaintext highlighter-rouge"> (voxel_x, voxel_y)</code> in the 2D grid. This means that the original point <code class="language-plaintext highlighter-rouge">(1.8, 2.5)</code> is associated with the voxel at grid position <code class="language-plaintext highlighter-rouge">(1, 2)</code>. By performing this computation for all points, we establish a one-to-one mapping between each point and its respective voxel in the 2D grid.</p>
  </li>
  <li><strong>Hash Table Insertion</strong>: Next, we start the hash table insertion, by passing the key as <code class="language-plaintext highlighter-rouge">voxel_offset</code>. The corresponding value for this key is the unique voxel counter, reffered to as <code class="language-plaintext highlighter-rouge">voxel_id</code>, which counts the current number of voxels being added to the hash table. The hash table is a simple array like data structure, which stores all keys first, followed by their corresponding values as shown in the Figure below.</li>
</ol>
<div class="row">
<div class="col-sm mt-3 mt-md-0 text-center">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_2/hash1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_2/hash1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_2/hash1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_2/hash1.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="hash table" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
</div>
<div class="caption">
    Initial Hash Table ( of size 12). First we store the keys and then the corresponding values.
</div>
<p>The insertion essentially happens in three steps :</p>

<ol>
  <li>
    <p><strong>Hashing the Key (Voxel_Offset):</strong> The first step in the insertion process is to hash the given key, which represents the <code class="language-plaintext highlighter-rouge">voxel_offset</code>. Hashing is a mathematical function that transforms the key into a unique numeric value. In our example, we use a simple hash function that involves multiplying the <code class="language-plaintext highlighter-rouge">voxel_offset</code> by 2.</p>
  </li>
  <li>
    <p><strong>Modulus Operation to Find Slot:</strong> After hashing the key, we apply the <code class="language-plaintext highlighter-rouge">modulus(%)</code> operation with the size of the hash table divided by 2. This helps us find the slot in the hash table where the key-value pair should be inserted. The modulus operation ensures that the slot index remains within the valid range of the hash table.</p>
  </li>
  <li>
    <p><strong>Compare and Swap Operation:</strong> The final step is to perform the compare and swap operation (CAS) on the hash table at the slot obtained from the modulus operation. The CAS operation checks if the slot is empty. If it is, the insertion is successful, and the key-value pair is placed in that slot. However, if the slot is not empty, it indicates a collision, meaning that another key with the same hash value already occupies that slot. In this case, we need to handle the collision by employing a linear probing technique, where we check the next slot in the hash table until we find an empty slot. Here we have assumed the size of hash table to be 12, but in practice the size of the hash table is much larger than number of keys, to avoid collisions.</p>
  </li>
</ol>

<p>By following these three basic steps, we can efficiently insert points into the hash table, associating each voxel’s position with its corresponding unique ID. This enables us to store relevant information about the voxels containing points and optimize the voxelization process. A gentle reminder, that these steps occur every point on seperate thread in parallel. Now, let’s dive into an example to illustrate this process in action.</p>

<h4 id="example-1">Example 1</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Psuedo Code : Example 1

INSERTION 1 : Pt(2.3,2.8) 

/* Simple case where the slot for the given key is empty.
We just insert the key to this slot and increment the value of total voxels by 1.*/

Step 1 : Voxel Computation 
voxel_x = floor(2.3) = 2
voxel_y = floor(2.8) = 2
voxel_offset = (voxel_y * size_y) + voxel_x  = 2*3 + 2 = 8 
key = voxel_offset

Step 2 : Hashing of key 
hash_key = hash(key) = key*2 = 8*2 = 16.

Step 3 : Find Slot using Modulus
slot = hash_key%(hash_size/2) = 16%6 = 4

Step 4 : Compare And Swap
value = total_voxels = 0.
// simple case 
hash_table[slot] = key
hash_table[slot+hash_size/2] = value
total_voxel++

</code></pre></div></div>
<p>As seen in the figure below, we simply insert the given key 8, at the computed slot 4, because it was empty. Then we insert the corresponding value at slot 10, (4+6).
The value is zero because this is the first unique voxel being inserted in the hash table.</p>

<div style="width: 70%;margin: 0 auto;">
<div class="row">
<div class="col-sm mt-3 mt-md-0 text-center">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_2/insert1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_2/insert1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_2/insert1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_2/insert1.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="hash table" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
</div>
<div class="caption">
    Example 1 : Hash insert operation for point Pt(2.3,2.8) in red. Inserting key 8 at slot (4) and value 0, at slot (4+6=10). 
</div> 
</div>

<h4 id="example-2">Example 2</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Psuedo Code : Example 2

INSERTION 2 : Pt(1.8,0.5) 

/* Simple case where the slot for the given key is empty.
We just insert the key to this slot and increment the value of total voxels by 1.*/

Step 1 : Voxel Computation 
voxel_x = floor(1.8) = 1
voxel_y = floor(0.5) = 0
voxel_offset = (voxel_y * size_y) + voxel_x = 0*3 + 1 = 1
key = voxel_offset

Step 2 : Hashing of key 
hash_key = hash(key) = key*2  = 1*2 = 2.

Step 3 : Find Slot using Modulus
slot = hash_key%(hash_size/2) = 2%6 = 2

Step 4 : Compare And Swap
value = total_voxels = 1.
// simple case 
hash_table[slot] = key
hash_table[slot+hash_size/2] = value
total_voxel++

</code></pre></div></div>

<p>As seen in the figure below, we simply insert the given key 1, at the computed slot 2, because it was empty. Then we insert the corresponding value at slot 8, (2+6).
The value is the count of the current voxels, which is 1.</p>

<div style="width: 70%;margin: 0 auto;">
<div class="row">
<div class="col-sm mt-3 mt-md-0 text-center">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_2/insert2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_2/insert2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_2/insert2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_2/insert2.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="hash table" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
</div>
<div class="caption">
     Example 2 : Hash insert operation for point Pt(1.8,0.5) shown in red. Inserting key 1 at slot (2) and value 1, at slot (2+6=8). 
</div> 
</div>

<h4 id="example-3">Example 3</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Psuedo Code : Example 3 

INSERTION 3 : Pt(1.7,2.8) 

/*Collision Case wherein the slot computed uusing hash key is already filled
 up by a different key. In this case, insert the key in the next free slot. */

Step 1 : Voxel Computation 
voxel_x = floor(1.7) = 1
voxel_y = floor(2.8) = 2
voxel_offset = (voxel_y * size_y) + voxel_x = 2*3 + 1 = 7
key = voxel_offset

Step 2 : Hashing of key 
hash_key = hash(key) = key*2 =  7*2 = 14.

Step 3 : Find Slot using Modulus
slot = hash_key%(hash_size/2) = 14%6 = 2

Step 4 : Compare And Swap
value = total_voxels = 2
// collision case
hash_table[slot]!= Empty &amp;&amp; hash_table[slot] != voxel_offset
// insert in next slot ( if free)
hash_table[++slot] = key
hash_table[slot+hash_size/2] = value
total_voxel++

</code></pre></div></div>
<p>Now we look at a more interesting example where the slot computed using the modulus of the hashed key is already present in the hash table. So a collision occurs, because the slot is 
already filled up by a different key. We look at this case while trying to insert a Point (1.7,2.8). In such a case of collision, we simply insert the key in the next free slot, which in this case is slot (3), as shown below.</p>

<div style="width: 70%;margin: 0 auto;">
<div class="row">
<div class="col-sm mt-3 mt-md-0 text-center">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_2/collision-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_2/collision-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_2/collision-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_2/collision.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="hash table" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
</div>
<div class="caption">
     Example 3 : Hash insert operation for point Pt(1.7,2.8) shown in red. The slot (2) already has key 1 present, which is different from key 7. This indicates a collision, we insert in 
     in the next empty slot which is slot 3. This is called linear probing.
</div> 
</div>

<h4 id="example-4">Example 4</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Psuedo Code : Example 4

INSERTION 4 : Pt(1.8,0.5) 

/* Already inserted case where the slot for the given key is not emppty.
 No insertion happens, since slot already has same key. */

Step 1 : Voxel Computation 
voxel_x = floor(1.8) = 1
voxel_y = floor(0.5) = 0
voxel_offset = (voxel_y * size_y) + voxel_x = 0*3 + 1 = 1
key = voxel_offset

Step 2 : Hashing of key 
hash_key = hash(key) = key*2  = 1*2 = 2.

Step 3 : Find Slot using Modulus
slot = hash_key%(hash_size/2) = 2%6 = 2

Step 4 : Compare And Swap
value = total_voxels = 3.
// already inserted case 
hash_table[slot] != Empty &amp;&amp; hash_table[slot] == key
// no insertion.

</code></pre></div></div>

<p>As seen in the figure below, we skip insertion, at the computed slot 2, because it already had the current points key 1. The goal is to only store unique <code class="language-plaintext highlighter-rouge">voxel_offset</code> as key
and the number of such unique voxels as values. Since this <code class="language-plaintext highlighter-rouge">voxel_offset</code> has already been inserted in the hash table and is not unique, we simply skip insertion.</p>

<div style="width: 70%;margin: 0 auto;">
<div class="row">
<div class="col-sm mt-3 mt-md-0 text-center">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_2/already-inserted-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_2/already-inserted-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_2/already-inserted-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_2/already-inserted.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="hash table" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
</div>
<div class="caption">
     Example 4 : Hash insert operation for point Pt(1.3,0.3) shown in red.
     No insertion happens, since slot already has same key.  
</div> 
</div>
<p>After successfully completing the hash insertion step, we have effectively built a hash table that stores the <code class="language-plaintext highlighter-rouge">voxel_offset</code> (representing the flattened voxel position) as keys and the corresponding <code class="language-plaintext highlighter-rouge">voxel_ID</code> as values. The <code class="language-plaintext highlighter-rouge">voxel_ID</code> is an indicator of the total number of unique voxels, which is updated as we encounter new unique voxels during the hash table construction process. This data structure offers two significant advantages. Firstly, it allows us to process only those voxels that contain points, thereby optimizing our further steps. Additionally, the process of mapping a point to its respective voxel becomes a constant-time operation <code class="language-plaintext highlighter-rouge">O(1)</code> due to the efficient key searching capability of the hash table. This constant-time retrieval is significantly faster compared to traditional “dictionary(map)” structures, which involve <code class="language-plaintext highlighter-rouge">O(log(n))</code> time complexity for key searches.
<br />
<br /></p>

<h3 id="b-voxelization">B. Voxelization</h3>
<p><br /></p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_2/steps-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_2/steps-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_2/steps-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_2/steps.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    The key steps involved in CUDA based Voxelization
</div>

<p>Having accomplished the challenging task of creating the hash table, we now move on to the second step: “Voxelization.” In this step, we utilize the hash table to efficiently store information of all points from the same voxel in contiguous memory locations. The objective is to construct an array called <code class="language-plaintext highlighter-rouge">voxel_temp</code> with a size of (<code class="language-plaintext highlighter-rouge">max_voxels</code> * <code class="language-plaintext highlighter-rouge">max_points_per_voxel</code> * <code class="language-plaintext highlighter-rouge">num_features</code>), where all point features for each voxel are stored serially. Points belonging to the same voxel are grouped together in memory, thereby optimizing data access and manipulation. If a voxel contains lesser points than <code class="language-plaintext highlighter-rouge">max_points_per_voxel</code>, then those memory locations are left empty. No points for guessing, that these operations are performed in parallel for every point on a seperate thread. We simplify the process by breaking it down into three logical steps.</p>

<ol>
  <li>
    <p><strong>Compute Voxel Offset from Point</strong>
For each point, we determine the corresponding <code class="language-plaintext highlighter-rouge">voxel_offset</code> as described earlier, using the <code class="language-plaintext highlighter-rouge">floor</code> operation. The <code class="language-plaintext highlighter-rouge">voxel_offset</code> represents a unique identifier for a specific voxel in our 2D grid.</p>
  </li>
  <li>
    <p><strong>Efficiently Find Voxel ID from the Hash Table</strong>
To quickly locate the voxel’s position in the array, we leverage the previously constructed hash table. Using the <code class="language-plaintext highlighter-rouge">voxel_offset</code> as the key, we perform a constant-time search in the hash table to find the corresponding <code class="language-plaintext highlighter-rouge">value</code>, which represents the unique <code class="language-plaintext highlighter-rouge">voxel_ID</code>. The intuition is that while making the hash table, the voxel that was identified first will be entered first in the <code class="language-plaintext highlighter-rouge">voxel_temp</code> array. So <code class="language-plaintext highlighter-rouge">voxel_ID</code> essentially provides us this voxel’s location in the <code class="language-plaintext highlighter-rouge">voxel_temp</code> array.</p>
  </li>
  <li>
    <p><strong>Store Point Features in the Voxel Array</strong>
With the <code class="language-plaintext highlighter-rouge">voxel_ID</code> in hand, we efficiently store the point’s features in the <code class="language-plaintext highlighter-rouge">voxel_temp</code> array. This array is designed to hold all the point features for every voxel in a serialized manner, ensuring that points from the same voxel are stored together. We use the <code class="language-plaintext highlighter-rouge">voxel_ID</code> to determine the correct position in the array to store this point’s features, allowing us to efficiently group all points for each voxel.</p>
  </li>
</ol>

<div style="width: 80%;margin: 0 auto;">
<div class="row">
<div class="col-sm mt-3 mt-md-0 text-center">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_2/voxel-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_2/voxel-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_2/voxel-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_2/voxel.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="hash table" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
</div>
<div class="caption">
     Voxelization :  Points A and B in voxel with voxel_offset=5 are stored in position 4 in voxel_temp array. This position 4 is derived from the hash table by looking up value corresponding to the key 5.
</div> 
</div>

<p>Let’s dive into an example to better understand the Voxelization process. Consider our 3x3 grid, and we have two points: <code class="language-plaintext highlighter-rouge">Point A(2.9, 1.7)</code> and <code class="language-plaintext highlighter-rouge">Point B(2.2, 1.3)</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Step 1: Compute Voxel Offset from Points
- For Point A, we calculate the voxel offset by considering its position in the grid.
- As Point A falls within the voxel at (2, 1), the voxel offset for Point A becomes 5. 
- Similarly, for Point B, the voxel offset is also 5 as it belongs to the same voxel.

Step 2: Find Voxel ID from the Hash Table
- We refer to our previously constructed hash table to quickly determine the voxel_ID 
  associated with voxel_offset 5.
- Upon checking the hash table, we find that the value corresponding to the key 5 is 4
- This indicates that this voxel has been assigned voxel ID 4.

Step 3: Store Point Features in the Voxel Temp Array
- Now that we have the voxel ID (4), we store the point features of Point A and Point B 
  in the voxel_temp array at the 4th position. 
- The voxel_temp array contains information about all points for each voxel, 
  ensuring that points within the same voxel are grouped together.
- Since max_points_per_voxel in our example is 3, but this voxel has only 2 points, we keep 
  the remaining space empty.

</code></pre></div></div>

<p>In the accompanying Figure above, we visually depict this process. We show the 3x3 grid, with <code class="language-plaintext highlighter-rouge">Point A</code> and<code class="language-plaintext highlighter-rouge"> Point B</code> marked inside the voxel they belong to. Next, we present the hash map, where we highlight<code class="language-plaintext highlighter-rouge"> voxel_offset (5)</code> and<code class="language-plaintext highlighter-rouge"> voxel_id (4)</code> to showcase how they are linked. Subsequently, we display the <code class="language-plaintext highlighter-rouge">voxel_temp</code> array, with 6 voxels filled up since only 6 of the 9 voxels in our 3x3 grid have points. Finally, we zoom into the voxel with <code class="language-plaintext highlighter-rouge">voxel_id</code> 4 to witness the two points, A and B, stored serially in this array with their respective <code class="language-plaintext highlighter-rouge">(x,y)</code> values.</p>

<p>This example helps illustrate how the Voxelization process organizes point data efficiently, grouping all points belonging to each voxel together, thanks to the hash map’s guidance. This method significantly speeds up access and processing of point cloud data, making voxel-based approaches highly effective for a wide range of applications.</p>

<p>In summary, after completing the Voxelization step, we achieve an organized arrangement where all points belonging to each voxel are conveniently stored together. The hash table plays a key role, acting as a guide that allows us to locate each voxel’s position in the array with ease.</p>

<p><br /></p>

<h3 id="c-feature-extraction">C. Feature extraction</h3>
<p>In the final step of our voxelization process, known as Feature Extraction, we aim to extract meaningful information from the <code class="language-plaintext highlighter-rouge">voxel_temp</code> array, which contains all point features grouped by their respective <code class="language-plaintext highlighter-rouge">voxel_IDs</code>. The goal is to compute average feature values for each voxel and store them in a <code class="language-plaintext highlighter-rouge">voxel_features</code> array. In the feature extraction step, since we are operating on the voxel level ( and not on point level ), we assign one thread for every voxel and perform all operations for every voxel in parallel using CUDA.</p>

<ol>
  <li>
    <p><strong>Prepare for Feature Extraction</strong>
The feature extraction process operates on a per-voxel basis, where each voxel’s features are processed independently. To begin, we initialize the <code class="language-plaintext highlighter-rouge">num_points_per_voxel</code> array to keep track of the number of points in each voxel. Then, for each voxel, we iterate over its points to calculate the total number of points and update the corresponding entry in the <code class="language-plaintext highlighter-rouge">num_points_per_voxel</code> array.</p>
  </li>
  <li>
    <p><strong>Calculate Average Feature Values</strong>
Next, we calculate the average feature values for each voxel. Starting with the first point in the <code class="language-plaintext highlighter-rouge">voxel_temp</code> array for a given voxel, we compute the offset to access this voxel’s data in the array. For subsequent points within the same voxel, we iterate over all features (e.g., x, y, z, intensity, time) and sum up their values.</p>
  </li>
  <li>
    <p><strong>Update Voxel Features</strong>
After summing up the features for all points within the voxel, we calculate the average by dividing the sum by the total number of points in that voxel. These averaged feature values are then updated in the <code class="language-plaintext highlighter-rouge">voxel_features</code> array at the position corresponding to the <code class="language-plaintext highlighter-rouge">voxel_ID</code>.</p>
  </li>
</ol>

<p>In summary, the Feature Extraction step processes <code class="language-plaintext highlighter-rouge">voxel_temp</code> data to calculate the average feature values for each voxel. This process is performed in parallel for all voxels, utilizing one thread per voxel. By the end of this step, the <code class="language-plaintext highlighter-rouge">voxel_features</code> array holds crucial information about each voxel’s characteristics in continuous memory, ready for further analysis and applications in point cloud processing. Each voxel’s information is stored in the <code class="language-plaintext highlighter-rouge">voxel_features</code> array at the position given by <code class="language-plaintext highlighter-rouge">voxel_ID</code>. For example we see feature extraction for the voxel at position 4 as shown in the Figure below.</p>

<div style="width: 80%;margin: 0 auto;">
<div class="row">
<div class="col-sm mt-3 mt-md-0 text-center">
    <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_2/feature_extract-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_2/feature_extract-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_2/feature_extract-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_2/feature_extract.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="hash table" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
</div>
<div class="caption">
     Feature Extraction :  Extract point features from voxel_temp array for every voxel and store the average in voxel_features array at it's voxel_ID. Here the features are (x,y) coordinates.
</div> 
</div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Voxel Feature Extraction for Voxel at Position 4

1. Prepare for Feature Extraction:
   We first initialize the num_points_per_voxel array to keep 
   track of the number of points in each voxel.
   For the voxel at position 4, num_points_per_voxel[4] = 2, as it contains two points.

2. Calculate Average Feature Values:
   We now iterate over the points in the voxel_temp array 
   for the voxel at position 4.Let's consider the features x and y 
   for this example.

   Iteration 1:
   - Point A: x = 2.9, y = 1.7
   - Sum of x = 2.9, Sum of y = 1.7

   Iteration 2:
   - Point B: x = 2.2, y = 1.3
   - Sum of x = 2.9 + 2.2 = 5.1, Sum of y = 1.7 + 1.3 = 3.0

3. Update Voxel Features:
   After iterating through all points in the voxel, we have the 
   sums of x and y for each feature.Now, we calculate the average by dividing
   the sum by the total number of points in the voxel (num_points_per_voxel[4] = 2).

   - Average x = 5.1 / 2 = 2.55
   - Average y = 3.0 / 2 = 1.5

   Finally, we update the voxel_features array for the voxel at position 4 with 
   the calculated average feature values: voxel_features[4] = (2.55, 1.5)

</code></pre></div></div>

<h2 id="4-inside-the-code">4. Inside the Code</h2>
<p>In this section, we will take a comprehensive look at the CUDA-powered voxelization implementation. We’ll dissect the code, step by step, to understand the magic behind its blazing-fast performance. To begin our exploration, let’s first setup and build the project locally and run the executable to see the CPU vs GPU performance. Then, we examine the folder structure that forms the foundation of this efficient voxelization engine. Understanding the organization of the code will serve as a solid starting point to grasp the inner workings of CUDA and its pivotal role in accelerating the voxelization process. So, let’s embark on this informative journey and unravel the secrets behind this powerful technique.</p>

<h4 id="41-setting-up-locally">4.1 Setting up Locally</h4>
<p>Before diving into the code, let’s set up the project locally by following these steps:
Setup Project Locally:</p>

<ol>
  <li>Install CUDA &gt; 11.1.</li>
  <li>Ensure you are using Ubuntu &gt; 18.04.</li>
  <li>Add CUDA path to <code class="language-plaintext highlighter-rouge">PATH</code> and <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code>.</li>
  <li>OPTIONAL : PCL &gt; 1.8.1 ( Only for visualization)</li>
</ol>

<p>Now, let’s build and run the project with the provided commands:
<br /></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/sanket-pixel/voxelize-cuda
<span class="nb">cd </span>voxelize-cuda
<span class="nb">mkdir </span>build <span class="o">&amp;&amp;</span> <span class="nb">cd </span>build
cmake ..
make
./voxelize_cuda ../data/test/ <span class="nt">--cpu</span> 
</code></pre></div></div>
<p>You can expect an output similar to this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>GPU has cuda devices: 1
----device id: 0 info----
  GPU : GeForce RTX 2060 
  Capbility: 7.5
  Global memory: 5912MB
  Const memory: 64KB
  SM in a block: 48KB
  warp size: 32
  threads in a block: 1024
  block dim: (1024,1024,64)
  grid dim: (2147483647,65535,65535)
-------------------------

Total 10
Average GPU Voxelization Time : 0.643269
Average CPU Voxelization Time : 374.432
Average GPU vs CPU Speedup : 582.076x times 

</code></pre></div></div>

<p>To start the visualization, make sure the PCL Library is installed before building. Then execute with <code class="language-plaintext highlighter-rouge">--visualize</code> flag.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./voxelize_cuda ../data/test/ <span class="nt">--visualize</span> 
</code></pre></div></div>
<p>Two windows will open up with titles <code class="language-plaintext highlighter-rouge">Point Cloud Viewer</code> and <code class="language-plaintext highlighter-rouge">Voxel Cloud Viewer</code> showing the original and voxelized point cloud respectively. Note that voxelized here means the point cloud after the feature extraction step. So the points represents the average of all points in each voxel.</p>

<p>To see detail logs of performance of every file, execute with <code class="language-plaintext highlighter-rouge">--verbose</code> flag.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./voxelize_cuda ../data/test/ <span class="nt">--verbose</span> 
</code></pre></div></div>

<p>By default the executable will just perform CUDA based Voxelization on GPU. To also perform voxelization
on the CPU, execute with the <code class="language-plaintext highlighter-rouge">--cpu</code> flag.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./voxelize_cuda ../data/test/ <span class="nt">--cpu</span> 
</code></pre></div></div>
<p>This will show time comparisions of CPU and GPU.</p>

<p>Once you have obtained this output, you can take a cup of coffee, as we are now ready to deep dive into the code. Let’s explore the implementation in detail.</p>

<h4 id="42-folder-structure">4.2. Folder structure</h4>

<p>The project directory contains the following files and folders:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ├── CMakeLists.txt
    ├── data
    │   └── test
    │       ├── pc1.bin
    │       └── pc2.bin
    |              .
    |              .
    |              .
    ├── include
    │   ├── common.h
    │   ├── kernel.h
    │   ├── visualizer.hpp
    │   ├── VoxelizerCPU.hpp
    │   └── VoxelizerGPU.h
    |   
    ├── main.cpp
    ├── README.md
    └── src
        ├── preprocess_kernel.cu
        ├── visualizer.cpp
        ├── VoxelizerCPU.cpp
        └── VoxelizerGPU.cpp
</code></pre></div></div>

<ul>
  <li><a href="#">CMakeLists.txt</a> - CMake configuration file for building the project</li>
  <li><strong>data</strong> - Directory containing test data used in the project.
    <ul>
      <li><strong>test</strong> - Subdirectory containing binary data files.</li>
    </ul>
  </li>
  <li><strong>include</strong> - Directory containing header files.
    <ul>
      <li><a href="#">common.h</a> - Header file with common definitions and macros.</li>
      <li><a href="#">kernel.h</a> - Header file with CUDA kernel function declarations.</li>
      <li><a href="#">visualizer.hpp</a> - Header file for Visualizer functions.</li>
      <li><a href="#">VoxelizerCPU.hpp</a> - Header file with CPU Voxelization functions</li>
      <li><a href="#">VoxelizerGPU.h</a> - Header file for the CUDA based Voxelization functions.</li>
    </ul>
  </li>
  <li><a href="#">main.cpp</a> - Main C++ source file that orchestrates the voxelization process.</li>
  <li><a href="#">README.md</a> - Markdown file containing project documentation and information.</li>
  <li><strong>src</strong> - Directory containing source files.
    <ul>
      <li><a href="#">preprocess_kernel.cu</a> - CUDA source file with kernel implementations.</li>
      <li><a href="#">visualizer.cpp</a> - Visualization function implementations.</li>
      <li><a href="#">VoxelizerCPU.cpp</a> - CPU Voxelization function implementations.</li>
      <li><a href="#">VoxelizerGPU.cpp</a> - GPU Voxelization function implementations.</li>
    </ul>
  </li>
</ul>

<h4 id="43-code-walkthrough">4.3 Code Walkthrough</h4>

<p>The main entry point of the program is the main function in the <code class="language-plaintext highlighter-rouge">main.cpp</code> file. It starts by checking the command-line arguments and loading the point cloud data from the specified folder.</p>

<h5 id="step-1-setup-and-initialization">Step 1: Setup and Initialization</h5>

<ul>
  <li>
    <p>The <code class="language-plaintext highlighter-rouge">GetDeviceInfo</code> function is used to print information about the CUDA devices available on the system.</p>
  </li>
  <li>
    <p>The <code class="language-plaintext highlighter-rouge">getFolderFile</code> function is used to get a list of files in the specified data folder with a “.bin” extension.</p>
  </li>
  <li>
    <p>The <code class="language-plaintext highlighter-rouge">loadData</code> function is used to load the binary data file into memory.</p>
  </li>
</ul>

<h5 id="step-2-voxelization-cpu">Step 2: Voxelization CPU</h5>

<p>The CPU Voxelization is handled by the <code class="language-plaintext highlighter-rouge">VoxelizerCPU</code> class, defined in the <code class="language-plaintext highlighter-rouge">VoxelizerCPU.h</code> and <code class="language-plaintext highlighter-rouge">VoxelizerCPU.cpp</code> files.
It performs Voxelization on CPU using standard C++ operations.</p>

<h4 id="step-3-voxelization-gpu">Step 3: Voxelization GPU</h4>
<p>The CUDA based GPU Voxelization is handled by the <code class="language-plaintext highlighter-rouge">VoxelizerGPU</code> class, defined in the <code class="language-plaintext highlighter-rouge">VoxelizerGPU.h</code> and <code class="language-plaintext highlighter-rouge">VoxelizerGPU.cpp</code> files.
It performs Voxelization on GPU using CUDA kernels for Hash Map Building, Voxelization and Feature Extraction.</p>

<p><strong>A. Hash Map Building</strong>:</p>

<p>The hash map building is performed in the <code class="language-plaintext highlighter-rouge">buildHashKernel</code> CUDA kernel defined in the <code class="language-plaintext highlighter-rouge">preprocess_kernel.cu</code> file. This kernel takes the input point cloud data and converts it into voxel coordinates using the specified voxel size and range. It then builds a hash table that maps each <code class="language-plaintext highlighter-rouge">voxel_offset</code> to its corresponding <code class="language-plaintext highlighter-rouge">voxel_ID</code>.</p>

<p><strong>B. Voxelization</strong>:</p>

<p>The voxelization is performed in the <code class="language-plaintext highlighter-rouge">voxelizationKernel</code> CUDA kernel, also defined in the <code class="language-plaintext highlighter-rouge">preprocess_kernel.cu</code> file. This kernel uses the hash table built in the previous step to assign each point to its corresponding voxel. It counts the number of points in each voxel and stores them in the <code class="language-plaintext highlighter-rouge">num_points_per_voxel</code> array. It also serializes the point features for each voxel in the <code class="language-plaintext highlighter-rouge">voxels_temp</code> array.</p>

<p><strong>C. Feature Extraction</strong>:</p>

<p>The feature extraction is handled by the <code class="language-plaintext highlighter-rouge">featureExtractionKernel</code> CUDA kernel, also defined in the <code class="language-plaintext highlighter-rouge">preprocess_kernel.cu</code> file. This kernel takes the serialized point features in the <code class="language-plaintext highlighter-rouge">voxels_temp</code> array and computes the average feature values for each voxel. It stores the averaged features in the <code class="language-plaintext highlighter-rouge">voxel_features</code> array.</p>

<h4 id="step-4-output-and-cleanup">Step 4: Output and Cleanup</h4>

<p>After the Voxelization GPU is complete for all the input files, the program outputs the results and frees the allocated memory.</p>

<h4 id="44-deep-dive">4.4 Deep Dive</h4>
<p>Now that we have taken a closer look at the basic walkthrough of the code, let’s embark on a more comprehensive deep dive, exploring the intricacies of the CUDA kernels and delving into the inner workings of the preprocessor class. We will gradually progress from the core CUDA kernel, which handles the voxelization process efficiently through parallelism, to the preprocessor class, where these kernels are utilized. Finally, we will uncover how the main cpp file leverages the functionalities of the preprocessor class to apply voxelization on the input point cloud data, culminating in the generation of 3D voxels. This step-by-step approach will allow us to understand how each component contributes to the overall process and how they interact harmoniously to produce the desired output. So, let’s begin our journey from inside to out, unraveling the complexities of the code and gaining a deeper understanding of its functioning.</p>

<p><strong>1. CUDA Kernel for Building HashTables</strong></p>

<p>The <code class="language-plaintext highlighter-rouge">buildHashKernel</code> is a CUDA kernel that performs the hash table building process. It takes the input point cloud data (<code class="language-plaintext highlighter-rouge">points</code>) and converts each point into voxel coordinates based on the specified voxel size and range. Then, it calls the <code class="language-plaintext highlighter-rouge">insertHashTable</code> function to insert each voxel offset as a key into the hash table, and the <code class="language-plaintext highlighter-rouge">real_voxel_num</code> variable is updated to keep track of the number of unique voxels in the hash table. The <code class="language-plaintext highlighter-rouge">buildHashKernel</code> function is executed by multiple CUDA threads in parallel, each processing a different point from the input point cloud. As a result, the hash table is efficiently built using GPU parallelism, and each voxel offset is uniquely assigned to an entry in the hash table.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">buildHashKernel</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">points</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">points_size</span><span class="p">,</span>
        <span class="kt">float</span> <span class="n">min_x_range</span><span class="p">,</span> <span class="kt">float</span> <span class="n">max_x_range</span><span class="p">,</span>
        <span class="kt">float</span> <span class="n">min_y_range</span><span class="p">,</span> <span class="kt">float</span> <span class="n">max_y_range</span><span class="p">,</span>
        <span class="kt">float</span> <span class="n">min_z_range</span><span class="p">,</span> <span class="kt">float</span> <span class="n">max_z_range</span><span class="p">,</span>
        <span class="kt">float</span> <span class="n">voxel_x_size</span><span class="p">,</span> <span class="kt">float</span> <span class="n">voxel_y_size</span><span class="p">,</span> <span class="kt">float</span> <span class="n">voxel_z_size</span><span class="p">,</span>
        <span class="kt">int</span> <span class="n">grid_y_size</span><span class="p">,</span> <span class="kt">int</span> <span class="n">grid_x_size</span><span class="p">,</span> <span class="kt">int</span> <span class="n">feature_num</span><span class="p">,</span>
	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="o">*</span><span class="n">hash_table</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="o">*</span><span class="n">real_voxel_num</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">point_idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">point_idx</span> <span class="o">&gt;=</span> <span class="n">points_size</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span><span class="p">;</span>
  <span class="p">}</span>
  
  <span class="kt">float</span> <span class="n">px</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">feature_num</span> <span class="o">*</span> <span class="n">point_idx</span><span class="p">];</span>
  <span class="kt">float</span> <span class="n">py</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">feature_num</span> <span class="o">*</span> <span class="n">point_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">];</span>
  <span class="kt">float</span> <span class="n">pz</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">feature_num</span> <span class="o">*</span> <span class="n">point_idx</span> <span class="o">+</span> <span class="mi">2</span><span class="p">];</span>

  <span class="k">if</span><span class="p">(</span> <span class="n">px</span> <span class="o">&lt;</span> <span class="n">min_x_range</span> <span class="o">||</span> <span class="n">px</span> <span class="o">&gt;=</span> <span class="n">max_x_range</span> <span class="o">||</span> <span class="n">py</span> <span class="o">&lt;</span> <span class="n">min_y_range</span> <span class="o">||</span> <span class="n">py</span> <span class="o">&gt;=</span> <span class="n">max_y_range</span>
    <span class="o">||</span> <span class="n">pz</span> <span class="o">&lt;</span> <span class="n">min_z_range</span> <span class="o">||</span> <span class="n">pz</span> <span class="o">&gt;=</span> <span class="n">max_z_range</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">voxel_idx</span> <span class="o">=</span> <span class="n">floorf</span><span class="p">((</span><span class="n">px</span> <span class="o">-</span> <span class="n">min_x_range</span><span class="p">)</span> <span class="o">/</span> <span class="n">voxel_x_size</span><span class="p">);</span>
  <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">voxel_idy</span> <span class="o">=</span> <span class="n">floorf</span><span class="p">((</span><span class="n">py</span> <span class="o">-</span> <span class="n">min_y_range</span><span class="p">)</span> <span class="o">/</span> <span class="n">voxel_y_size</span><span class="p">);</span>
  <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">voxel_idz</span> <span class="o">=</span> <span class="n">floorf</span><span class="p">((</span><span class="n">pz</span> <span class="o">-</span> <span class="n">min_z_range</span><span class="p">)</span> <span class="o">/</span> <span class="n">voxel_z_size</span><span class="p">);</span>
  <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">voxel_offset</span> <span class="o">=</span> <span class="n">voxel_idz</span> <span class="o">*</span> <span class="n">grid_y_size</span> <span class="o">*</span> <span class="n">grid_x_size</span>
	                    <span class="o">+</span> <span class="n">voxel_idy</span> <span class="o">*</span> <span class="n">grid_x_size</span>
                            <span class="o">+</span> <span class="n">voxel_idx</span><span class="p">;</span>
  <span class="n">insertHashTable</span><span class="p">(</span><span class="n">voxel_offset</span><span class="p">,</span> <span class="n">real_voxel_num</span><span class="p">,</span> <span class="n">points_size</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hash_table</span><span class="p">);</span>
<span class="p">}</span>

</code></pre></div></div>

<p>Here’s how the buildHashKernel works:</p>
<ol>
  <li>
    <p><code class="language-plaintext highlighter-rouge">__global__ void buildHashKernel(...)</code>: This line defines the <code class="language-plaintext highlighter-rouge">buildHashKernel</code> function as a CUDA kernel using the <code class="language-plaintext highlighter-rouge">__global__</code> function modifier. As a kernel, this function will be executed in parallel by multiple threads on the GPU.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">int point_idx = blockIdx.x * blockDim.x + threadIdx.x</code>: This line calculates the index of the current point to be processed by the CUDA thread.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">if (point_idx &gt;= points_size) { return; }</code>: This condition checks if the thread index is out of bounds (i.e., beyond the number of points in the input points array). If so, the thread returns early to avoid processing invalid data.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">float px = points[feature_num * point_idx]; ...</code>: These lines extract the X, Y, and Z coordinates of the current point from the input points array based on the <code class="language-plaintext highlighter-rouge">feature_num</code> (the number of features per point).</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">if (px &lt; min_x_range \|\| px &gt;= max_x_range \|\| ...</code>: This condition checks if the current point lies within the specified 3D range (min/max X, Y, Z). If the point is outside this range, it is not considered for voxelization, and the thread returns early.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">unsigned int voxel_idx = floorf((px - min_x_range) / voxel_x_size); ...</code>: These lines calculate the voxel coordinates (<code class="language-plaintext highlighter-rouge">voxel_idx</code>, <code class="language-plaintext highlighter-rouge">voxel_idy</code>, <code class="language-plaintext highlighter-rouge">voxel_idz</code>) corresponding to the current point’s X, Y, and Z coordinates based on the specified voxel sizes and ranges.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">unsigned int voxel_offset = voxel_idz * grid_y_size * grid_x_size ...</code>: This line calculates the <code class="language-plaintext highlighter-rouge">voxel_offset</code> based on the voxel coordinates. The <code class="language-plaintext highlighter-rouge">voxel_offset</code> is a flattened  index for each voxel within the 3D grid.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">insertHashTable(voxel_offset, real_voxel_num, points_size * 2 * 2, hash_table);</code>:  This line calls the <code class="language-plaintext highlighter-rouge">insertHashTable</code> function to insert the current <code class="language-plaintext highlighter-rouge">voxel_offset</code> into the hash table. It also updates the <code class="language-plaintext highlighter-rouge">real_voxel_num</code> variable using the <code class="language-plaintext highlighter-rouge">atomicAdd</code> function to keep track of the number of unique voxels added to the hash table.</p>
  </li>
</ol>

<p><strong>2. Inserting into HashTable</strong></p>

<p>Next, let’s move on to the <code class="language-plaintext highlighter-rouge">insertHashTable</code> function:</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Function to insert a key-value pair into the hash table</span>
<span class="n">__device__</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">insertHashTable</span><span class="p">(</span><span class="k">const</span> <span class="kt">uint32_t</span> <span class="n">key</span><span class="p">,</span> <span class="kt">uint32_t</span> <span class="o">*</span><span class="n">value</span><span class="p">,</span>
		<span class="k">const</span> <span class="kt">uint32_t</span> <span class="n">hash_size</span><span class="p">,</span> <span class="kt">uint32_t</span> <span class="o">*</span><span class="n">hash_table</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">uint64_t</span> <span class="n">hash_value</span> <span class="o">=</span> <span class="n">hash</span><span class="p">(</span><span class="n">key</span><span class="p">);</span>
  <span class="kt">uint32_t</span> <span class="n">slot</span> <span class="o">=</span> <span class="n">hash_value</span> <span class="o">%</span> <span class="p">(</span><span class="n">hash_size</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span><span class="cm">/*key, value*/</span><span class="p">;</span>
  <span class="kt">uint32_t</span> <span class="n">empty_key</span> <span class="o">=</span> <span class="n">UINT32_MAX</span><span class="p">;</span>
  <span class="k">while</span> <span class="p">(</span><span class="nb">true</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">uint32_t</span> <span class="n">pre_key</span> <span class="o">=</span> <span class="n">atomicCAS</span><span class="p">(</span><span class="n">hash_table</span> <span class="o">+</span> <span class="n">slot</span><span class="p">,</span> <span class="n">empty_key</span><span class="p">,</span> <span class="n">key</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">pre_key</span> <span class="o">==</span> <span class="n">empty_key</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">hash_table</span><span class="p">[</span><span class="n">slot</span> <span class="o">+</span> <span class="n">hash_size</span> <span class="o">/</span> <span class="mi">2</span> <span class="cm">/*offset*/</span><span class="p">]</span> <span class="o">=</span> <span class="n">atomicAdd</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
      <span class="k">break</span><span class="p">;</span>
    <span class="p">}</span> <span class="k">else</span> <span class="nf">if</span> <span class="p">(</span><span class="n">pre_key</span> <span class="o">==</span> <span class="n">key</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">break</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">slot</span> <span class="o">=</span> <span class="p">(</span><span class="n">slot</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">hash_size</span> <span class="o">/</span> <span class="mi">2</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Explanation:</p>

<p>The <code class="language-plaintext highlighter-rouge">insertHashTable</code> function is responsible for inserting a key-value pair into the hash table. It uses the previously explained <code class="language-plaintext highlighter-rouge">hash</code> function to compute the hash value of the key, and then it resolves hash collisions using a technique called linear probing.</p>

<p>Here’s how the <code class="language-plaintext highlighter-rouge">insertHashTable</code> function works:</p>

<ol>
  <li>
    <p><code class="language-plaintext highlighter-rouge">uint64_t hash_value = hash(key)</code>: This line computes the hash value of the input <code class="language-plaintext highlighter-rouge">key</code> using the <code class="language-plaintext highlighter-rouge">hash</code> function.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">uint32_t slot = hash_value % (hash_size / 2)</code>: This line calculates the initial slot index in the hash table by taking the modulo of the hash value with half of the hash table size. This ensures that the slot index is within the valid range of the hash table.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">uint32_t empty_key = UINT32_MAX</code>: This line sets the <code class="language-plaintext highlighter-rouge">empty_key</code> variable to the maximum value of a 32-bit unsigned integer. This value is used to indicate an empty slot in the hash table.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">while (true) { ... }</code>: This is a loop that continues until the key is successfully inserted into the hash table. It handles hash collisions using linear probing.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">uint32_t pre_key = atomicCAS(hash_table + slot, empty_key, key)</code>: This line performs an atomic compare-and-swap operation (CAS) on the hash table. It checks if the slot at the current index (<code class="language-plaintext highlighter-rouge">slot</code>) is empty (i.e., contains <code class="language-plaintext highlighter-rouge">empty_key</code>). If it is empty, it atomically swaps the value with the input <code class="language-plaintext highlighter-rouge">key</code>, effectively inserting the key into the hash table.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">if (pre_key == empty_key) { ... }</code>: This condition checks if the CAS operation was successful, indicating that the key was inserted into the hash table. If successful, the function proceeds to update the offset in the hash table for the corresponding value (used in feature extraction).</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">hash_table[slot + hash_size / 2 /*offset*/] = atomicAdd(value, 1)</code>: This line atomically increments the value in the hash table at the offset <code class="language-plaintext highlighter-rouge">slot + hash_size / 2</code>, effectively storing the value for the given key. The <code class="language-plaintext highlighter-rouge">atomicAdd</code> function ensures that multiple threads trying to insert the same key concurrently will get unique values.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">else if (pre_key == key) { ... }</code>: This condition handles the case when the slot already contains the same key (a duplicate). In this case, the function breaks out of the loop, as there’s no need to insert the key again.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">slot = (slot + 1) % (hash_size / 2)</code>: This line updates the slot index using linear probing by incrementing it by 1 and wrapping around to the beginning if it exceeds half of the hash table size.</p>
  </li>
</ol>

<p>The <code class="language-plaintext highlighter-rouge">insertHashTable</code> function plays a crucial role in building the hash table, which is later used in voxelization and feature extraction.</p>

<p><strong>3. Hash Function</strong></p>

<p>Sure, let’s start by explaining the base function, which is the <code class="language-plaintext highlighter-rouge">hash</code> function:</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Hash function for generating a 64-bit hash value</span>
<span class="n">__device__</span> <span class="kr">inline</span> <span class="kt">uint64_t</span> <span class="nf">hash</span><span class="p">(</span><span class="kt">uint64_t</span> <span class="n">k</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">k</span> <span class="o">^=</span> <span class="n">k</span> <span class="o">&gt;&gt;</span> <span class="mi">16</span><span class="p">;</span>
  <span class="n">k</span> <span class="o">*=</span> <span class="mh">0x85ebca6b</span><span class="p">;</span>
  <span class="n">k</span> <span class="o">^=</span> <span class="n">k</span> <span class="o">&gt;&gt;</span> <span class="mi">13</span><span class="p">;</span>
  <span class="n">k</span> <span class="o">*=</span> <span class="mh">0xc2b2ae35</span><span class="p">;</span>
  <span class="n">k</span> <span class="o">^=</span> <span class="n">k</span> <span class="o">&gt;&gt;</span> <span class="mi">16</span><span class="p">;</span>
  <span class="k">return</span> <span class="n">k</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Explanation:</p>

<p>The <code class="language-plaintext highlighter-rouge">hash</code> function is a simple hash function that takes a 64-bit integer <code class="language-plaintext highlighter-rouge">k</code> as input and generates a 64-bit hash value using bitwise operations and multiplication with constants.</p>

<p>Here’s how the hash function works:</p>

<ol>
  <li>
    <p><code class="language-plaintext highlighter-rouge">k ^= k &gt;&gt; 16</code>: This line performs a bitwise XOR operation between <code class="language-plaintext highlighter-rouge">k</code> and <code class="language-plaintext highlighter-rouge">k</code> right-shifted by 16 bits. This step introduces a level of randomness to the bits.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">k *= 0x85ebca6b</code>: This line multiplies <code class="language-plaintext highlighter-rouge">k</code> by a constant value (<code class="language-plaintext highlighter-rouge">0x85ebca6b</code>). Multiplication helps in spreading out the bits and reducing collisions.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">k ^= k &gt;&gt; 13</code>: This line again performs a bitwise XOR operation between <code class="language-plaintext highlighter-rouge">k</code> and <code class="language-plaintext highlighter-rouge">k</code> right-shifted by 13 bits. This step further increases the randomness of the bits.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">k *= 0xc2b2ae35</code>: This line multiplies <code class="language-plaintext highlighter-rouge">k</code> by another constant value (<code class="language-plaintext highlighter-rouge">0xc2b2ae35</code>) to spread out the bits even more.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">k ^= k &gt;&gt; 16</code>: Finally, this line performs a bitwise XOR operation between <code class="language-plaintext highlighter-rouge">k</code> and <code class="language-plaintext highlighter-rouge">k</code> right-shifted by 16 bits. This step is the last step of mixing the bits to produce the final hash value.</p>
  </li>
</ol>

<p>The <code class="language-plaintext highlighter-rouge">hash</code> function is used in the hash table building process to generate unique hash values for the voxel offsets in the point cloud data.</p>

<p><strong>4. Voxelization Kernel</strong></p>

<p>In the voxelizationKernel CUDA kernel, each thread processes an individual point from the input point cloud. The goal of this function is to efficiently convert the points into their corresponding 3D voxel representations. It first calculates the voxel coordinates for each point based on the specified voxel sizes and ranges. Then, it uses a hash table lookup to find the corresponding voxel ID for each voxel offset. If the voxel ID is within the specified maximum number of voxels, the function atomically adds the point to the voxel and updates the voxel indices accordingly. This ensures a fast and optimized voxelization process for large and sparse point clouds.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">voxelizationKernel</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">points</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">points_size</span><span class="p">,</span>
                                   <span class="kt">float</span> <span class="n">min_x_range</span><span class="p">,</span> <span class="kt">float</span> <span class="n">max_x_range</span><span class="p">,</span>
                                   <span class="kt">float</span> <span class="n">min_y_range</span><span class="p">,</span> <span class="kt">float</span> <span class="n">max_y_range</span><span class="p">,</span>
                                   <span class="kt">float</span> <span class="n">min_z_range</span><span class="p">,</span> <span class="kt">float</span> <span class="n">max_z_range</span><span class="p">,</span>
                                   <span class="kt">float</span> <span class="n">voxel_x_size</span><span class="p">,</span> <span class="kt">float</span> <span class="n">voxel_y_size</span><span class="p">,</span> <span class="kt">float</span> <span class="n">voxel_z_size</span><span class="p">,</span>
                                   <span class="kt">int</span> <span class="n">grid_y_size</span><span class="p">,</span> <span class="kt">int</span> <span class="n">grid_x_size</span><span class="p">,</span> <span class="kt">int</span> <span class="n">feature_num</span><span class="p">,</span> <span class="kt">int</span> <span class="n">max_voxels</span><span class="p">,</span>
                                   <span class="kt">int</span> <span class="n">max_points_per_voxel</span><span class="p">,</span>
                                   <span class="kt">unsigned</span> <span class="kt">int</span> <span class="o">*</span><span class="n">hash_table</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="o">*</span><span class="n">num_points_per_voxel</span><span class="p">,</span>
                                   <span class="kt">float</span> <span class="o">*</span><span class="n">voxels_temp</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="o">*</span><span class="n">voxel_indices</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="o">*</span><span class="n">real_voxel_num</span><span class="p">)</span> <span class="p">{</span>
  
  <span class="c1">// give every point to a thread. Find the index of the current point within this kernel.</span>
  <span class="kt">int</span> <span class="n">point_idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">point_idx</span> <span class="o">&gt;=</span> <span class="n">points_size</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="c1">// points is the array of points in the point cloud storing point features (x, y, z, intensity, t)</span>
  <span class="c1">// in a serialized format. We access px, py, pz of this point now.</span>
  <span class="c1">// feature_num is 5, representing the number of features per point.</span>
  <span class="kt">float</span> <span class="n">px</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">feature_num</span> <span class="o">*</span> <span class="n">point_idx</span><span class="p">];</span>
  <span class="kt">float</span> <span class="n">py</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">feature_num</span> <span class="o">*</span> <span class="n">point_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">];</span>
  <span class="kt">float</span> <span class="n">pz</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">feature_num</span> <span class="o">*</span> <span class="n">point_idx</span> <span class="o">+</span> <span class="mi">2</span><span class="p">];</span>

  <span class="c1">// If the point is outside the range along the (x, y, z) dimensions, stop further </span>
  <span class="c1">// processing and return.</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">px</span> <span class="o">&lt;</span> <span class="n">min_x_range</span> <span class="o">||</span> <span class="n">px</span> <span class="o">&gt;=</span> <span class="n">max_x_range</span> <span class="o">||</span> <span class="n">py</span> <span class="o">&lt;</span> <span class="n">min_y_range</span> <span class="o">||</span> <span class="n">py</span> <span class="o">&gt;=</span> <span class="n">max_y_range</span>
    <span class="o">||</span> <span class="n">pz</span> <span class="o">&lt;</span> <span class="n">min_z_range</span> <span class="o">||</span> <span class="n">pz</span> <span class="o">&gt;=</span> <span class="n">max_z_range</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="c1">// Now find the voxel id for this point using the usual voxel conversion logic.</span>
  <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">voxel_idx</span> <span class="o">=</span> <span class="n">floorf</span><span class="p">((</span><span class="n">px</span> <span class="o">-</span> <span class="n">min_x_range</span><span class="p">)</span> <span class="o">/</span> <span class="n">voxel_x_size</span><span class="p">);</span>
  <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">voxel_idy</span> <span class="o">=</span> <span class="n">floorf</span><span class="p">((</span><span class="n">py</span> <span class="o">-</span> <span class="n">min_y_range</span><span class="p">)</span> <span class="o">/</span> <span class="n">voxel_y_size</span><span class="p">);</span>
  <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">voxel_idz</span> <span class="o">=</span> <span class="n">floorf</span><span class="p">((</span><span class="n">pz</span> <span class="o">-</span> <span class="n">min_z_range</span><span class="p">)</span> <span class="o">/</span> <span class="n">voxel_z_size</span><span class="p">);</span>
  <span class="c1">// Now find the voxel offset, which is the index of the voxel if all voxels are flattened.</span>
  <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">voxel_offset</span> <span class="o">=</span> <span class="n">voxel_idz</span> <span class="o">*</span> <span class="n">grid_y_size</span> <span class="o">*</span> <span class="n">grid_x_size</span>
                            <span class="o">+</span> <span class="n">voxel_idy</span> <span class="o">*</span> <span class="n">grid_x_size</span>
                            <span class="o">+</span> <span class="n">voxel_idx</span><span class="p">;</span>
  <span class="c1">// We perform a scatter operation to voxels, and the result is stored in 'voxel_id'.</span>
  <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">voxel_id</span> <span class="o">=</span> <span class="n">lookupHashTable</span><span class="p">(</span><span class="n">voxel_offset</span><span class="p">,</span> <span class="n">points_size</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hash_table</span><span class="p">);</span>
  <span class="c1">// If the current voxel id is greater than max_voxels, simply return.</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">voxel_id</span> <span class="o">&gt;=</span> <span class="n">max_voxels</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span><span class="p">;</span>
  <span class="p">}</span>
  
  <span class="c1">// With the voxel id, we can now atomically increment the counter for the number of points in the voxel.</span>
  <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">current_num</span> <span class="o">=</span> <span class="n">atomicAdd</span><span class="p">(</span><span class="n">num_points_per_voxel</span> <span class="o">+</span> <span class="n">voxel_id</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>

  <span class="c1">// If the current number of points in the voxel exceeds the maximum allowed points per voxel, we return.</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">current_num</span> <span class="o">&gt;=</span> <span class="n">max_points_per_voxel</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="c1">// Now we can proceed to add the current point to the voxel's feature list.</span>
  <span class="c1">// Calculate the destination offset where the point's features will be stored in the voxels_temp array.</span>
  <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">dst_offset</span> <span class="o">=</span> <span class="n">voxel_id</span> <span class="o">*</span> <span class="p">(</span><span class="n">feature_num</span> <span class="o">*</span> <span class="n">max_points_per_voxel</span><span class="p">)</span> <span class="o">+</span> <span class="n">current_num</span> <span class="o">*</span> <span class="n">feature_num</span><span class="p">;</span>
  <span class="c1">// Calculate the source offset of the current point's features in the points array.</span>
  <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">src_offset</span> <span class="o">=</span> <span class="n">point_idx</span> <span class="o">*</span> <span class="n">feature_num</span><span class="p">;</span>
  
  <span class="c1">// Copy the point's features from the points array to the corresponding location in the voxels_temp array.</span>
  <span class="c1">// This effectively adds the point's features to the voxel's feature list.</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">feature_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">feature_idx</span> <span class="o">&lt;</span> <span class="n">feature_num</span><span class="p">;</span> <span class="o">++</span><span class="n">feature_idx</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">voxels_temp</span><span class="p">[</span><span class="n">dst_offset</span> <span class="o">+</span> <span class="n">feature_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">src_offset</span> <span class="o">+</span> <span class="n">feature_idx</span><span class="p">];</span>
  <span class="p">}</span>
  
  <span class="c1">// Store additional information about the voxel (its indices along X, Y, Z axes) for later processing.</span>
  <span class="n">uint4</span> <span class="n">idx</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="n">voxel_idz</span><span class="p">,</span> <span class="n">voxel_idy</span><span class="p">,</span> <span class="n">voxel_idx</span><span class="p">};</span>
  <span class="p">((</span><span class="n">uint4</span> <span class="o">*</span><span class="p">)</span><span class="n">voxel_indices</span><span class="p">)[</span><span class="n">voxel_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span><span class="p">;</span>

<span class="p">}</span>
</code></pre></div></div>

<p>Explanation:</p>

<p>Here’s how the <code class="language-plaintext highlighter-rouge">voxelizationKernel</code> works:</p>

<ol>
  <li>
    <p><code class="language-plaintext highlighter-rouge">int point_idx = blockIdx.x * blockDim.x + threadIdx.x;</code>: This line calculates the index of the current point to be processed by the CUDA thread.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">if (point_idx &gt;= points_size) { return; }</code>: This condition checks if the thread index is out of bounds, i.e., beyond the number of points in the input points array. If so, the thread returns early to avoid processing invalid data.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">float px = points[feature_num * point_idx]; float py = points[feature_num * point_idx + 1]; float pz = points[feature_num * point_idx + 2];</code>: These lines extract the X, Y, and Z coordinates of the current point from the input points array based on the <code class="language-plaintext highlighter-rouge">feature_num</code> (the number of features per point).</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">if (px &lt; min_x_range || px &gt;= max_x_range || py &lt; min_y_range || py &gt;= max_y_range || pz &lt; min_z_range || pz &gt;= max_z_range) { return; }</code>: This condition checks if the current point lies within the specified 3D range (min/max X, Y, Z). If the point is outside this range, it is not considered for voxelization, and the thread returns early.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">unsigned int voxel_idx = floorf((px - min_x_range) / voxel_x_size); unsigned int voxel_idy = floorf((py - min_y_range) / voxel_y_size); unsigned int voxel_idz = floorf((pz - min_z_range) / voxel_z_size);</code>: These lines calculate the voxel coordinates (<code class="language-plaintext highlighter-rouge">voxel_idx</code>, <code class="language-plaintext highlighter-rouge">voxel_idy</code>, <code class="language-plaintext highlighter-rouge">voxel_idz</code>) corresponding to the current point’s X, Y, and Z coordinates based on the specified voxel sizes and ranges.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">unsigned int voxel_offset = voxel_idz * grid_y_size * grid_x_size + voxel_idy * grid_x_size + voxel_idx;</code>: This line calculates the voxel offset based on the voxel coordinates. The voxel offset is a unique identifier for each voxel within the 3D grid.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">unsigned int voxel_id = lookupHashTable(voxel_offset, points_size * 2 * 2, hash_table);</code>: This line calls the <code class="language-plaintext highlighter-rouge">lookupHashTable</code> function to find the corresponding voxel ID for the current voxel offset using the hash table.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">if (voxel_id &gt;= max_voxels) { return; }</code>: This condition checks if the current voxel ID is greater than or equal to <code class="language-plaintext highlighter-rouge">max_voxels</code>, indicating that the maximum number of allowed voxels has been reached. If so, the thread returns early.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">unsigned int current_num = atomicAdd(num_points_per_voxel + voxel_id, 1);</code>: This line uses the <code class="language-plaintext highlighter-rouge">atomicAdd</code> function to atomically increment the number of points in the voxel represented by <code class="language-plaintext highlighter-rouge">voxel_id</code> in the <code class="language-plaintext highlighter-rouge">num_points_per_voxel</code> array.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">if (current_num &lt; max_points_per_voxel) { ... }</code>: This condition checks if the current number of points in the voxel is less than the maximum allowed per voxel. If so, the thread proceeds to add the current point’s features to the voxel.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">unsigned int dst_offset = voxel_id * (feature_num * max_points_per_voxel) + current_num * feature_num; unsigned int src_offset = point_idx * feature_num;</code>: These lines calculate the offsets for copying the current point’s features to the voxel in the <code class="language-plaintext highlighter-rouge">voxels_temp</code> array.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">for (int feature_idx = 0; feature_idx &lt; feature_num; ++feature_idx) { voxels_temp[dst_offset + feature_idx] = points[src_offset + feature_idx]; }</code>: This loop copies the features of the current point to the appropriate location in the <code class="language-plaintext highlighter-rouge">voxels_temp</code> array, effectively adding the point to the voxel.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">uint4 idx = {0, voxel_idz, voxel_idy, voxel_idx}; ((uint4 *)voxel_indices)[voxel_id] = idx;</code>: These lines create an index vector (<code class="language-plaintext highlighter-rouge">idx</code>) containing information about the voxel’s position in the grid and store it in the <code class="language-plaintext highlighter-rouge">voxel_indices</code> array at the location corresponding to <code class="language-plaintext highlighter-rouge">voxel_id</code>. This allows quick lookup of voxel information during subsequent processing.</p>
  </li>
</ol>

<p>The <code class="language-plaintext highlighter-rouge">voxelizationKernel</code> efficiently assigns each point to its corresponding voxel, ensuring that points are appropriately added to the voxel’s feature list without exceeding the maximum allowed points per voxel.</p>

<p><strong>5. Voxelization Launch</strong></p>

<p>The <code class="language-plaintext highlighter-rouge">voxelizationLaunch</code> function is a key step in the voxelization process. It is responsible for launching two CUDA kernels: <code class="language-plaintext highlighter-rouge">buildHashKernel</code> and <code class="language-plaintext highlighter-rouge">voxelizationKernel</code>. Let’s break down the function and its components:</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cudaError_t</span> <span class="nf">voxelizationLaunch</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">points</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">points_size</span><span class="p">,</span> <span class="kt">float</span> <span class="n">min_x_range</span><span class="p">,</span> <span class="kt">float</span> <span class="n">max_x_range</span><span class="p">,</span>
                          <span class="kt">float</span> <span class="n">min_y_range</span><span class="p">,</span> <span class="kt">float</span> <span class="n">max_y_range</span><span class="p">,</span><span class="kt">float</span> <span class="n">min_z_range</span><span class="p">,</span> <span class="kt">float</span> <span class="n">max_z_range</span><span class="p">,</span>
                          <span class="kt">float</span> <span class="n">voxel_x_size</span><span class="p">,</span> <span class="kt">float</span> <span class="n">voxel_y_size</span><span class="p">,</span> <span class="kt">float</span> <span class="n">voxel_z_size</span><span class="p">,</span> <span class="kt">int</span> <span class="n">grid_y_size</span><span class="p">,</span>
                          <span class="kt">int</span> <span class="n">grid_x_size</span><span class="p">,</span> <span class="kt">int</span> <span class="n">feature_num</span><span class="p">,</span> <span class="kt">int</span> <span class="n">max_voxels</span><span class="p">,</span><span class="kt">int</span> <span class="n">max_points_per_voxel</span><span class="p">,</span>
                          <span class="kt">unsigned</span> <span class="kt">int</span> <span class="o">*</span><span class="n">hash_table</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="o">*</span><span class="n">num_points_per_voxel</span><span class="p">,</span><span class="kt">float</span> <span class="o">*</span><span class="n">voxel_features</span><span class="p">,</span> 
                          <span class="kt">unsigned</span> <span class="kt">int</span> <span class="o">*</span><span class="n">voxel_indices</span><span class="p">,</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="o">*</span><span class="n">real_voxel_num</span><span class="p">,</span> <span class="n">cudaStream_t</span> <span class="n">stream</span><span class="p">)</span> 
      <span class="p">{</span>
    <span class="c1">// how many threads in each block</span>
    <span class="kt">int</span> <span class="n">threadNum</span> <span class="o">=</span> <span class="n">THREADS_FOR_VOXEL</span><span class="p">;</span>
    <span class="c1">// how many blocks needed if each point gets on thread.</span>
    <span class="n">dim3</span> <span class="n">blocks</span><span class="p">((</span><span class="n">points_size</span><span class="o">+</span><span class="n">threadNum</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">threadNum</span><span class="p">);</span>
    <span class="c1">// how many threads in each block</span>
    <span class="n">dim3</span> <span class="n">threads</span><span class="p">(</span><span class="n">threadNum</span><span class="p">);</span>
    <span class="c1">// how many blocks needed to launch the kernel, how many threads in each block,</span>
    <span class="c1">// how many bytes for dynamic shared memory  ( zero here), cuda stream</span>
    <span class="n">buildHashKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span> <span class="n">threads</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">stream</span><span class="o">&gt;&gt;&gt;</span>
      <span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">points_size</span><span class="p">,</span>
          <span class="n">min_x_range</span><span class="p">,</span> <span class="n">max_x_range</span><span class="p">,</span>
          <span class="n">min_y_range</span><span class="p">,</span> <span class="n">max_y_range</span><span class="p">,</span>
          <span class="n">min_z_range</span><span class="p">,</span> <span class="n">max_z_range</span><span class="p">,</span>
          <span class="n">voxel_x_size</span><span class="p">,</span> <span class="n">voxel_y_size</span><span class="p">,</span> <span class="n">voxel_z_size</span><span class="p">,</span>
          <span class="n">grid_y_size</span><span class="p">,</span> <span class="n">grid_x_size</span><span class="p">,</span> <span class="n">feature_num</span><span class="p">,</span> <span class="n">hash_table</span><span class="p">,</span>
    <span class="n">real_voxel_num</span><span class="p">);</span>
    <span class="n">voxelizationKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span> <span class="n">threads</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">stream</span><span class="o">&gt;&gt;&gt;</span>
      <span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">points_size</span><span class="p">,</span>
          <span class="n">min_x_range</span><span class="p">,</span> <span class="n">max_x_range</span><span class="p">,</span>
          <span class="n">min_y_range</span><span class="p">,</span> <span class="n">max_y_range</span><span class="p">,</span>
          <span class="n">min_z_range</span><span class="p">,</span> <span class="n">max_z_range</span><span class="p">,</span>
          <span class="n">voxel_x_size</span><span class="p">,</span> <span class="n">voxel_y_size</span><span class="p">,</span> <span class="n">voxel_z_size</span><span class="p">,</span>
          <span class="n">grid_y_size</span><span class="p">,</span> <span class="n">grid_x_size</span><span class="p">,</span> <span class="n">feature_num</span><span class="p">,</span> <span class="n">max_voxels</span><span class="p">,</span>
          <span class="n">max_points_per_voxel</span><span class="p">,</span> <span class="n">hash_table</span><span class="p">,</span>
    <span class="n">num_points_per_voxel</span><span class="p">,</span> <span class="n">voxel_features</span><span class="p">,</span> <span class="n">voxel_indices</span><span class="p">,</span> <span class="n">real_voxel_num</span><span class="p">);</span>
    <span class="n">cudaError_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">cudaGetLastError</span><span class="p">();</span>
    <span class="k">return</span> <span class="n">err</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>
<p>Explanation:
Here’s how the <code class="language-plaintext highlighter-rouge">voxelizationLaunch</code> function works:</p>

<ol>
  <li>
    <p><code class="language-plaintext highlighter-rouge">int threadNum = THREADS_FOR_VOXEL;</code>: This line sets the number of threads per block for the CUDA kernel. The value is obtained from the constant <code class="language-plaintext highlighter-rouge">THREADS_FOR_VOXEL</code>, which likely represents an optimal number of threads for efficient computation.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">dim3 blocks((points_size+threadNum-1)/threadNum);</code>: This line calculates the number of blocks needed to launch the kernel based on the total number of points (<code class="language-plaintext highlighter-rouge">points_size</code>) and the <code class="language-plaintext highlighter-rouge">threadNum</code>. It ensures that all points are processed by the threads efficiently.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">dim3 threads(threadNum);</code>: This line sets the number of threads in each block based on the previously calculated <code class="language-plaintext highlighter-rouge">threadNum</code>.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">buildHashKernel&lt;&lt;&lt;blocks, threads, 0, stream&gt;&gt;&gt;(...)</code>: This line launches the <code class="language-plaintext highlighter-rouge">buildHashKernel</code> CUDA kernel. It processes the input points to build the hash table, which maps voxel offsets to voxel IDs.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">voxelizationKernel&lt;&lt;&lt;blocks, threads, 0, stream&gt;&gt;&gt;(...)</code>: This line launches the <code class="language-plaintext highlighter-rouge">voxelizationKernel</code> CUDA kernel. It voxelizes the input points based on the computed hash table, assigning points to corresponding voxels and storing the voxel features.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">cudaError_t err = cudaGetLastError(); return err;</code>: These lines check for any errors that occurred during kernel launches. If there are any errors, they will be returned by the function, indicating a problem in the GPU computation.</p>
  </li>
</ol>

<p>The <code class="language-plaintext highlighter-rouge">voxelizationLaunch</code> function serves as the entry point to initiate the voxelization process on the GPU. It efficiently divides the data into blocks and threads, launches the necessary CUDA kernels (<code class="language-plaintext highlighter-rouge">buildHashKernel</code> and <code class="language-plaintext highlighter-rouge">voxelizationKernel</code>), and checks for any errors in the GPU computation. By effectively utilizing the GPU’s parallel processing capabilities, voxelization of large point clouds can be done efficiently and quickly.</p>

<p>Overall, the <code class="language-plaintext highlighter-rouge">voxelizationLaunch</code> function is a crucial step in the voxelization process, coordinating the parallel execution of the CUDA kernels to efficiently process and voxelate the input point cloud data.</p>

<p><strong>6. Feature Extraction Kernel</strong></p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">featureExtractionKernel</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">voxels_temp</span><span class="p">,</span>
                                        <span class="kt">unsigned</span> <span class="kt">int</span> <span class="o">*</span><span class="n">num_points_per_voxel</span><span class="p">,</span>
                                        <span class="kt">int</span> <span class="n">max_points_per_voxel</span><span class="p">,</span> <span class="kt">int</span> <span class="n">feature_num</span><span class="p">,</span> <span class="n">half</span> <span class="o">*</span><span class="n">voxel_features</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">voxel_idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

    <span class="n">num_points_per_voxel</span><span class="p">[</span><span class="n">voxel_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_points_per_voxel</span><span class="p">[</span><span class="n">voxel_idx</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">max_points_per_voxel</span> <span class="o">?</span>
                                      <span class="n">max_points_per_voxel</span> <span class="o">:</span> <span class="n">num_points_per_voxel</span><span class="p">[</span><span class="n">voxel_idx</span><span class="p">];</span>

    <span class="kt">int</span> <span class="n">valid_points_num</span> <span class="o">=</span> <span class="n">num_points_per_voxel</span><span class="p">[</span><span class="n">voxel_idx</span><span class="p">];</span>

    <span class="kt">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">voxel_idx</span> <span class="o">*</span> <span class="n">max_points_per_voxel</span> <span class="o">*</span> <span class="n">feature_num</span><span class="p">;</span>

    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">feature_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">feature_idx</span> <span class="o">&lt;</span> <span class="n">feature_num</span><span class="p">;</span> <span class="o">++</span><span class="n">feature_idx</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">point_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">point_idx</span> <span class="o">&lt;</span> <span class="n">valid_points_num</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="o">++</span><span class="n">point_idx</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">voxels_temp</span><span class="p">[</span><span class="n">offset</span> <span class="o">+</span> <span class="n">feature_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">voxels_temp</span><span class="p">[</span><span class="n">offset</span> <span class="o">+</span> <span class="p">(</span><span class="n">point_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">feature_num</span> <span class="o">+</span> <span class="n">feature_idx</span><span class="p">];</span>
        <span class="p">}</span>
        <span class="n">voxels_temp</span><span class="p">[</span><span class="n">offset</span> <span class="o">+</span> <span class="n">feature_idx</span><span class="p">]</span> <span class="o">/=</span> <span class="n">valid_points_num</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">feature_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">feature_idx</span> <span class="o">&lt;</span> <span class="n">feature_num</span><span class="p">;</span> <span class="o">++</span><span class="n">feature_idx</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">dst_offset</span> <span class="o">=</span> <span class="n">voxel_idx</span> <span class="o">*</span> <span class="n">feature_num</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">src_offset</span> <span class="o">=</span> <span class="n">voxel_idx</span> <span class="o">*</span> <span class="n">feature_num</span> <span class="o">*</span> <span class="n">max_points_per_voxel</span><span class="p">;</span>
        <span class="n">voxel_features</span><span class="p">[</span><span class="n">dst_offset</span> <span class="o">+</span> <span class="n">feature_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">__float2half</span><span class="p">(</span><span class="n">voxels_temp</span><span class="p">[</span><span class="n">src_offset</span> <span class="o">+</span> <span class="n">feature_idx</span><span class="p">]);</span>
    <span class="p">}</span>
<span class="p">}</span>

</code></pre></div></div>

<p>Here’s how the <code class="language-plaintext highlighter-rouge">featureExtractionKernel</code> works:</p>

<ol>
  <li>
    <p><code class="language-plaintext highlighter-rouge">int voxel_idx = blockIdx.x * blockDim.x + threadIdx.x;</code>: This line calculates the index of the current voxel to be processed by the CUDA thread. Each CUDA thread corresponds to one voxel, and the <code class="language-plaintext highlighter-rouge">voxel_idx</code> uniquely identifies the voxel.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">num_points_per_voxel[voxel_idx] = num_points_per_voxel[voxel_idx] &gt; max_points_per_voxel ? max_points_per_voxel : num_points_per_voxel[voxel_idx];</code>: This line checks if the number of points in the current voxel exceeds the <code class="language-plaintext highlighter-rouge">max_points_per_voxel</code>. If it does, it clips the value to ensure that the feature extraction is performed on a maximum of <code class="language-plaintext highlighter-rouge">max_points_per_voxel</code> points for each voxel.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">int valid_points_num = num_points_per_voxel[voxel_idx];</code>: This line retrieves the actual number of valid points in the current voxel, which may have been clipped in the previous step.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">int offset = voxel_idx * max_points_per_voxel * feature_num;</code>: This line calculates the offset for accessing the voxel’s features in the <code class="language-plaintext highlighter-rouge">voxels_temp</code> array. It represents the index from which the current voxel’s features start in the <code class="language-plaintext highlighter-rouge">voxels_temp</code> array.</p>
  </li>
  <li>
    <p>The goal of feature extraction is to take the average for each feature (x, y, z, intensity, time) of every point in the voxel. The next few lines of code achieve this by iterating over each feature and each point in the voxel, summing up the feature values, and then dividing the sum by the number of valid points to obtain the average value.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">for (int feature_idx = 0; feature_idx &lt; feature_num; ++feature_idx) { ... }</code>: This loop iterates over each feature (x, y, z, intensity, time) and calculates the average value for that feature in the current voxel.</p>
  </li>
  <li>
    <p>The next loop within the feature extraction loop iterates over each point in the voxel (<code class="language-plaintext highlighter-rouge">point_idx</code>), starting from the second point (index 1) since the first point’s feature values were already added to <code class="language-plaintext highlighter-rouge">voxels_temp</code>.</p>
  </li>
  <li>
    <p>The feature values of each point are added to the corresponding feature in <code class="language-plaintext highlighter-rouge">voxels_temp</code>. After the loop, <code class="language-plaintext highlighter-rouge">voxels_temp[offset + feature_idx]</code> contains the sum of feature values for all points in the current voxel for the given feature.</p>
  </li>
  <li>
    <p>Finally, the sum for each feature is divided by the <code class="language-plaintext highlighter-rouge">valid_points_num</code> to calculate the average feature value for the current voxel. This average feature value is then stored in <code class="language-plaintext highlighter-rouge">voxels_temp</code> in the same location where the sum was stored earlier.</p>
  </li>
  <li>
    <p>The next loop moves the averaged voxel features from <code class="language-plaintext highlighter-rouge">voxels_temp</code> to the <code class="language-plaintext highlighter-rouge">voxel_features</code> array, ensuring that the features for each voxel are stored contiguously in <code class="language-plaintext highlighter-rouge">voxel_features</code>. The features are converted to the “half” data type (<code class="language-plaintext highlighter-rouge">__float2half</code>) for memory efficiency.</p>
  </li>
</ol>

<p>In summary, the <code class="language-plaintext highlighter-rouge">featureExtractionKernel</code> takes each voxel represented by a CUDA thread and calculates the average feature values (x, y, z, intensity, time) for all valid points in that voxel. The averaged voxel features are then stored in the <code class="language-plaintext highlighter-rouge">voxel_features</code> array, which represents the final output of the feature extraction process. The GPU’s parallel processing capabilities are utilized to efficiently perform this feature extraction on multiple voxels simultaneously, speeding up the overall computation for large point clouds.</p>

<p><strong>7. Feature Extraction Launch</strong></p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">cudaError_t</span> <span class="nf">featureExtractionLaunch</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">voxels_temp</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="o">*</span><span class="n">num_points_per_voxel</span><span class="p">,</span>
        <span class="k">const</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">real_voxel_num</span><span class="p">,</span> <span class="kt">int</span> <span class="n">max_points_per_voxel</span><span class="p">,</span> <span class="kt">int</span> <span class="n">feature_num</span><span class="p">,</span>
	<span class="n">half</span> <span class="o">*</span><span class="n">voxel_features</span><span class="p">,</span> <span class="n">cudaStream_t</span> <span class="n">stream</span><span class="p">)</span>
<span class="p">{</span>
  <span class="kt">int</span> <span class="n">threadNum</span> <span class="o">=</span> <span class="n">THREADS_FOR_VOXEL</span><span class="p">;</span>
  <span class="n">dim3</span> <span class="n">blocks</span><span class="p">((</span><span class="n">real_voxel_num</span> <span class="o">+</span> <span class="n">threadNum</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">threadNum</span><span class="p">);</span>
  <span class="n">dim3</span> <span class="n">threads</span><span class="p">(</span><span class="n">threadNum</span><span class="p">);</span>
  <span class="n">featureExtractionKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span> <span class="n">threads</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">stream</span><span class="o">&gt;&gt;&gt;</span>
    <span class="p">(</span><span class="n">voxels_temp</span><span class="p">,</span> <span class="n">num_points_per_voxel</span><span class="p">,</span>
        <span class="n">max_points_per_voxel</span><span class="p">,</span> <span class="n">feature_num</span><span class="p">,</span> <span class="n">voxel_features</span><span class="p">);</span>
  <span class="n">cudaError_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">cudaGetLastError</span><span class="p">();</span>
  <span class="k">return</span> <span class="n">err</span><span class="p">;</span>
<span class="p">}</span>

</code></pre></div></div>
<p>The featureExtractionLaunch function is the launch function for the featureExtractionKernel, responsible for processing voxel data and extracting features. It takes the necessary input arrays, determines the block and thread configuration based on the number of voxels, launches the kernel, and captures any CUDA errors that might occur during execution.</p>

<p><strong>8.Generate Voxels</strong></p>

<p>This function in the <code class="language-plaintext highlighter-rouge">VoxelizerGPU.cpp</code> file is responsible for performing voxelization and feature extraction on a set of input points using CUDA on the GPU. Here’s how it works:</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">VoxelizerGPU</span><span class="o">::</span><span class="n">generateVoxels</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">points</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">points_size</span><span class="p">,</span> <span class="n">cudaStream_t</span> <span class="n">stream</span><span class="p">)</span>
<span class="p">{</span>
    <span class="c1">// flash memory for every run </span>
    <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaMemsetAsync</span><span class="p">(</span><span class="n">hash_table_</span><span class="p">,</span> <span class="mh">0xff</span><span class="p">,</span> <span class="n">hash_table_size_</span><span class="p">,</span> <span class="n">stream</span><span class="p">));</span>
    <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaMemsetAsync</span><span class="p">(</span><span class="n">voxels_temp_</span><span class="p">,</span> <span class="mh">0xff</span><span class="p">,</span> <span class="n">voxels_temp_size_</span><span class="p">,</span> <span class="n">stream</span><span class="p">));</span>

    <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaMemsetAsync</span><span class="p">(</span><span class="n">d_voxel_num_</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">voxel_num_size_</span><span class="p">,</span> <span class="n">stream</span><span class="p">));</span>
    <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaMemsetAsync</span><span class="p">(</span><span class="n">d_real_num_voxels_</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span><span class="p">),</span> <span class="n">stream</span><span class="p">));</span>
    <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaStreamSynchronize</span><span class="p">(</span><span class="n">stream</span><span class="p">));</span>

    <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">voxelizationLaunch</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">points_size</span><span class="p">,</span>
          <span class="n">params_</span><span class="p">.</span><span class="n">min_x_range</span><span class="p">,</span> <span class="n">params_</span><span class="p">.</span><span class="n">max_x_range</span><span class="p">,</span>
          <span class="n">params_</span><span class="p">.</span><span class="n">min_y_range</span><span class="p">,</span> <span class="n">params_</span><span class="p">.</span><span class="n">max_y_range</span><span class="p">,</span>
          <span class="n">params_</span><span class="p">.</span><span class="n">min_z_range</span><span class="p">,</span> <span class="n">params_</span><span class="p">.</span><span class="n">max_z_range</span><span class="p">,</span>
          <span class="n">params_</span><span class="p">.</span><span class="n">pillar_x_size</span><span class="p">,</span> <span class="n">params_</span><span class="p">.</span><span class="n">pillar_y_size</span><span class="p">,</span> <span class="n">params_</span><span class="p">.</span><span class="n">pillar_z_size</span><span class="p">,</span>
          <span class="n">params_</span><span class="p">.</span><span class="n">getGridYSize</span><span class="p">(),</span> <span class="n">params_</span><span class="p">.</span><span class="n">getGridXSize</span><span class="p">(),</span> <span class="n">params_</span><span class="p">.</span><span class="n">feature_num</span><span class="p">,</span> <span class="n">params_</span><span class="p">.</span><span class="n">max_voxels</span><span class="p">,</span>
          <span class="n">params_</span><span class="p">.</span><span class="n">max_points_per_voxel</span><span class="p">,</span> <span class="n">hash_table_</span><span class="p">,</span>
    <span class="n">d_voxel_num_</span><span class="p">,</span> <span class="cm">/*d_voxel_features_*/</span><span class="n">voxels_temp_</span><span class="p">,</span> <span class="n">d_voxel_indices_</span><span class="p">,</span>
    <span class="n">d_real_num_voxels_</span><span class="p">,</span> <span class="n">stream</span><span class="p">));</span>
    <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaMemcpyAsync</span><span class="p">(</span><span class="n">h_real_num_voxels_</span><span class="p">,</span> <span class="n">d_real_num_voxels_</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">,</span> <span class="n">stream</span><span class="p">));</span>
    <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaStreamSynchronize</span><span class="p">(</span><span class="n">stream</span><span class="p">));</span>

    <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">featureExtractionLaunch</span><span class="p">(</span><span class="n">voxels_temp_</span><span class="p">,</span> <span class="n">d_voxel_num_</span><span class="p">,</span>
          <span class="o">*</span><span class="n">h_real_num_voxels_</span><span class="p">,</span> <span class="n">params_</span><span class="p">.</span><span class="n">max_points_per_voxel</span><span class="p">,</span> <span class="n">params_</span><span class="p">.</span><span class="n">feature_num</span><span class="p">,</span>
    <span class="n">d_voxel_features_</span><span class="p">,</span> <span class="n">stream</span><span class="p">));</span>

    <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaStreamSynchronize</span><span class="p">(</span><span class="n">stream</span><span class="p">));</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<ol>
  <li>
    <p>Memory Initialization: The function starts by clearing the flash memory for each run. It uses <code class="language-plaintext highlighter-rouge">cudaMemsetAsync</code> to set the <code class="language-plaintext highlighter-rouge">hash_table_</code> and <code class="language-plaintext highlighter-rouge">voxels_temp_</code> memory regions to 0xFF asynchronously. It also sets the <code class="language-plaintext highlighter-rouge">d_voxel_num_</code> and <code class="language-plaintext highlighter-rouge">d_real_num_voxels_</code> memory regions to 0 asynchronously.</p>
  </li>
  <li>
    <p>Voxelization: Next, the function calls <code class="language-plaintext highlighter-rouge">voxelizationLaunch</code> with the input <code class="language-plaintext highlighter-rouge">points</code> array and various parameters such as <code class="language-plaintext highlighter-rouge">min_x_range</code>, <code class="language-plaintext highlighter-rouge">max_x_range</code>, <code class="language-plaintext highlighter-rouge">pillar_x_size</code>, <code class="language-plaintext highlighter-rouge">max_voxels</code>, etc. This function performs voxelization on the input points, generates a hash table to map voxel offsets to voxel IDs, and records the number of points per voxel in <code class="language-plaintext highlighter-rouge">d_voxel_num_</code>. The result is stored in <code class="language-plaintext highlighter-rouge">voxels_temp_</code>.</p>
  </li>
  <li>
    <p>Synchronization: After voxelization, the function synchronizes the CUDA stream to ensure that the previous kernel launch and memory operations are completed before proceeding.</p>
  </li>
  <li>
    <p>Feature Extraction: The function then calls <code class="language-plaintext highlighter-rouge">featureExtractionLaunch</code> with <code class="language-plaintext highlighter-rouge">voxels_temp_</code> and other related parameters. This function calculates the average of each feature (x, y, z, intensity, t) for each voxel and stores the results in the <code class="language-plaintext highlighter-rouge">d_voxel_features_</code> memory region using the <code class="language-plaintext highlighter-rouge">d_voxel_num_</code> information.</p>
  </li>
  <li>
    <p>Final Synchronization: Lastly, the function synchronizes the CUDA stream again to ensure all computations are completed, and then it returns 0 to indicate successful execution.</p>
  </li>
</ol>

<p>Overall, this function efficiently processes a large number of points by leveraging the parallel processing power of the GPU, leading to faster voxelization and feature extraction, crucial steps in point cloud processing and 3D data analysis.</p>

<p>This completes the code deep dive of all crucial components of performing Voxelization on GPU using CUDA Kernels. Now, in the next section, we will get some insights on the speed improvements provided by the GPU by comparing it with just CPU based Voxelization.</p>

<h2 id="5-conclusion">5. Conclusion</h2>

<p>We have already looked at how to make the executable perform Voxelization on both GPU and CPU. But let us revisit the command.
By default the executable will just perform CUDA based Voxelization on GPU. To also perform voxelization
on the CPU, execute with the <code class="language-plaintext highlighter-rouge">--cpu</code> flag.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./voxelize_cuda ../data/test/ <span class="nt">--cpu</span> 
</code></pre></div></div>

<p>The output of this command will looks something like this :</p>

<p>You can expect an output similar to this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>GPU has cuda devices: 1
----device id: 0 info----
  GPU : GeForce RTX 2060 
  Capbility: 7.5
  Global memory: 5912MB
  Const memory: 64KB
  SM in a block: 48KB
  warp size: 32
  threads in a block: 1024
  block dim: (1024,1024,64)
  grid dim: (2147483647,65535,65535)
-------------------------

Total 10
Average GPU Voxelization Time : 0.643269
Average CPU Voxelization Time : 374.432
Average GPU vs CPU Speedup : 582.076x times 

</code></pre></div></div>

<div style="width: 100%;margin: 0 auto;">
<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_2/elon.gif-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_2/elon.gif-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_2/elon.gif-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_2/elon.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
What getting 580x times speedup feels like.
</div>
</div>

<p>Let than sink in for a moment. The GPU-based Voxelization, driven by the incredible CUDA programming, has showcased an astronomical performance boost of over 580 times compared to the traditional CPU approach. It’s like witnessing a swift and majestic leap from a snail’s pace to warp speed travel. The difference is staggering. To put this into perspective, if this process took 1 second for the GPU using CUDA programming, it would take around 10 minutes for the same process on the CPU. This is what 580x time means.</p>

<p>The potential of CUDA programming is awe-inspiring and goes far beyond just voxelization. These concepts open doors to a world of possibilities in diverse fields. Whether it’s 3D detection, medical imaging, simulations, or artificial intelligence, CUDA unleashes a new dimension of coding brilliance.</p>

<p>So, fasten your seatbelts as you venture into the thrilling realm of CUDA programming. Embrace the power and speed it offers, and watch your code transform into a force to be reckoned with. Sure, the journey might have its challenges, but the jaw-dropping speedup you’ll achieve is absolutely worth the effort.</p>

<p>With CUDA, the sky is no longer the limit; it’s just the beginning. So go forth and conquer the universe of parallel programming! 🚀💻</p>

<p>That concludes our exhilarating exploration of GPU-based Voxelization using CUDA. Happy coding, and may your adventures in parallel programming be as thrilling as this one!</p>]]></content><author><name></name></author><category term="cuda" /><category term="nvidia" /><category term="cuda" /><summary type="html"><![CDATA[Voxelization using CUDA programming]]></summary></entry><entry><title type="html">Have you met TensorRT?</title><link href="https://sanket-pixel.github.io//blog/2023/introduction-to-tensorrt/" rel="alternate" type="text/html" title="Have you met TensorRT?" /><published>2023-07-12T19:53:00+00:00</published><updated>2023-07-12T19:53:00+00:00</updated><id>https://sanket-pixel.github.io//blog/2023/introduction-to-tensorrt</id><content type="html" xml:base="https://sanket-pixel.github.io//blog/2023/introduction-to-tensorrt/"><![CDATA[<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_1/main_photo-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_1/main_photo-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_1/main_photo-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_1/main_photo.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="have you met" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Have you met TensorRT?
</div>

<h5 id="welcome-to-our-blog-where-we-explore-the-fascinating-world-of-tensorrta-powerful-tool-that-unleashes-unparalleled-speed-and-efficiency-in-ai-in-this-first-part-well-take-a-glimpse-into-the-extraordinary-capabilities-of-tensorrt-and-its-impact-on-deep-learning-imagine-a-magical-optimization-framework-that-enhances-ai-models-enabling-them-to-process-data-at-lightning-speed-without-compromising-accuracy-its-like-giving-ai-a-boost-of-superpowers-making-it-faster-smarter-and-more-efficient-join-us-on-this-captivating-journey-as-we-uncover-the-wonders-of-tensorrt-and-its-potential-to-revolutionize-the-field-of-artificial-intelligence">Welcome to our blog, where we explore the fascinating world of TensorRT—a powerful tool that unleashes unparalleled speed and efficiency in AI. In this first part, we’ll take a glimpse into the extraordinary capabilities of TensorRT and its impact on deep learning. Imagine a magical optimization framework that enhances AI models, enabling them to process data at lightning speed without compromising accuracy. It’s like giving AI a boost of superpowers, making it faster, smarter, and more efficient. Join us on this captivating journey as we uncover the wonders of TensorRT and its potential to revolutionize the field of artificial intelligence.</h5>

<p><br /></p>

<h3 id="source-from-github">Source from Github</h3>
<p>For those interested in exploring the code and gaining a deeper understanding of the concepts discussed in this blog on TensorRT and image classification, you can find the complete source code in the corresponding GitHub repository. The repository link is <a href="https://github.com/sanket-pixel/tensorrt_deeploy">this</a>.
, which houses an array of edge AI blogs and their source code for further exploration.</p>

<p>In particular, the source code for this specific blog, covering the fundamentals of TensorRT, image classification with PyTorch, ONNX conversion, TensorRT engine generation, and inference speedup measurement, is available in the notebook found <a href="https://github.com/sanket-pixel/tensorrt_deeploy/blob/main/1_fundamentals/python/1_fundamentals.ipynb">here</a>. By delving into this notebook, you can follow along with the step-by-step implementations and gain hands-on experience in harnessing the power of TensorRT for edge AI applications. Happy coding and exploring the realms of accelerated AI with PyTorch and TensorRT!</p>

<p><br /></p>

<h3 id="pre-requisites-and-installation">Pre-requisites and Installation</h3>
<h5 id="1-hardware-requirements">1. Hardware requirements</h5>
<ul>
  <li>NVIDIA GPU</li>
</ul>

<h5 id="2-software-requirements">2. Software requirements</h5>
<ul>
  <li>Ubuntu &gt;= 18.04</li>
  <li>Python &gt;= 3.8</li>
</ul>

<h5 id="3-installation-guide">3. Installation Guide</h5>
<ol>
  <li>Create conda environment and install required python packages.
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create -n trt python=3.8
conda activate trt
pip install -r requirements.txt
</code></pre></div>    </div>
  </li>
  <li>Install TensorRT 
Install TensorRT:</li>
</ol>

<ul>
  <li>
    <p>Download and install NVIDIA CUDA 11.4 or later following the official instructions: <a href="https://developer.nvidia.com/cuda-toolkit-archive">link</a></p>
  </li>
  <li>
    <p>Download and extract CuDNN library for your CUDA version (&gt;8.9.0) from: <a href="https://developer.nvidia.com/cudnn">link</a></p>
  </li>
  <li>
    <p>Download and extract NVIDIA TensorRT library for your CUDA version from: <a href="https://developer.nvidia.com/nvidia-tensorrt-8x-download">link</a>. Minimum required version is 8.5. Follow the Installation Guide for your system and ensure Python’s part is installed.</p>
  </li>
  <li>
    <p>Add the absolute path to CUDA, TensorRT, and CuDNN libs to the environment variable PATH or LD_LIBRARY_PATH.</p>
  </li>
  <li>
    <p>Install PyCUDA:</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install pycuda
</code></pre></div>    </div>
    <p><br /></p>
  </li>
</ul>

<h3 id="introduction">Introduction</h3>

<p>TensorRT is an optimization framework designed to accelerate AI inference, making it faster and more efficient. Think of it as a performance booster for AI models, enhancing their capabilities without compromising accuracy. Imagine you have a collection of handwritten letters, each representing a unique story. You wish to organize and analyze these letters, but they are scattered and unstructured. That’s when you bring in a talented editor who transforms these letters into a beautifully composed novel, ready to be read and understood.</p>

<p>In this analogy, the handwritten letters represent a PyTorch model—an impressive piece of work but lacking the efficiency needed for real-time inference. The editor symbolizes TensorRT, refining the model and optimizing it to perform with lightning-fast speed and accuracy. Similar to how the editor transforms the letters into a coherent novel, TensorRT takes the PyTorch model and enhances it, making it highly efficient and ready to tackle complex tasks in a fraction of the time. With TensorRT’s optimization techniques, just as the editor refines the structure and language of the letters, the model undergoes a transformative process. TensorRT eliminates inefficiencies, fuses layers, and calibrates precision, resulting in an optimized model that can process data swiftly and accurately—like a beautifully composed novel ready to be enjoyed.</p>

<p>In our upcoming blog posts, we will take you on a journey where we explore practical examples of TensorRT in action. We will witness its impact on image classification, object detection, and more. Through these real-world applications, you will discover how TensorRT empowers AI practitioners to achieve remarkable performance gains, opening doors to innovative solutions and possibilities.</p>

<p>So, without further ado, let’s dive into the realm of TensorRT and witness firsthand the transformative power it holds in the field of artificial intelligence.</p>

<p><br /></p>

<h3 id="step-1---hotdog-classification-using-pure-pytorch">Step 1 :  Hotdog Classification Using Pure Pytorch</h3>

<p>Now that we have familiarized ourselves with the wonders of TensorRT, let’s dive into a practical example to witness its impact firsthand. Imagine a scenario where we want to classify images of different objects, specifically determining whether an image contains a hotdog or not. To tackle this deliciously challenging task, we will leverage a pretrained PyTorch model based on ResNet architecture, which has been trained on the vast and diverse ImageNet dataset.</p>

<p>The problem at hand is intriguing yet straightforward: we aim to develop an AI model capable of differentiating between hotdogs and other objects. By utilizing the power of deep learning and the wealth of knowledge encoded within the pretrained PyTorch model, we can accomplish this with remarkable accuracy.</p>

<p>To begin, we take an image of a mouthwatering hotdog as our test subject. The pretrained PyTorch model, being a master of image recognition, will scrutinize the visual features of the hotdog and perform intricate calculations to make its classification decision. It will utilize its knowledge of patterns, shapes, and textures acquired during its training on the vast ImageNet dataset, making an educated guess as to whether the image depicts a hotdog or something else entirely.</p>

<p>This process might seem effortless to us, but behind the scenes, the AI model performs an intricate dance of calculations and computations. It analyzes the pixels of the image, extracts features, and applies complex mathematical operations to arrive at a confident prediction.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">import</span> <span class="n">cv2</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torchvision.transforms</span> <span class="kn">import</span> <span class="n">Resize</span><span class="p">,</span> <span class="n">Compose</span><span class="p">,</span> <span class="n">ToTensor</span><span class="p">,</span> <span class="n">Normalize</span>

<span class="k">def</span> <span class="nf">preprocess_image</span><span class="p">(</span><span class="n">img_path</span><span class="p">):</span>
    <span class="c1"># transformations for the input data
</span>    <span class="n">transforms</span> <span class="o">=</span> <span class="nc">Compose</span><span class="p">([</span>
        <span class="nc">ToTensor</span><span class="p">(),</span>
        <span class="nc">Resize</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
        <span class="nc">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]),</span>
    <span class="p">])</span>

    <span class="c1"># read input image
</span>    <span class="n">input_img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">imread</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
    <span class="c1"># do transformations
</span>    <span class="n">input_data</span> <span class="o">=</span> <span class="nf">transforms</span><span class="p">(</span><span class="n">input_img</span><span class="p">)</span>
    <span class="n">batch_data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch_data</span>

<span class="k">def</span> <span class="nf">postprocess</span><span class="p">(</span><span class="n">output_data</span><span class="p">):</span>
    <span class="c1"># get class names
</span>    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">../data/imagenet-classes.txt</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">.</span><span class="nf">readlines</span><span class="p">()]</span>
    <span class="c1"># calculate human-readable value by softmax
</span>    <span class="n">confidences</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">output_data</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="c1"># find top predicted classes
</span>    <span class="n">_</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">output_data</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># print the top classes predicted by the model
</span>    <span class="k">while</span> <span class="n">confidences</span><span class="p">[</span><span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]]</span> <span class="o">&gt;</span> <span class="mi">50</span><span class="p">:</span>
        <span class="n">class_idx</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
        <span class="nf">print</span><span class="p">(</span>
            <span class="sh">"</span><span class="s">class:</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">classes</span><span class="p">[</span><span class="n">class_idx</span><span class="p">],</span>
            <span class="sh">"</span><span class="s">, confidence:</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">confidences</span><span class="p">[</span><span class="n">class_idx</span><span class="p">].</span><span class="nf">item</span><span class="p">(),</span>
            <span class="sh">"</span><span class="s">%, index:</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">class_idx</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="nb">input</span> <span class="o">=</span> <span class="nf">preprocess_image</span><span class="p">(</span><span class="sh">"</span><span class="s">../data/hotdog.jpg</span><span class="sh">"</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="nf">postprocess</span><span class="p">(</span><span class="n">output</span><span class="p">)</span></code></pre></figure>

<p>In the above code, we have a script that demonstrates the usage of a pretrained ResNet-50 model from torchvision for image classification. Let’s break down the code and understand its functionality:</p>

<p>First, we import the necessary libraries, including <code class="language-plaintext highlighter-rouge">models</code> from <code class="language-plaintext highlighter-rouge">torchvision</code>, <code class="language-plaintext highlighter-rouge">cv2</code>, and <code class="language-plaintext highlighter-rouge">torch</code>, which provide the tools for working with deep learning models and image processing.</p>

<p>The script defines two important functions. The <code class="language-plaintext highlighter-rouge">preprocess_image</code> function takes an image path as input and applies a series of transformations to preprocess the image. These transformations include converting the image to a tensor, resizing it to a specific size (in this case, 224x224), and normalizing its pixel values using mean and standard deviation values commonly used in ImageNet dataset preprocessing.</p>

<p>Next, we have the <code class="language-plaintext highlighter-rouge">postprocess</code> function, which processes the output of the model. It reads the class names from a text file (<code class="language-plaintext highlighter-rouge">imagenet-classes.txt</code>), calculates the confidence scores using softmax, and sorts the output to find the top predicted classes. It then prints the class name, confidence score, and class index for each top prediction.</p>

<p>Moving on, we preprocess the input image using the <code class="language-plaintext highlighter-rouge">preprocess_image</code> function. In this case, we are using the image <code class="language-plaintext highlighter-rouge">hotdog.jpg</code>. The resulting tensor is stored in the <code class="language-plaintext highlighter-rouge">input</code> variable and is moved to the GPU (assuming it is available) using <code class="language-plaintext highlighter-rouge">.cuda()</code>.</p>

<p>We then load the ResNet-50 model using <code class="language-plaintext highlighter-rouge">models.resnet50(pretrained=True)</code>. This fetches the pretrained weights from the model zoo. The model is set to evaluation mode (<code class="language-plaintext highlighter-rouge">model.eval()</code>), and its parameters are moved to the GPU using <code class="language-plaintext highlighter-rouge">.cuda()</code>.</p>

<p>Now, we perform a forward pass through the model by passing the preprocessed input tensor (<code class="language-plaintext highlighter-rouge">input</code>) to the ResNet-50 model (<code class="language-plaintext highlighter-rouge">model</code>). This gives us the output tensor (<code class="language-plaintext highlighter-rouge">output</code>).</p>

<p>Finally, we call the <code class="language-plaintext highlighter-rouge">postprocess</code> function with the <code class="language-plaintext highlighter-rouge">output</code> tensor to interpret and display the classification results. It prints the top predicted classes along with their corresponding confidence scores and class indices.</p>

<p>By following this code, a reader can classify an input image using the pretrained ResNet-50 model, obtaining the predicted class labels and their confidence scores. This example demonstrates the power of deep learning in image classification tasks and showcases how pretrained models can be easily utilized for real-world applications.</p>

<p>If everything goes well, you should see an output similar to this :</p>

<p><code class="language-plaintext highlighter-rouge">class: hotdog, hot dog, red hot , confidence: 60.50566864013672 %, index: 934</code></p>

<p>This output represents the classification result of the input image (in this case, a hotdog image) using the pretrained ResNet-50 model. The model has predicted that the image belongs to the class “hotdog, hot dog, red hot” with a confidence score of 60.51%. The corresponding class index is 934.</p>

<p>In simpler terms, the model has successfully recognized the image as a hotdog with a relatively high level of confidence. This showcases the capability of the ResNet-50 model to accurately classify objects in images, making it a valuable tool for various computer vision tasks.</p>

<p><br /></p>

<h3 id="step-2-pytorch-to-onnx-conversion">Step 2: PyTorch to ONNX Conversion</h3>
<p>In the previous section, we successfully built and utilized a PyTorch model for hotdog classification. Now, let’s take a step further and optimize the inference performance using TensorRT. In this section, we will explore the process of converting a PyTorch model to into a TensorRT engine.</p>

<p>To convert our PyTorch model to a TensorRT engine, we’ll follow a two-step process that involves the intermediate conversion to the ONNX format. This allows us to seamlessly integrate PyTorch and TensorRT, unlocking the benefits of accelerated inference.</p>

<p>The first step is to convert our PyTorch model to the ONNX format. ONNX, short for Open Neural Network Exchange, acts as a bridge between different deep learning frameworks. It provides a standardized representation of our model’s architecture and parameters, ensuring compatibility across platforms and frameworks.By exporting our PyTorch model to ONNX, we capture its structure and operations in a portable and platform-independent format. This enables us to work with the model using other frameworks, such as TensorRT, without losing important information or needing to reimplement the model from scratch.</p>

<p>To convert our PyTorch model to ONNX, we need to follow a few simple steps. First, we initialize an empty PyTorch model with the same architecture as our trained model. Then, we load the weights from our trained PyTorch model into the new model. After that, we export the model to the ONNX format using the torch.onnx.export function, specifying the input tensor shape and the desired output file name.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ONNX_FILE_PATH</span> <span class="o">=</span> <span class="sh">'</span><span class="s">../deploy_tools/resnet50.onnx</span><span class="sh">'</span>
<span class="n">torch</span><span class="p">.</span><span class="n">onnx</span><span class="p">.</span><span class="nf">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">ONNX_FILE_PATH</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">input</span><span class="sh">'</span><span class="p">],</span>
                  <span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">output</span><span class="sh">'</span><span class="p">],</span> <span class="n">export_params</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>The above lines are used to export a PyTorch model to the ONNX format. Here’s a breakdown of what each line does:</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">ONNX_FILE_PATH = '../deploy_tools/resnet50.onnx'</code>: This line defines the path and filename where the exported ONNX model will be saved. In this example, the ONNX file will be saved as “resnet50.onnx” in the “../deploy_tools” directory.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">torch.onnx.export(model, input, ONNX_FILE_PATH, input_names=['input'], output_names=['output'], export_params=True)</code>: This line exports the PyTorch model to ONNX format using the <code class="language-plaintext highlighter-rouge">torch.onnx.export</code> function. It takes several arguments:</p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">model</code>: This is the PyTorch model object that you want to export to ONNX.</li>
      <li><code class="language-plaintext highlighter-rouge">input</code>: This represents an example input tensor that will be used to trace the model. The shape and data type of this tensor should match the expected input for the model.</li>
      <li><code class="language-plaintext highlighter-rouge">ONNX_FILE_PATH</code>: This is the path and filename where the exported ONNX model will be saved, as defined in the previous line.</li>
      <li><code class="language-plaintext highlighter-rouge">input_names=['input']</code>: This specifies the names of the input nodes in the exported ONNX model. In this case, the input node will be named “input”.</li>
      <li><code class="language-plaintext highlighter-rouge">output_names=['output']</code>: This specifies the names of the output nodes in the exported ONNX model. In this case, the output node will be named “output”.</li>
      <li><code class="language-plaintext highlighter-rouge">export_params=True</code>: This indicates whether to export the parameters (weights and biases) of the model along with the model architecture. Setting it to <code class="language-plaintext highlighter-rouge">True</code> means the parameters will be included in the exported ONNX model.</li>
    </ul>
  </li>
</ul>

<p>Using the above code, the PyTorch model will be converted to the ONNX format and saved as an ONNX file at the specified location. The exported ONNX model can then be used in other frameworks or tools that support ONNX, allowing for interoperability and deployment in different runtime environments. Once we have successfully converted our PyTorch model to the ONNX format, it’s time to take a closer look at the inner workings of this intermediate representation. To gain a visual understanding of our ONNX model, we can utilize a powerful tool called Netron.</p>

<p>Netron is a user-friendly model visualization tool that allows us to explore and analyze the structure of our ONNX model. With its intuitive interface and interactive features, Netron offers a delightful experience for visualizing deep learning models. To visualize your ONNX model and confirm the success of the conversion process, you can follow these steps using the online tool Netron:</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Visit the Netron website</code>:
Go to the Netron website by using <a href="https://netron.app/">netron.app</a></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Load the ONNX model</code>:
Click on the “Open” button on the Netron website. This will prompt you to select your ONNX file from your local machine.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Explore the model</code>:
Once the ONNX model is loaded, Netron will display a visual representation of its structure. You can navigate through the model’s layers, examine node connections, and inspect input and output shapes.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Analyze the model</code>:
Use Netron to gain insights into the architecture and operations of your ONNX model. Verify that the conversion from PyTorch to ONNX was successful by examining the model’s structure and checking the expected input and output configurations.</p>
  </li>
</ul>

<p>Here is an example of how the onnx model visualization looks like in Netron :</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_1/onnx-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_1/onnx-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_1/onnx-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_1/onnx.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
ONNX example visualization from netron
</div>

<p><br /></p>

<h3 id="step-3--building-tensorrt-engine-from-onnx">Step 3 : Building TensorRT Engine from ONNX</h3>

<p>Now that we have successfully converted our PyTorch model to the ONNX format, it’s time to take the next step towards unleashing the power of TensorRT.</p>

<p>TensorRT is a high-performance deep learning inference optimizer and runtime library developed by NVIDIA. It is designed to optimize and accelerate neural network models, taking full advantage of GPU capabilities. By converting our ONNX model to TensorRT, we can harness the exceptional speed and efficiency offered by GPUs.</p>

<p>The process of converting ONNX to TensorRT involves leveraging the TensorRT Python API. This API provides a straightforward way to generate a TensorRT engine, which is a highly optimized representation of our model for efficient inference.</p>

<p>With the TensorRT engine in hand, we can take advantage of various optimizations and techniques offered by TensorRT. These include layer fusion, precision calibration, and dynamic tensor memory management, all aimed at maximizing inference performance.</p>

<p>In our upcoming sections, we will explore the Python code required to generate the TensorRT engine from our ONNX model. By following these steps, we will unlock the immense potential of TensorRT and experience a significant boost in inference speed and efficiency.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pycuda.driver</span> <span class="k">as</span> <span class="n">cuda</span>
<span class="kn">import</span> <span class="n">pycuda.autoinit</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">tensorrt</span> <span class="k">as</span> <span class="n">trt</span>
</code></pre></div></div>
<p>In these lines, we import the necessary libraries for our script: <code class="language-plaintext highlighter-rouge">pycuda.driver</code> for CUDA operations, <code class="language-plaintext highlighter-rouge">pycuda.autoinit</code> for initializing the GPU, <code class="language-plaintext highlighter-rouge">numpy</code> for numerical computations, and <code class="language-plaintext highlighter-rouge">tensorrt</code> for working with TensorRT.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">TRT_LOGGER</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Logger</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">Logger</span><span class="p">.</span><span class="n">WARNING</span><span class="p">)</span>
</code></pre></div></div>
<p>Here, we create a TensorRT logger object (<code class="language-plaintext highlighter-rouge">TRT_LOGGER</code>) with the log level set to <code class="language-plaintext highlighter-rouge">trt.Logger.WARNING</code>. This logger is used to manage logging messages during the conversion process.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">builder</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Builder</span><span class="p">(</span><span class="n">TRT_LOGGER</span><span class="p">)</span>
</code></pre></div></div>
<p>We create a TensorRT builder object (<code class="language-plaintext highlighter-rouge">builder</code>) using the previously defined logger. The builder is responsible for building TensorRT networks.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">EXPLICIT_BATCH</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="p">(</span><span class="nb">int</span><span class="p">)(</span><span class="n">trt</span><span class="p">.</span><span class="n">NetworkDefinitionCreationFlag</span><span class="p">.</span><span class="n">EXPLICIT_BATCH</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">create_network</span><span class="p">(</span><span class="n">EXPLICIT_BATCH</span><span class="p">)</span>
</code></pre></div></div>
<p>Here, we define the <code class="language-plaintext highlighter-rouge">EXPLICIT_BATCH</code> flag, which enables explicit batch mode in TensorRT. We then create a TensorRT network using the builder, specifying the <code class="language-plaintext highlighter-rouge">EXPLICIT_BATCH</code> flag.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">parser</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">OnnxParser</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">TRT_LOGGER</span><span class="p">)</span>
<span class="n">success</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="nf">parse_from_file</span><span class="p">(</span><span class="n">ONNX_FILE_PATH</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">parser</span><span class="p">.</span><span class="n">num_errors</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">parser</span><span class="p">.</span><span class="nf">get_error</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span>
</code></pre></div></div>
<p>We create an ONNX parser object (<code class="language-plaintext highlighter-rouge">parser</code>) associated with the network and logger. The parser is responsible for parsing the ONNX file and populating the TensorRT network. We parse the ONNX model by calling <code class="language-plaintext highlighter-rouge">parser.parse_from_file(ONNX_FILE_PATH)</code>. If there are any parsing errors, we retrieve and print them using a loop.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">config</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">create_builder_config</span><span class="p">()</span>
<span class="n">config</span><span class="p">.</span><span class="nf">set_memory_pool_limit</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">MemoryPoolType</span><span class="p">.</span><span class="n">WORKSPACE</span><span class="p">,</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">22</span><span class="p">)</span> <span class="c1"># 1 MiB
</span><span class="n">config</span><span class="p">.</span><span class="nf">set_flag</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">BuilderFlag</span><span class="p">.</span><span class="n">FP16</span><span class="p">)</span>

</code></pre></div></div>
<p>We create a builder configuration object (<code class="language-plaintext highlighter-rouge">config</code>) that allows us to configure various settings. Here, we create a memory pool limit configuration, setting the workspace memory pool limit to 1 MiB (1 « 22). The <code class="language-plaintext highlighter-rouge">config.set_flag(trt.BuilderFlag.FP16)</code> sets the quantization precision to <code class="language-plaintext highlighter-rouge">FP16</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">serialized_engine</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">build_serialized_network</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
</code></pre></div></div>
<p>Using the builder and configuration, we invoke <code class="language-plaintext highlighter-rouge">builder.build_serialized_network(network, config)</code> to build the TensorRT engine and obtain the serialized engine data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">../deploy_tools/resnet.engine</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">wb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">serialized_engine</span><span class="p">)</span>
</code></pre></div></div>
<p>Finally, we open a file named “resnet50.engine” in binary write mode (<code class="language-plaintext highlighter-rouge">"wb"</code>) using a <code class="language-plaintext highlighter-rouge">with open</code> block. We write the serialized engine data to the file, saving the TensorRT engine for future inference.</p>

<p>These lines of code collectively convert the provided ONNX model into a TensorRT engine, utilizing the TensorRT Python API and its optimization capabilities. Putting it all together, the final script is as follows :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pycuda.driver</span> <span class="k">as</span> <span class="n">cuda</span>
<span class="kn">import</span> <span class="n">pycuda.autoinit</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">tensorrt</span> <span class="k">as</span> <span class="n">trt</span>

<span class="n">TRT_LOGGER</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Logger</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">Logger</span><span class="p">.</span><span class="n">WARNING</span><span class="p">)</span>
<span class="n">builder</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Builder</span><span class="p">(</span><span class="n">TRT_LOGGER</span><span class="p">)</span>

<span class="n">EXPLICIT_BATCH</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="p">(</span><span class="nb">int</span><span class="p">)(</span><span class="n">trt</span><span class="p">.</span><span class="n">NetworkDefinitionCreationFlag</span><span class="p">.</span><span class="n">EXPLICIT_BATCH</span><span class="p">)</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">create_network</span><span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="nf">int</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">NetworkDefinitionCreationFlag</span><span class="p">.</span><span class="n">EXPLICIT_BATCH</span><span class="p">))</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">OnnxParser</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">TRT_LOGGER</span><span class="p">)</span>
<span class="n">success</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="nf">parse_from_file</span><span class="p">(</span><span class="n">ONNX_FILE_PATH</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">parser</span><span class="p">.</span><span class="n">num_errors</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">parser</span><span class="p">.</span><span class="nf">get_error</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">create_builder_config</span><span class="p">()</span>
<span class="n">config</span><span class="p">.</span><span class="nf">set_memory_pool_limit</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">MemoryPoolType</span><span class="p">.</span><span class="n">WORKSPACE</span><span class="p">,</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">22</span><span class="p">)</span> <span class="c1"># 1 MiB
</span><span class="n">config</span><span class="p">.</span><span class="nf">set_flag</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="n">BuilderFlag</span><span class="p">.</span><span class="n">FP16</span><span class="p">)</span>

<span class="n">serialized_engine</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="nf">build_serialized_network</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">../deploy_tools/resnet50.engine</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">wb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">serialized_engine</span><span class="p">)</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="step-4--performing-inference-on-tensorrt-engine">Step 4 : Performing Inference on TensorRT Engine</h3>

<p>Performing inference using the TensorRT engine is a breeze! We start by loading the serialized engine, which contains all the optimizations applied by TensorRT to our deep learning model. With the engine and execution context set up, we prepare input and output buffers that will hold the data for classification and predictions. The magic lies in TensorRT’s ability to work with GPU memory, tapping into the parallel processing power of the GPU for lightning-fast inference.</p>

<p>Next, we feed our input data, like an image, to the engine. It swiftly processes the data through its optimized network, streamlining calculations and fusing operations for maximum efficiency. The outcome? Rapid and accurate predictions for our image classification task.</p>

<p>Once the engine makes predictions on the GPU, we fetch the results by transferring the output data back to the CPU memory. This step enables us to perform any further post-processing as needed for our application’s specific requirements.</p>

<p>Let’s break down the provided inference code step by step and explain what each part does:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load the serialized TensorRT engine from the file
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">../deploy_tools/resnet.engine</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">serialized_engine</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">runtime</span><span class="p">.</span><span class="nf">deserialize_cuda_engine</span><span class="p">(</span><span class="n">serialized_engine</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">engine</span><span class="p">.</span><span class="nf">create_execution_context</span><span class="p">()</span>
</code></pre></div></div>

<p>In this part of the code, we start by loading the pre-built TensorRT engine from the file <code class="language-plaintext highlighter-rouge">resnet.engine</code> using a file stream. The engine’s serialized data is read as binary and stored in the variable <code class="language-plaintext highlighter-rouge">serialized_engine</code>. Next, we use the TensorRT runtime library to deserialize this binary data into a usable engine using <code class="language-plaintext highlighter-rouge">deserialize_cuda_engine()</code> function, which is then stored in the <code class="language-plaintext highlighter-rouge">engine</code> variable. The <code class="language-plaintext highlighter-rouge">Engine</code> object represents our highly optimized deep learning model tailored for execution on NVIDIA GPUs. We create an execution context named <code class="language-plaintext highlighter-rouge">context</code>, which allows us to interact with the engine and perform inference.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Determine dimensions and create page-locked memory buffers for host inputs/outputs
</span><span class="n">h_input</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">pagelocked_empty</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="nf">volume</span><span class="p">(</span><span class="n">engine</span><span class="p">.</span><span class="nf">get_binding_shape</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">h_output</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">pagelocked_empty</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="nf">volume</span><span class="p">(</span><span class="n">engine</span><span class="p">.</span><span class="nf">get_binding_shape</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
</code></pre></div></div>
<p>In this section, we determine the dimensions of the input and output bindings required by the TensorRT engine. The <code class="language-plaintext highlighter-rouge">engine.get_binding_shape(0)</code> returns the shape (dimensions) of the input binding, and <code class="language-plaintext highlighter-rouge">engine.get_binding_shape(1)</code> returns the shape of the output binding. In general, the input and output  bindings are stored serially. This implies that if there are multiple inputs are outputs, their shapes can be accessed using their resepctive indices in the bindings. For example, if we had 2 inputs and 1 output, the first two inputs will have indices <code class="language-plaintext highlighter-rouge">(0)</code> and <code class="language-plaintext highlighter-rouge">(1)</code>, while the output will have index <code class="language-plaintext highlighter-rouge">(2)</code>. We use <code class="language-plaintext highlighter-rouge">cuda.pagelocked_empty()</code> to create page-locked (pinned) memory buffers on the host (CPU) to hold the input and output data. Page-locked memory ensures that data transfers between the host (CPU) and the device (GPU) are efficient and do not involve memory swapping.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Allocate device memory for inputs and outputs.
</span><span class="n">d_input</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">mem_alloc</span><span class="p">(</span><span class="n">h_input</span><span class="p">.</span><span class="n">nbytes</span><span class="p">)</span>
<span class="n">d_output</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">mem_alloc</span><span class="p">(</span><span class="n">h_output</span><span class="p">.</span><span class="n">nbytes</span><span class="p">)</span>
</code></pre></div></div>
<p>In this part, we allocate device memory on the GPU for storing the input and output data during inference using <code class="language-plaintext highlighter-rouge">cuda.mem_alloc()</code> function. The size of the memory buffers is determined by the size of the page-locked memory buffers <code class="language-plaintext highlighter-rouge">h_input</code> and <code class="language-plaintext highlighter-rouge">h_output</code> that we created earlier.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create a CUDA stream to perform asynchronous memory transfers and execution
</span><span class="n">stream</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nc">Stream</span><span class="p">()</span>
</code></pre></div></div>
<p>Here, we create a CUDA stream named <code class="language-plaintext highlighter-rouge">stream</code>, which allows us to perform asynchronous memory transfers and inference execution on the GPU. Asynchronous execution helps overlap data transfers and computations, improving overall performance.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Preprocess the input image and transfer it to the GPU.
</span><span class="n">host_input</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">preprocess_image</span><span class="p">(</span><span class="sh">"</span><span class="s">../data/hotdog.jpg</span><span class="sh">"</span><span class="p">).</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">)</span>
<span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_htod_async</span><span class="p">(</span><span class="n">d_input</span><span class="p">,</span> <span class="n">host_input</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
</code></pre></div></div>
<p>In this part, we preprocess the input image, which in this case is <code class="language-plaintext highlighter-rouge">hotdog.jpg</code> to prepare it for inference. The image is converted to a NumPy array and set to dtype np.float32, which matches the data type expected by the TensorRT engine. The <code class="language-plaintext highlighter-rouge">preprocess_image</code> function is used to perform any necessary transformations or normalization specific to the model’s input requirements. The preprocessed input image is then asynchronously transferred from the host (CPU) to the device (GPU) memory using <code class="language-plaintext highlighter-rouge">cuda.memcpy_htod_async()</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Run inference on the TensorRT engine.
</span><span class="n">context</span><span class="p">.</span><span class="nf">execute_async_v2</span><span class="p">(</span><span class="n">bindings</span><span class="o">=</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">d_input</span><span class="p">),</span> <span class="nf">int</span><span class="p">(</span><span class="n">d_output</span><span class="p">)],</span> <span class="n">stream_handle</span><span class="o">=</span><span class="n">stream</span><span class="p">.</span><span class="n">handle</span><span class="p">)</span>
</code></pre></div></div>
<p>Here, we execute the inference on the TensorRT engine using the execution context’s <code class="language-plaintext highlighter-rouge">execute_async_v2()</code> function. We pass the input and output bindings to the engine using the bindings parameter, which is a list containing the device memory addresses of the input and output data. The stream_handle parameter ensures that the inference runs asynchronously on the GPU.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Transfer predictions back from the GPU to the CPU.
</span><span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_dtoh_async</span><span class="p">(</span><span class="n">h_output</span><span class="p">,</span> <span class="n">d_output</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
</code></pre></div></div>
<p>After the inference is complete, the output predictions reside in the device (GPU) memory. We use <code class="language-plaintext highlighter-rouge">cuda.memcpy_dtoh_async()</code> to transfer the predictions from the GPU to the host (CPU) memory in an asynchronous manner.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Synchronize the stream to wait for the inference to finish.
</span><span class="n">stream</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
</code></pre></div></div>
<p>Before accessing the output predictions on the CPU, we synchronize the CUDA stream using <code class="language-plaintext highlighter-rouge">stream.synchronize()</code>. This ensures that all GPU computations and data transfers are complete before we proceed to post-process the predictions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Convert the output data to a Torch tensor and perform post-processing.
</span><span class="n">tensorrt_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">h_output</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">postprocess</span><span class="p">(</span><span class="n">tensorrt_output</span><span class="p">)</span>
</code></pre></div></div>

<p>Finally, we convert the output predictions from the host (CPU) memory, which are stored in the h_output buffer, into a Torch tensor. The <code class="language-plaintext highlighter-rouge">unsqueeze(0)</code> operation is used to add a batch dimension to the tensor if required. The Torch tensor output_data now contains the final predictions obtained from the TensorRT engine. Depending on the specific task, we can perform further post-processing using the postprocess function to interpret the results and present them in a human-readable format.</p>

<p>Putting it all together, the inference on TensorRT engine script looks like this</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">runtime</span> <span class="o">=</span> <span class="n">trt</span><span class="p">.</span><span class="nc">Runtime</span><span class="p">(</span><span class="n">TRT_LOGGER</span><span class="p">)</span>

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">../deploy_tools/resnet.engine</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">serialized_engine</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>
    
<span class="n">engine</span> <span class="o">=</span> <span class="n">runtime</span><span class="p">.</span><span class="nf">deserialize_cuda_engine</span><span class="p">(</span><span class="n">serialized_engine</span><span class="p">)</span>

<span class="n">context</span> <span class="o">=</span> <span class="n">engine</span><span class="p">.</span><span class="nf">create_execution_context</span><span class="p">()</span>

<span class="c1"># Determine dimensions and create page-locked memory buffers (i.e. won't be swapped to disk) to hold host inputs/outputs.
</span><span class="n">h_input</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">pagelocked_empty</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="nf">volume</span><span class="p">(</span><span class="n">engine</span><span class="p">.</span><span class="nf">get_binding_shape</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">h_input</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">pagelocked_empty</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="nf">volume</span><span class="p">(</span><span class="n">engine</span><span class="p">.</span><span class="nf">get_binding_shape</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">h_output</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">pagelocked_empty</span><span class="p">(</span><span class="n">trt</span><span class="p">.</span><span class="nf">volume</span><span class="p">(</span><span class="n">engine</span><span class="p">.</span><span class="nf">get_binding_shape</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="c1"># Allocate device memory for inputs and outputs.
</span><span class="n">d_input</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">mem_alloc</span><span class="p">(</span><span class="n">h_input</span><span class="p">.</span><span class="n">nbytes</span><span class="p">)</span>
<span class="n">d_output</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">mem_alloc</span><span class="p">(</span><span class="n">h_output</span><span class="p">.</span><span class="n">nbytes</span><span class="p">)</span>
<span class="c1"># Create a stream in which to copy inputs/outputs and run inference.
</span><span class="n">stream</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nc">Stream</span><span class="p">()</span>
<span class="c1"># read input image and preprocess
</span><span class="n">host_input</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">preprocess_image</span><span class="p">(</span><span class="sh">"</span><span class="s">../data/hotdog.jpg</span><span class="sh">"</span><span class="p">).</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">)</span>
<span class="c1"># Transfer input data to the GPU.
</span><span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_htod_async</span><span class="p">(</span><span class="n">d_input</span><span class="p">,</span> <span class="n">host_input</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
<span class="c1"># Run inference.
</span><span class="n">context</span><span class="p">.</span> <span class="nf">execute_async_v2</span><span class="p">(</span><span class="n">bindings</span><span class="o">=</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">d_input</span><span class="p">),</span> <span class="nf">int</span><span class="p">(</span><span class="n">d_output</span><span class="p">)],</span> <span class="n">stream_handle</span><span class="o">=</span><span class="n">stream</span><span class="p">.</span><span class="n">handle</span><span class="p">)</span>
<span class="c1"># Transfer predictions back from the GPU.
</span><span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_dtoh_async</span><span class="p">(</span><span class="n">h_output</span><span class="p">,</span> <span class="n">d_output</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
<span class="c1"># Synchronize the stream
</span><span class="n">stream</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
<span class="c1"># postprocess output
</span><span class="n">tensorrt_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">h_output</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">postprocess</span><span class="p">(</span><span class="n">tensorrt_output</span><span class="p">)</span>
</code></pre></div></div>

<p>With this, the inference process using the TensorRT engine is complete. The optimized engine takes advantage of GPU acceleration and sophisticated optimizations to deliver rapid and efficient predictions for our image classification model.</p>

<p><br /></p>

<h3 id="time-to-retrospect">Time to Retrospect</h3>
<p><br /></p>

<h4 id="consistency-validation">Consistency Validation</h4>

<p>As we draw the curtains on our exploration of PyTorch and TensorRT in the world of image classification, it’s time to reflect on the intriguing findings of our journey. One crucial aspect of this quest was comparing the output values obtained from both PyTorch and TensorRT after quantization. With a keen eye for accuracy, we scrutinized the outputs to measure any discrepancies introduced during the optimization process. Our quest for precision revealed that while quantization and optimization indeed caused minute deviations in the output values, these variations were negligible and well within acceptable limits. Thus, we could confidently establish the compatibility and reliability of TensorRT’s quantization process in preserving the essence of our image classification task.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Calculate MAE between pure torch output and TensorRT inference output
</span><span class="n">mae</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="nf">cpu</span><span class="p">()</span> <span class="o">-</span> <span class="n">tensorrt_output</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">MAE:</span><span class="sh">"</span><span class="p">,</span> <span class="n">mae</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
</code></pre></div></div>

<p>The output of this block of code looks would look something like :</p>

<p><code class="language-plaintext highlighter-rouge">MAE: 0.00590712483972311</code></p>

<p>The Mean Absolute Error between torch <code class="language-plaintext highlighter-rouge">ouput</code> and <code class="language-plaintext highlighter-rouge">tensorrt_output</code> is tending towards zero which indicates that even the post quantization results are similar ( if not equal ) to the pure pytorch output.</p>

<p><br /></p>

<h4 id="latency-measurement">Latency Measurement</h4>

<p>In this analysis, we are focused on comparing the inference speed between pure PyTorch and TensorRT. To achieve this, we run each inference method for multiple iterations and measure the time it takes to process a single input image. By doing so, we gain valuable insights into the real-world performance of both approaches.</p>

<p>For the pure PyTorch inference, we employ a pre-trained ResNet model and run the image classification task multiple times, recording the time taken to process each image. The average latency is then calculated over the specified number of iterations. On the other hand, for the TensorRT inference, we have optimized the same ResNet model using TensorRT and leveraged GPU acceleration to further speed up the inference process. Once again, we run the image classification task multiple times and calculate the average latency.</p>

<p>By comparing the average latencies of both methods, we can quantitatively gauge the speedup offered by TensorRT over pure PyTorch. This performance analysis provides a clear picture of the benefits that TensorRT’s optimization and GPU acceleration bring to the table, paving the way for more efficient and rapid deployment of deep learning models in real-world applications. With these results in hand, we can confidently choose the best inference approach tailored to our specific needs, whether it’s maximum accuracy with PyTorch or lightning-fast performance with TensorRT.</p>

<p>Here is the script to measure latency of pure torch inference.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pure torch latency measurement 
</span><span class="kn">import</span> <span class="n">time</span>

<span class="c1"># Pure PyTorch Inference
</span><span class="k">def</span> <span class="nf">pytorch_inference</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>

<span class="c1"># Number of iterations
</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Run Pure PyTorch Inference for 1000 iterations
</span><span class="n">total_pytorch_latency</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">latency</span> <span class="o">=</span> <span class="nf">pytorch_inference</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
        <span class="n">total_pytorch_latency</span> <span class="o">+=</span> <span class="n">latency</span>
<span class="n">average_pytorch_latency</span> <span class="o">=</span> <span class="p">(</span><span class="n">total_pytorch_latency</span> <span class="o">/</span> <span class="n">num_iterations</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">empty_cache</span><span class="p">()</span>
</code></pre></div></div>

<p>And, now let us look at how to measure inference speedup offered by TensorRT engine.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># TensorRT latency measurement 
</span><span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="c1"># TensorRT FP16 Inference
</span><span class="k">def</span> <span class="nf">tensorrt_inference</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">d_input</span><span class="p">,</span> <span class="n">d_output</span><span class="p">,</span> <span class="n">host_input</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_htod_async</span><span class="p">(</span><span class="n">d_input</span><span class="p">,</span> <span class="n">host_input</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">context</span><span class="p">.</span><span class="nf">execute_async_v2</span><span class="p">(</span><span class="n">bindings</span><span class="o">=</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">d_input</span><span class="p">),</span> <span class="nf">int</span><span class="p">(</span><span class="n">d_output</span><span class="p">)],</span> <span class="n">stream_handle</span><span class="o">=</span><span class="n">stream</span><span class="p">.</span><span class="n">handle</span><span class="p">)</span>
    <span class="n">cuda</span><span class="p">.</span><span class="nf">memcpy_dtoh_async</span><span class="p">(</span><span class="n">h_output</span><span class="p">,</span> <span class="n">d_output</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">stream</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
    <span class="n">tensorrt_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">h_output</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">tensorrt_output</span><span class="p">,</span> <span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>

<span class="c1"># Number of iterations
</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Run TensorRT FP16 Inference for 1000 iterations
</span><span class="n">total_tensorrt_latency</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">tensorrt_output</span><span class="p">,</span> <span class="n">latency</span> <span class="o">=</span> <span class="nf">tensorrt_inference</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">d_input</span><span class="p">,</span> <span class="n">d_output</span><span class="p">,</span> <span class="n">host_input</span><span class="p">)</span>
        <span class="n">total_tensorrt_latency</span> <span class="o">+=</span> <span class="n">latency</span>
<span class="n">average_tensorrt_latency</span> <span class="o">=</span> <span class="p">(</span><span class="n">total_tensorrt_latency</span> <span class="o">/</span> <span class="n">num_iterations</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>

</code></pre></div></div>

<p>Now, let’s visualize the comparison of inference latencies between pure PyTorch and TensorRT using a bar chart.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">latencies</span> <span class="o">=</span> <span class="p">[</span><span class="n">average_pytorch_latency</span><span class="p">,</span> <span class="n">average_tensorrt_latency</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Pure PyTorch</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">TensorRT</span><span class="sh">'</span><span class="p">]</span>

<span class="c1"># Create a bar chart
</span><span class="n">plt</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">latencies</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">])</span>

<span class="c1"># Add labels and title
</span><span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Inference Method</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Average Latency (ms)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Comparison of Latency: Pure PyTorch vs. TensorRT</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Show the plot
</span><span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p>We have collected the average latencies for both methods and stored them in the latencies list, while the corresponding method names, <code class="language-plaintext highlighter-rouge">Pure PyTorch</code> and <code class="language-plaintext highlighter-rouge">TensorRT</code>, are in the labels list.</p>

<p>Using the <code class="language-plaintext highlighter-rouge">matplotlib.pyplot.bar()</code> function, we create a bar chart where each bar represents one of the inference methods. The height of each bar corresponds to the average latency of that method, measured in milliseconds. We have assigned distinct colors, ‘blue’ for pure PyTorch and ‘green’ for TensorRT, making it easy to visually differentiate between the two.</p>

<p>The output plot would look as follows :</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0 text-center"> <!-- Add 'text-center' class here -->
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_1/latency_compare-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_1/latency_compare-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_1/latency_compare-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blog/blog_1/latency_compare.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="latency compare" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Latency comparison between Pure Torch and TensorRT inference.
</div>

<p>In the bar chart, we have two bars representing the inference methods: ‘Pure PyTorch’ and ‘TensorRT.’ The height of each bar represents the average latency measured in milliseconds for each method. The average torch latency is approximately <code class="language-plaintext highlighter-rouge">5.50 ms</code>, while the average tensorrt latency is approximately <code class="language-plaintext highlighter-rouge">1.48 ms</code>.</p>

<p>The significant disparity between the two bars immediately catches our attention. The ‘TensorRT’ bar is remarkably shorter than the ‘Pure PyTorch’ bar, indicating that TensorRT outperforms PyTorch in terms of inference speed. The speedup offered by TensorRT can be calculated as the ratio of the average torch latency to the average tensorrt latency:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Speedup = Average Torch Latency / Average TensorRT Latency
Speedup = 5.50 ms / 1.48 ms ≈ 3.71
</code></pre></div></div>

<p>This means that TensorRT achieves an impressive speedup of approximately <strong><code class="language-plaintext highlighter-rouge">3.71 times</code></strong> faster than pure PyTorch. Such a significant improvement in inference speed can have a profound impact on real-world applications, enabling faster response times and enhancing overall system efficiency.</p>

<p><br /></p>

<h3 id="conclusion">Conclusion</h3>
<p>In conclusion, our journey through image classification using PyTorch and TensorRT has been an enlightening experience. We witnessed the power of PyTorch in providing accurate and reliable classification results. However, the real revelation came when we optimized the model using TensorRT.</p>

<p>TensorRT’s quantization and GPU acceleration brought remarkable benefits to the table. We observed a negligible error in the output values after quantization, ensuring the preservation of accuracy. The speedup comparison was awe-inspiring, with TensorRT demonstrating its prowess by achieving a speedup of approximately 3.71 times faster than pure PyTorch.</p>

<p>This performance boost provided by TensorRT opens up new avenues for deploying deep learning models in real-time applications where speed and efficiency are crucial. With PyTorch for precision and TensorRT for optimization, we are equipped to tackle diverse AI challenges with unmatched accuracy and exceptional speed.</p>

<p>As we conclude this journey, we stand confident in embracing the synergistic power of PyTorch and TensorRT, paving the way for transformative advancements in the world of AI and deep learning. The road ahead beckons, and we look forward to applying these invaluable insights to usher in a new era of intelligent applications and cutting-edge innovations.</p>

<p>In the upcoming part of the blog, we will delve into the world of C++ and explore how to build the TensorRT engine and perform inference for the same image classification model. Transitioning from Python to C++ empowers us with the potential to deploy our optimized models in production environments with even greater efficiency. We will witness firsthand the seamless integration of TensorRT’s powerful optimizations and GPU acceleration with C++ code, unlocking the full potential of our deep learning model in high-performance applications. Get ready to embark on the next phase of our exciting journey into the realm of C++ and TensorRT!</p>]]></content><author><name></name></author><category term="edge-ai" /><category term="nvidia" /><category term="tensorrt" /><category term="deep-learning" /><summary type="html"><![CDATA[Introduction to TensorRT in python.]]></summary></entry></feed>